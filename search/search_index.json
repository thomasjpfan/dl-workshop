{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Differential Programming with JAX Thanks for stopping by to read this online book on differential programming! What you will learn From a practical standpoint, this book will teach you the basics of how to use JAX, in particular the idioms and how they map onto what we might alrady know in Python. From a more abstract standpoint, this book will give you practice with a more \"functional\" style of programming (in contrast to an object-oriented style or an imperative style). My goal for you is to finish reading the book having the confidence to write differentiable numeric models of the world. The key operative word here being \"differentiable\" - you can calculate and evaluate the gradient of a model (written as a function) w.r.t. its parameters (which are passed in as inputs). Along the way, you might see the connections between topics that you might be familiar with (Bayesian statistics, deep learning, and more) and differntial computing. If they pop out to you through this book and the examples in there, then I know you'll likely enjoy the thrill of seeing a new connection in your personal knowledge graph. How to use this book For online readers This website, which is freely available to all, can be read in order from start to end. If you're already familiar with differential computing and are curious about how to write JAX programs, head over to the section on JAX programming. If you're curious about how to write neural network models, head over to the stax section. There's also a collection of \"case study\"/\"recipe\"-like chapters, in which we set up a computing problem of relevance and walk through how to write a JAX program there, leveraging what we have learned in the rest of the book. For interactive coding learners If you're the type who likes to execute code and break it in order to learn about what's going on, or if you're in an online interactive learning session with me, then you're in luck! The entire book has been written using Jupyter notebooks and Markdown files, and any section written as a Jupyter notebook has an \"open in Binder\" badge available at the top. Look for the button that looks like the one below: This will bring you to a pre-built Binder sesion that you can use to execute the code, break it, and play around with the ideas in the book. There are exercises interspersed throughout the book that you can stop to read through as well. If you prefer to set up an environment locally, here are instructions for you: conda env create -f environment.yml conda activate dl-workshop # older versions of conda use `source activate` rather than `conda activate` python -m ipykernel install --user --name dl-workshop jupyter labextension install @jupyter-widgets/jupyterlab-manager If you want jax with GPU, you will need to build from source, or follow the installation instructions If you are using Jupyter Lab, you will want to also ensure that ipywidgets is installed: # only if you don't have ipywidgets installed. conda install -c conda-forge ipywidgets # the next line is necessary. jupyter labextension install @jupyter-widgets/jupyterlab-manager Further Reading Demystifying Different Variants of Gradient Descent Optimization Algorithm","title":"Home"},{"location":"#differential-programming-with-jax","text":"Thanks for stopping by to read this online book on differential programming!","title":"Differential Programming with JAX"},{"location":"#what-you-will-learn","text":"From a practical standpoint, this book will teach you the basics of how to use JAX, in particular the idioms and how they map onto what we might alrady know in Python. From a more abstract standpoint, this book will give you practice with a more \"functional\" style of programming (in contrast to an object-oriented style or an imperative style). My goal for you is to finish reading the book having the confidence to write differentiable numeric models of the world. The key operative word here being \"differentiable\" - you can calculate and evaluate the gradient of a model (written as a function) w.r.t. its parameters (which are passed in as inputs). Along the way, you might see the connections between topics that you might be familiar with (Bayesian statistics, deep learning, and more) and differntial computing. If they pop out to you through this book and the examples in there, then I know you'll likely enjoy the thrill of seeing a new connection in your personal knowledge graph.","title":"What you will learn"},{"location":"#how-to-use-this-book","text":"","title":"How to use this book"},{"location":"#for-online-readers","text":"This website, which is freely available to all, can be read in order from start to end. If you're already familiar with differential computing and are curious about how to write JAX programs, head over to the section on JAX programming. If you're curious about how to write neural network models, head over to the stax section. There's also a collection of \"case study\"/\"recipe\"-like chapters, in which we set up a computing problem of relevance and walk through how to write a JAX program there, leveraging what we have learned in the rest of the book.","title":"For online readers"},{"location":"#for-interactive-coding-learners","text":"If you're the type who likes to execute code and break it in order to learn about what's going on, or if you're in an online interactive learning session with me, then you're in luck! The entire book has been written using Jupyter notebooks and Markdown files, and any section written as a Jupyter notebook has an \"open in Binder\" badge available at the top. Look for the button that looks like the one below: This will bring you to a pre-built Binder sesion that you can use to execute the code, break it, and play around with the ideas in the book. There are exercises interspersed throughout the book that you can stop to read through as well. If you prefer to set up an environment locally, here are instructions for you: conda env create -f environment.yml conda activate dl-workshop # older versions of conda use `source activate` rather than `conda activate` python -m ipykernel install --user --name dl-workshop jupyter labextension install @jupyter-widgets/jupyterlab-manager If you want jax with GPU, you will need to build from source, or follow the installation instructions If you are using Jupyter Lab, you will want to also ensure that ipywidgets is installed: # only if you don't have ipywidgets installed. conda install -c conda-forge ipywidgets # the next line is necessary. jupyter labextension install @jupyter-widgets/jupyterlab-manager","title":"For interactive coding learners"},{"location":"#further-reading","text":"Demystifying Different Variants of Gradient Descent Optimization Algorithm","title":"Further Reading"},{"location":"00-preliminaries/01-preface/","text":"Preface The years between 2010 to 2020 was a breakout decade for deep learning. There, we saw an explosion of tooling, model building, flashy demos of performance gains, and more. But beneath the surface hype of deep learning models and methods, I witnessed the maturity of tooling surrounding this idea of \"differential programming\" and composable program transforms. That tooling is the focus of this material you're reading here. On differential programming What do we mean by differential programming ? From one perspective, it is the core of modern-day learning systems, where mathematical derivatives , also known as gradients , are used in optimization and learning tasks. For example, we might use gradient descent to optimize a linear model that maps predictor variables to and output variable of interest. As another example, we might use gradient descent to optimize the parameters of a neural network model that classifies molecules as being biodegradable or not based on their descriptors alone. (As you will see in this book, the \"learning\" in deep learning is nothing more than a optimization of parameters by gradient descent.) With differential computing, the key activity that we engage in is the calculation of derivatives, and tooling that helps us compute derivatives automatically , such that we do not have to calculate them by hand, is central to differential computing. If you took a calculus class, the chain rule will feature prominently here, and JAX provides the tooling that gives us automatic differentiation , i.e. a \"program transformation\" that automatically calculates a derivative function for any other function that calculates a scalar-valued output. On program transformations As mentioned in the last paragraph, when structured in the way that JAX does it, derivative computation falls under this umbrella of \"program transformations\". That is to say, JAX provides functions that transforms a program from one form into another. (We will see by exactly what syntax we'll need to automatically create gradient functions later in the book.) Now, gradients aren't the only program transformation that exist. A function that maps a scalar function over a vector of inputs, thereby producing a vector of outputs instead, is another example of a program transformation. In this book, we will explore through some of the program transformations that are available in JAX, and see how they can be used to write beautifully structured array programs that are more flat than nested, more explicit than implicit, and are tens to hundreds of times more performant than vanilla Python/NumPy programs. The choice of JAX vs. other array frameworks As of the time of writing, there are two dominant deep learning frameworks that also provide automatic differentiation. They are PyTorch and TensorFlow, which have each enjoyed their zenith of fame. JAX generally distinguishes itself from PyTorch and TensorFlow in two ways by being developed against the idiomatic NumPy and SciPy APIs, thereby being extremely compatible with the rest of the PyData ecosystem; extending the Python language with more program transforms than just differential computing transformations, properly documenting the reasons why they depart from the very small subset of idioms that they don't follow. In particular, I would like to highlight the first point. API interoperability between computing packages is crucial for a thriving data science ecosystem. JAX's NumPy and SciPy wrappers ensure that all computations done using existing NumPy and SciPy code can very easily be transformed into differential-compatible computations for which program transforms provided by JAX can be easily applied. As you'll see in the book, you can even plot JAX arrays in matplotlib , the venerable Python plotting library, because of its developer's compatibility efforts.","title":"Preface"},{"location":"00-preliminaries/01-preface/#preface","text":"The years between 2010 to 2020 was a breakout decade for deep learning. There, we saw an explosion of tooling, model building, flashy demos of performance gains, and more. But beneath the surface hype of deep learning models and methods, I witnessed the maturity of tooling surrounding this idea of \"differential programming\" and composable program transforms. That tooling is the focus of this material you're reading here.","title":"Preface"},{"location":"00-preliminaries/01-preface/#on-differential-programming","text":"What do we mean by differential programming ? From one perspective, it is the core of modern-day learning systems, where mathematical derivatives , also known as gradients , are used in optimization and learning tasks. For example, we might use gradient descent to optimize a linear model that maps predictor variables to and output variable of interest. As another example, we might use gradient descent to optimize the parameters of a neural network model that classifies molecules as being biodegradable or not based on their descriptors alone. (As you will see in this book, the \"learning\" in deep learning is nothing more than a optimization of parameters by gradient descent.) With differential computing, the key activity that we engage in is the calculation of derivatives, and tooling that helps us compute derivatives automatically , such that we do not have to calculate them by hand, is central to differential computing. If you took a calculus class, the chain rule will feature prominently here, and JAX provides the tooling that gives us automatic differentiation , i.e. a \"program transformation\" that automatically calculates a derivative function for any other function that calculates a scalar-valued output.","title":"On differential programming"},{"location":"00-preliminaries/01-preface/#on-program-transformations","text":"As mentioned in the last paragraph, when structured in the way that JAX does it, derivative computation falls under this umbrella of \"program transformations\". That is to say, JAX provides functions that transforms a program from one form into another. (We will see by exactly what syntax we'll need to automatically create gradient functions later in the book.) Now, gradients aren't the only program transformation that exist. A function that maps a scalar function over a vector of inputs, thereby producing a vector of outputs instead, is another example of a program transformation. In this book, we will explore through some of the program transformations that are available in JAX, and see how they can be used to write beautifully structured array programs that are more flat than nested, more explicit than implicit, and are tens to hundreds of times more performant than vanilla Python/NumPy programs.","title":"On program transformations"},{"location":"00-preliminaries/01-preface/#the-choice-of-jax-vs-other-array-frameworks","text":"As of the time of writing, there are two dominant deep learning frameworks that also provide automatic differentiation. They are PyTorch and TensorFlow, which have each enjoyed their zenith of fame. JAX generally distinguishes itself from PyTorch and TensorFlow in two ways by being developed against the idiomatic NumPy and SciPy APIs, thereby being extremely compatible with the rest of the PyData ecosystem; extending the Python language with more program transforms than just differential computing transformations, properly documenting the reasons why they depart from the very small subset of idioms that they don't follow. In particular, I would like to highlight the first point. API interoperability between computing packages is crucial for a thriving data science ecosystem. JAX's NumPy and SciPy wrappers ensure that all computations done using existing NumPy and SciPy code can very easily be transformed into differential-compatible computations for which program transforms provided by JAX can be easily applied. As you'll see in the book, you can even plot JAX arrays in matplotlib , the venerable Python plotting library, because of its developer's compatibility efforts.","title":"The choice of JAX vs. other array frameworks"},{"location":"00-preliminaries/02-prerequisites/","text":"Pre-requisite Knowledge Before you go on, here's some topics and Python package APIs that are handy for you to know. My notes: Python: requisite. NumPy API: requisite. SciPy API: requisite. How to take derivatives: good to know, I'll cover a bit of it. How to write functional style programs: good to know, but I'll cover it. Deep learning: not necessary; one chapter covers how to write neural networks using nothing but NumPy, and how to train them using JAX's grad , and another chapter covers a way of writing neural netowrks using stax . Statistical modelling: needed only for the chapters using likelihoods and finding maximum likelihood param values using gradient descent.","title":"Prerequisites"},{"location":"00-preliminaries/02-prerequisites/#pre-requisite-knowledge","text":"Before you go on, here's some topics and Python package APIs that are handy for you to know. My notes: Python: requisite. NumPy API: requisite. SciPy API: requisite. How to take derivatives: good to know, I'll cover a bit of it. How to write functional style programs: good to know, but I'll cover it. Deep learning: not necessary; one chapter covers how to write neural networks using nothing but NumPy, and how to train them using JAX's grad , and another chapter covers a way of writing neural netowrks using stax . Statistical modelling: needed only for the chapters using likelihoods and finding maximum likelihood param values using gradient descent.","title":"Pre-requisite Knowledge"},{"location":"00-preliminaries/03-setup/","text":"Getting Setup If you're interested in executing the code...","title":"Setup"},{"location":"00-preliminaries/03-setup/#getting-setup","text":"If you're interested in executing the code...","title":"Getting Setup"},{"location":"01-differential-programming/01-neural-nets-from-scratch/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Neural Networks from Scratch In this chapter, we are going to explore differential computing in the place where it was most highly leveraged: the training of neural networks. Now, as with all topics, to learn something most clearly, it pays to have an anchoring example that we start with. In this section, we'll lean heavily on linear regression as that anchoring example . We'll also explore what gradient-based optimization is, see an elementary example of that in action, and then connect those ideas back to optimization of a linear model. Once we're done there, then we'll see the exact same ideas in action with a logistic regression model, before finally seeing them in action again with a neural network model. The big takeaway from this chapter is that basically all supervised learning tasks can be broken into: model loss optimizer Hope you enjoy it! If you're ready, let's take a look at linear regression. import jax.numpy as np from jax import jit import numpy.random as npr import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider from pyprojroot import here Linear Regression Linear regression is foundational to deep learning. It should be a model that everybody has been exposed to before in school. A humorous take I have heard about linear models is that if you zoom in enough into whatever system of the world you're modelling, anything can basically look linear. One of the advantages of linear models is its simplicity. It basically has two parameters, one explaining a \"baseline\" (intercept) and the other explaining strength of relationships (slope). Yet one of the disadvantages of linear models is also its simplicity. A linear model has a strong presumption of linearity. NOTE TO SELF: I need to rewrite this introduction. It is weak. Equation Form Linear regression, as a model, is expressed as follows: y = wx + b y = wx + b Here: The model is the equation, y = wx + b y = wx + b . y y is the output data. x x is our input data. w w is a slope parameter. b b is our intercept parameter. Implicit in the model is the fact that we have transformed y y by another function, the \"identity\" function, f(x) = x f(x) = x . In this model, y y and x x are, in a sense, \"fixed\", because this is the data that we have obtained. On the other hand, w w and b b are the parameters of interest, and we are interested in learning the parameter values for w w and b b that let our model best explain the data . Make Simulated Data To explore this idea in a bit more depth as applied to a linear regression model, let us start by making some simulated data with a bit of injected noise. Exercise: Simulate Data Fill in w_true and b_true with values that you like, or else leave them alone and follow along. from dl_workshop.answers import x , w_true , b_true , noise # exercise: specify ground truth w as w_true. # w_true = ... # exercise: specify ground truth b as b_true # b_true = ... # exercise: write a function to return the linear equation def make_y ( x , w , b ): \"\"\"Your answer here.\"\"\" return None # Comment out my answer below so it doesn't clobber over yours. from dl_workshop.answers import make_y y = make_y ( x , w_true , b_true ) # Plot ground truth data plt . scatter ( x , y ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ); Exercise: Take bad guesses Now, let's plot what would be a very bad estimate of w w and b b . Replace the values assigned to w and b with something of your preference, or feel free to leave them alone and go on. # Plot a very bad estimate w = - 5 # exercise: fill in a bad value for w b = 3 # exercise: fill in a bad value for b y_est = w * x + b # exercise: fill in the equation. plt . plot ( x , y_est , color = 'red' , label = 'bad model' ) plt . scatter ( x , y , label = 'data' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . legend (); Regression Loss Function How bad is our model? We can quantify this by looking at a metric called the \"mean squared error\". The mean squared error is defined as \"the average of the sum of squared errors\". \"Mean squared error\" is but one of many loss functions that are available in deep learning frameworks. It is commonly used for regression tasks. Loss functions are designed to quantify how bad our model is in predicting the data. Exercise: Mean Squared Error Implement the mean squred error function in NumPy code. def mse ( y_true : np . ndarray , y_pred : np . ndarray ) -> float : \"\"\"Implement the function here\"\"\" from dl_workshop.answers import mse # Calculate the mean squared error between print ( mse ( y , y_est )) Activity: Optimize model by hand. Now, we're going to optimize this model by hand. If you're viewing this on the website, I'd encourage you to launch a binder session to play around! import pandas as pd from ipywidgets import interact , FloatSlider import seaborn as sns @interact ( w = FloatSlider ( value = 0 , min =- 10 , max = 10 ), b = FloatSlider ( value = 0 , min =- 10 , max = 30 )) def plot_model ( w , b ): y_est = w * x + b plt . scatter ( x , y ) plt . plot ( x , y_est ) plt . title ( f \"MSE: { mse ( y , y_est ) : .2f } \" ) sns . despine () Loss Minimization As you were optimizing the model, notice what happens to the mean squared error score: it goes down! Implicit in what you were doing is gradient-based optimization. As a \"human\" doing the optimization, you were aware that you needed to move the sliders for w w and b b in particular directions in order to get a best-fit model. The thing we'd like to learn how to do now is to get a computer to automatically perform this procedure . Let's see how to make that happen.","title":"Neural Networks from Scratch"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#neural-networks-from-scratch","text":"In this chapter, we are going to explore differential computing in the place where it was most highly leveraged: the training of neural networks. Now, as with all topics, to learn something most clearly, it pays to have an anchoring example that we start with. In this section, we'll lean heavily on linear regression as that anchoring example . We'll also explore what gradient-based optimization is, see an elementary example of that in action, and then connect those ideas back to optimization of a linear model. Once we're done there, then we'll see the exact same ideas in action with a logistic regression model, before finally seeing them in action again with a neural network model. The big takeaway from this chapter is that basically all supervised learning tasks can be broken into: model loss optimizer Hope you enjoy it! If you're ready, let's take a look at linear regression. import jax.numpy as np from jax import jit import numpy.random as npr import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider from pyprojroot import here","title":"Neural Networks from Scratch"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#linear-regression","text":"Linear regression is foundational to deep learning. It should be a model that everybody has been exposed to before in school. A humorous take I have heard about linear models is that if you zoom in enough into whatever system of the world you're modelling, anything can basically look linear. One of the advantages of linear models is its simplicity. It basically has two parameters, one explaining a \"baseline\" (intercept) and the other explaining strength of relationships (slope). Yet one of the disadvantages of linear models is also its simplicity. A linear model has a strong presumption of linearity. NOTE TO SELF: I need to rewrite this introduction. It is weak.","title":"Linear Regression"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#equation-form","text":"Linear regression, as a model, is expressed as follows: y = wx + b y = wx + b Here: The model is the equation, y = wx + b y = wx + b . y y is the output data. x x is our input data. w w is a slope parameter. b b is our intercept parameter. Implicit in the model is the fact that we have transformed y y by another function, the \"identity\" function, f(x) = x f(x) = x . In this model, y y and x x are, in a sense, \"fixed\", because this is the data that we have obtained. On the other hand, w w and b b are the parameters of interest, and we are interested in learning the parameter values for w w and b b that let our model best explain the data .","title":"Equation Form"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#make-simulated-data","text":"To explore this idea in a bit more depth as applied to a linear regression model, let us start by making some simulated data with a bit of injected noise.","title":"Make Simulated Data"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#exercise-simulate-data","text":"Fill in w_true and b_true with values that you like, or else leave them alone and follow along. from dl_workshop.answers import x , w_true , b_true , noise # exercise: specify ground truth w as w_true. # w_true = ... # exercise: specify ground truth b as b_true # b_true = ... # exercise: write a function to return the linear equation def make_y ( x , w , b ): \"\"\"Your answer here.\"\"\" return None # Comment out my answer below so it doesn't clobber over yours. from dl_workshop.answers import make_y y = make_y ( x , w_true , b_true ) # Plot ground truth data plt . scatter ( x , y ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' );","title":"Exercise: Simulate Data"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#exercise-take-bad-guesses","text":"Now, let's plot what would be a very bad estimate of w w and b b . Replace the values assigned to w and b with something of your preference, or feel free to leave them alone and go on. # Plot a very bad estimate w = - 5 # exercise: fill in a bad value for w b = 3 # exercise: fill in a bad value for b y_est = w * x + b # exercise: fill in the equation. plt . plot ( x , y_est , color = 'red' , label = 'bad model' ) plt . scatter ( x , y , label = 'data' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . legend ();","title":"Exercise: Take bad guesses"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#regression-loss-function","text":"How bad is our model? We can quantify this by looking at a metric called the \"mean squared error\". The mean squared error is defined as \"the average of the sum of squared errors\". \"Mean squared error\" is but one of many loss functions that are available in deep learning frameworks. It is commonly used for regression tasks. Loss functions are designed to quantify how bad our model is in predicting the data.","title":"Regression Loss Function"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#exercise-mean-squared-error","text":"Implement the mean squred error function in NumPy code. def mse ( y_true : np . ndarray , y_pred : np . ndarray ) -> float : \"\"\"Implement the function here\"\"\" from dl_workshop.answers import mse # Calculate the mean squared error between print ( mse ( y , y_est ))","title":"Exercise: Mean Squared Error"},{"location":"01-differential-programming/01-neural-nets-from-scratch/#activity-optimize-model-by-hand","text":"Now, we're going to optimize this model by hand. If you're viewing this on the website, I'd encourage you to launch a binder session to play around! import pandas as pd from ipywidgets import interact , FloatSlider import seaborn as sns @interact ( w = FloatSlider ( value = 0 , min =- 10 , max = 10 ), b = FloatSlider ( value = 0 , min =- 10 , max = 30 )) def plot_model ( w , b ): y_est = w * x + b plt . scatter ( x , y ) plt . plot ( x , y_est ) plt . title ( f \"MSE: { mse ( y , y_est ) : .2f } \" ) sns . despine ()","title":"Activity: Optimize model by hand."},{"location":"01-differential-programming/01-neural-nets-from-scratch/#loss-minimization","text":"As you were optimizing the model, notice what happens to the mean squared error score: it goes down! Implicit in what you were doing is gradient-based optimization. As a \"human\" doing the optimization, you were aware that you needed to move the sliders for w w and b b in particular directions in order to get a best-fit model. The thing we'd like to learn how to do now is to get a computer to automatically perform this procedure . Let's see how to make that happen.","title":"Loss Minimization"},{"location":"01-differential-programming/02-gradient-optimization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Gradient-Based Optimization Implicit in what you were doing was something we formally call \"gradient-based optimization\". This is a very important point to understand. If you get this for a linear model, you will understand how this works for more complex models. Hence, we are going to go into a small crash-course detour on what gradient-based optimization is. Derivatives At the risk of ticking off mathematicians for a sloppy definition, for this book's purposes, a useful way of defining the derivative is: How much our output changes as we take a small step on the inputs, taken in the limit of going to very small steps. If we have a function: f(w) = w^2 + 3w - 5 f(w) = w^2 + 3w - 5 What is the derivative of f(x) f(x) with respect to w w ? From first-year undergraduate calculus, we should be able to calculate this: f'(w) = 2w + 3 f'(w) = 2w + 3 As a matter of style, we will use the apostrophe marks to indicate derivatives. 1 apostrophe mark means first derivative, 2nd apostrophe mark means 2nd derivative, and so on. Minimizing f(w) f(w) Analytically What is the value of w w that minimizes f(w) f(w) ? Again, from undergraduate calculus, we know that at a minima of a function (whether it is a global or local), the first derivative will be equal to zero, i.e. f'(w) = 0 f'(w) = 0 . By taking advantage of this property, we can analytically solve for the value of w w at the minima. 2w + 3 = 0 2w + 3 = 0 Hence, w = -\\frac{3}{2} = 1.5 w = -\\frac{3}{2} = 1.5 To check whether the value of w w at the place where f'(w) = 0 f'(w) = 0 is a minima or maxima, we can use another piece of knowledge from 1st year undergraduate calculus: The sign of the second derivative will tell us whether this is a minima or maxima. If the second derivative is positive regardless of the value of w w , then the point is a minima. (Smiley faces are positive!) If the second derivative is negative regardless of the value of w w , then the point is a maxima. (Frowning faces are negative!) Hence, f''(w) = 2 f''(w) = 2 We can see that f''(w) > 0 f''(w) > 0 for all w w , hence the stationary point we find is going to be a local minima. Minimizing f(w) f(w) Computationally An alternative way of looking at this is to take advantage of f'(w) f'(w) , the gradient, evaluated at a particular w w . A known property of the gradient is that if you take steps in the negative direction of the gradient, you will eventually reach a function's minima. If you take small steps in the positive direction of the gradient, you will reach a function's maxima (if it exists). Exercise: Implement gradient functions by hand Let's implement this using the function f(w) f(w) , done using NumPy. Firstly, implement the aforementioned function f f below. # Exercise: Write f(w) as a function. def f ( w ): \"\"\"Your answer here.\"\"\" return None from dl_workshop.answers import f f ( 2.5 ) This function is the objective function that we wish to optimize, where \"optimization\" means finding the minima or maxima. Now, implement the gradient function \\frac{df}{dw} \\frac{df}{dw} below in the function df : # Exercise: Write df(w) as a function. def df ( w ): \"\"\"Your answer here\"\"\" return None from dl_workshop.answers import df df ( 2.5 ) This function is the gradient of the objective w.r.t. the parameter of interest . It will help us find out the direction in which to change the parameter w w in order to optimize the objective function. Now, pick a number at random to start with. You can specify a number explicitly, or use a random number generator to draw a number. # Exercise: Pick a number to start w at. w = 10.0 # start with a float This gives us a starting point for optimization. Finally, write an \"optimization loop\", in which you adjust the value of w w in the negative direction of the gradient of f f w.r.t. w w (i.e. \\frac{df}{dw} \\frac{df}{dw} ). # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w ) Congratulations, you have just implemented gradient descent ! Gradient descent is an optimization routine : a way of programming a computer to do optimization for you so that you don't have to do it by hand. Minimizing f(w) f(w) with jax jax is a Python package for automatically computing gradients; it provides what is known as an \"automatic differentiation\" system on top of the NumPy API. This way, we do not have to specify the gradient function by hand-calculating it; rather, jax will know how to automatically take the derivative of a Python function w.r.t. the first argument, leveraging the chain rule to help calculate gradients. With jax , our example above is modified in only a slightly different way: from jax import grad import jax from tqdm.autonotebook import tqdm # This is what changes: we use autograd's `grad` function to automatically return a gradient function. df = grad ( f ) # Exercise: Pick a number to start w at. w = - 10.0 # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w ) Summary In this section, we saw one way to program a computer to automatically leverage gradients to find the optima of a polynomial function. This builds our knowledge and intuition for the next section, in which we find the optimal point of a linear regression loss function.","title":"Gradient Based Optimization"},{"location":"01-differential-programming/02-gradient-optimization/#gradient-based-optimization","text":"Implicit in what you were doing was something we formally call \"gradient-based optimization\". This is a very important point to understand. If you get this for a linear model, you will understand how this works for more complex models. Hence, we are going to go into a small crash-course detour on what gradient-based optimization is.","title":"Gradient-Based Optimization"},{"location":"01-differential-programming/02-gradient-optimization/#derivatives","text":"At the risk of ticking off mathematicians for a sloppy definition, for this book's purposes, a useful way of defining the derivative is: How much our output changes as we take a small step on the inputs, taken in the limit of going to very small steps. If we have a function: f(w) = w^2 + 3w - 5 f(w) = w^2 + 3w - 5 What is the derivative of f(x) f(x) with respect to w w ? From first-year undergraduate calculus, we should be able to calculate this: f'(w) = 2w + 3 f'(w) = 2w + 3 As a matter of style, we will use the apostrophe marks to indicate derivatives. 1 apostrophe mark means first derivative, 2nd apostrophe mark means 2nd derivative, and so on.","title":"Derivatives"},{"location":"01-differential-programming/02-gradient-optimization/#minimizing-fwfw-analytically","text":"What is the value of w w that minimizes f(w) f(w) ? Again, from undergraduate calculus, we know that at a minima of a function (whether it is a global or local), the first derivative will be equal to zero, i.e. f'(w) = 0 f'(w) = 0 . By taking advantage of this property, we can analytically solve for the value of w w at the minima. 2w + 3 = 0 2w + 3 = 0 Hence, w = -\\frac{3}{2} = 1.5 w = -\\frac{3}{2} = 1.5 To check whether the value of w w at the place where f'(w) = 0 f'(w) = 0 is a minima or maxima, we can use another piece of knowledge from 1st year undergraduate calculus: The sign of the second derivative will tell us whether this is a minima or maxima. If the second derivative is positive regardless of the value of w w , then the point is a minima. (Smiley faces are positive!) If the second derivative is negative regardless of the value of w w , then the point is a maxima. (Frowning faces are negative!) Hence, f''(w) = 2 f''(w) = 2 We can see that f''(w) > 0 f''(w) > 0 for all w w , hence the stationary point we find is going to be a local minima.","title":"Minimizing f(w)f(w) Analytically"},{"location":"01-differential-programming/02-gradient-optimization/#minimizing-fwfw-computationally","text":"An alternative way of looking at this is to take advantage of f'(w) f'(w) , the gradient, evaluated at a particular w w . A known property of the gradient is that if you take steps in the negative direction of the gradient, you will eventually reach a function's minima. If you take small steps in the positive direction of the gradient, you will reach a function's maxima (if it exists).","title":"Minimizing f(w)f(w) Computationally"},{"location":"01-differential-programming/02-gradient-optimization/#exercise-implement-gradient-functions-by-hand","text":"Let's implement this using the function f(w) f(w) , done using NumPy. Firstly, implement the aforementioned function f f below. # Exercise: Write f(w) as a function. def f ( w ): \"\"\"Your answer here.\"\"\" return None from dl_workshop.answers import f f ( 2.5 ) This function is the objective function that we wish to optimize, where \"optimization\" means finding the minima or maxima. Now, implement the gradient function \\frac{df}{dw} \\frac{df}{dw} below in the function df : # Exercise: Write df(w) as a function. def df ( w ): \"\"\"Your answer here\"\"\" return None from dl_workshop.answers import df df ( 2.5 ) This function is the gradient of the objective w.r.t. the parameter of interest . It will help us find out the direction in which to change the parameter w w in order to optimize the objective function. Now, pick a number at random to start with. You can specify a number explicitly, or use a random number generator to draw a number. # Exercise: Pick a number to start w at. w = 10.0 # start with a float This gives us a starting point for optimization. Finally, write an \"optimization loop\", in which you adjust the value of w w in the negative direction of the gradient of f f w.r.t. w w (i.e. \\frac{df}{dw} \\frac{df}{dw} ). # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w ) Congratulations, you have just implemented gradient descent ! Gradient descent is an optimization routine : a way of programming a computer to do optimization for you so that you don't have to do it by hand.","title":"Exercise: Implement gradient functions by hand"},{"location":"01-differential-programming/02-gradient-optimization/#minimizing-fwfw-with-jax","text":"jax is a Python package for automatically computing gradients; it provides what is known as an \"automatic differentiation\" system on top of the NumPy API. This way, we do not have to specify the gradient function by hand-calculating it; rather, jax will know how to automatically take the derivative of a Python function w.r.t. the first argument, leveraging the chain rule to help calculate gradients. With jax , our example above is modified in only a slightly different way: from jax import grad import jax from tqdm.autonotebook import tqdm # This is what changes: we use autograd's `grad` function to automatically return a gradient function. df = grad ( f ) # Exercise: Pick a number to start w at. w = - 10.0 # Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient. for i in range ( 1000 ): w = w - df ( w ) * 0.01 # 0.01 is the size of the step taken. print ( w )","title":"Minimizing f(w)f(w) with jax"},{"location":"01-differential-programming/02-gradient-optimization/#summary","text":"In this section, we saw one way to program a computer to automatically leverage gradients to find the optima of a polynomial function. This builds our knowledge and intuition for the next section, in which we find the optimal point of a linear regression loss function.","title":"Summary"},{"location":"01-differential-programming/03-linear-model-optimization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Optimizing Linear Models What are we optimizing? In linear regression, we are: minimizing (i.e. optimizing) the loss function with respect to the linear regression parameters. Here are the parallels to the example above: In the example above, we minimized f(w) f(w) , the polynomial function. With linear regression, we are minimizing the mean squared error. In the example above, we minimized f(w) f(w) with respect to w w , where w w is the key parameter of f f . With linear regression, we minimize mean squared error of our model prediction with respect to the linear regression parameters. (Let's call the parameters collectively \\theta \\theta , such that \\theta = (w, b) \\theta = (w, b) . Ingredients for \"Optimizing\" a Model At this point, we have learned what the ingredients are for optimizing a model: A model, which is a function that maps inputs x x to outputs y y , and its parameters of the model. Not to belabour the point, but in our linear regression case, this is w w and b b ; Usually, in the literature, we call this parameter set \\theta \\theta , such that \\theta \\theta encompasses all parameters of the model. Loss function, which tells us how bad our predictions are. Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function. Keep note: Because we are optimizing the loss w.r.t. two parameters, finding the w w and b b coordinates that minimize the loss is like finding the minima of a bowl. The latter point, which is \"how to adjust the parameter values to minimize the loss function\", is the key point to understand here. Writing this in JAX/NumPy How do we optimize the parameters of our linear regression model using JAX? Let's explore how to do this. Exercise: Define the linear regression model Firstly, let's define our model function. Write it out as a Python function, named linear_model , such that the parameters \\theta \\theta are the first argument, and the data x are the second argument. It should return the model prediction. What should the data type of \\theta \\theta be? You can decide, as long as it's a built-in Python data type, or NumPy data type, or some combination of. # Exercise: Define the model in this function def linear_model ( theta , x ): pass from dl_workshop.answers import linear_model Exercise: Initialize linear regression model parameters using random numbers Using a random number generator, such as the numpy.random.normal function, write a function that returns random number starting points for each linear model parameter. Make sure it returns params in the form that are accepted by the linear_model function defined above. Hint: NumPy's random module (which is distinct from JAX's) has been imported for you in the namespace npr . def initialize_linear_params (): pass # Comment this out if you fill in your answer above. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params () Exercise: Define the mean squared error loss function with linear model parameters as first argument Now, define the mean squared error loss function, called mseloss , such that 1. the parameters \\theta \\theta are accepted as the first argument, 2. model function as the second argument, 3. x as the third argument, 4. y as the fourth argument, and 5. returns a scalar valued result. This is the function we will be differentiating, and JAX's grad function will take the derivative of the function w.r.t. the first argument. Thus, \\theta \\theta must be the first argument! # Differentiable loss function w.r.t. 1st argument def mseloss ( theta , model , x , y ): pass from dl_workshop.answers import mseloss Now, we generate a new function called dmseloss , by calling grad on mseloss ! The new function dmseloss will have the exact same signature as mseloss , but will instead return the value of the gradient evaluated at each of the parameters in \\theta \\theta , in the same data structure as \\theta \\theta . # Put your answer here. # The actual dmseloss function is also present in the answers, # but _seriously_, go fill the one-liner to get dmseloss defined! # If you fill out the one-liner above, # remember to comment out the answer below # so that mine doesn't clobber over yours! from dl_workshop.answers import dmseloss I've provided an execution of the function below, so that you have an intuition of what's being returned. In my implementation, because theta are passed in as a 2-tuple, the gradients are returned as a 2-tuple as well. The return type will match up with how you pass in the parameters. from dl_workshop.answers import x , make_y , b_true , w_true # Create y by replacing my b_true and w_true with whatever you want y = make_y ( x , w_true , b_true ) dmseloss ( dict ( w = 0.3 , b = 0.5 ), linear_model , x , y ) Exercise: Write the optimization routine Finally, write the optimization routine! Make it run for 3,000 iterations, and record the loss on each iteration. Don't forget to update your parameters! (How you do so will depend on how you've set up the parameters.) # Write your optimization routine below. # And if you implemented your optimization loop, # feel free to comment out the next two lines from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , linear_model , mseloss , x , y , n_steps = 3000 ) Now, let's plot the loss score over time. It should be going downwards. import matplotlib.pyplot as plt plt . plot ( losses ) plt . xlabel ( 'iteration' ) plt . ylabel ( 'mse' ); Inspect your parameters to see if they've become close to the true values! print ( theta ) Summary Ingredients of Linear Model From these first three sections, have seen how the following components play inside a linear model: Model specification (\"equations\", e.g. y = wx + b y = wx + b ) and the parameters of the model to be optimized ( w w and b b , or more generally, \\theta \\theta ). Loss function: tells us how wrong our model parameters are w.r.t. the data ( MSE MSE ) Optimization routine (for-loop) Let's now explore a few pictorial representations of the model. Linear Regression In Pictures Linear regression can be expressed pictorially, not just in equation form. Here are two ways of visualizing linear regression. Matrix Form Linear regression in one dimension looks like this: Linear regression in higher dimensions looks like this: This is also known in the statistical world as \"multiple linear regression\". The general idea, though, should be pretty easy to catch. You can do linear regression that projects any arbitrary number of input dimensions to any arbitrary number of output dimensions. Neural Diagram We can draw a \"neural diagram\" based on the matrix view, with the implicit \"identity\" function included in orange. The neural diagram is one that we commonly see in the introductions to deep learning. As you can see here, linear regression, when visualized this way, can be conceptually thought of as the baseline model for understanding deep learning. The neural diagram also expresses the \"compute graph\" that transforms input variables to output variables.","title":"Linear Model"},{"location":"01-differential-programming/03-linear-model-optimization/#optimizing-linear-models","text":"","title":"Optimizing Linear Models"},{"location":"01-differential-programming/03-linear-model-optimization/#what-are-we-optimizing","text":"In linear regression, we are: minimizing (i.e. optimizing) the loss function with respect to the linear regression parameters. Here are the parallels to the example above: In the example above, we minimized f(w) f(w) , the polynomial function. With linear regression, we are minimizing the mean squared error. In the example above, we minimized f(w) f(w) with respect to w w , where w w is the key parameter of f f . With linear regression, we minimize mean squared error of our model prediction with respect to the linear regression parameters. (Let's call the parameters collectively \\theta \\theta , such that \\theta = (w, b) \\theta = (w, b) .","title":"What are we optimizing?"},{"location":"01-differential-programming/03-linear-model-optimization/#ingredients-for-optimizing-a-model","text":"At this point, we have learned what the ingredients are for optimizing a model: A model, which is a function that maps inputs x x to outputs y y , and its parameters of the model. Not to belabour the point, but in our linear regression case, this is w w and b b ; Usually, in the literature, we call this parameter set \\theta \\theta , such that \\theta \\theta encompasses all parameters of the model. Loss function, which tells us how bad our predictions are. Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function. Keep note: Because we are optimizing the loss w.r.t. two parameters, finding the w w and b b coordinates that minimize the loss is like finding the minima of a bowl. The latter point, which is \"how to adjust the parameter values to minimize the loss function\", is the key point to understand here.","title":"Ingredients for \"Optimizing\" a Model"},{"location":"01-differential-programming/03-linear-model-optimization/#writing-this-in-jaxnumpy","text":"How do we optimize the parameters of our linear regression model using JAX? Let's explore how to do this.","title":"Writing this in JAX/NumPy"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-define-the-linear-regression-model","text":"Firstly, let's define our model function. Write it out as a Python function, named linear_model , such that the parameters \\theta \\theta are the first argument, and the data x are the second argument. It should return the model prediction. What should the data type of \\theta \\theta be? You can decide, as long as it's a built-in Python data type, or NumPy data type, or some combination of. # Exercise: Define the model in this function def linear_model ( theta , x ): pass from dl_workshop.answers import linear_model","title":"Exercise: Define the linear regression model"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-initialize-linear-regression-model-parameters-using-random-numbers","text":"Using a random number generator, such as the numpy.random.normal function, write a function that returns random number starting points for each linear model parameter. Make sure it returns params in the form that are accepted by the linear_model function defined above. Hint: NumPy's random module (which is distinct from JAX's) has been imported for you in the namespace npr . def initialize_linear_params (): pass # Comment this out if you fill in your answer above. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params ()","title":"Exercise: Initialize linear regression model parameters using random numbers"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-define-the-mean-squared-error-loss-function-with-linear-model-parameters-as-first-argument","text":"Now, define the mean squared error loss function, called mseloss , such that 1. the parameters \\theta \\theta are accepted as the first argument, 2. model function as the second argument, 3. x as the third argument, 4. y as the fourth argument, and 5. returns a scalar valued result. This is the function we will be differentiating, and JAX's grad function will take the derivative of the function w.r.t. the first argument. Thus, \\theta \\theta must be the first argument! # Differentiable loss function w.r.t. 1st argument def mseloss ( theta , model , x , y ): pass from dl_workshop.answers import mseloss Now, we generate a new function called dmseloss , by calling grad on mseloss ! The new function dmseloss will have the exact same signature as mseloss , but will instead return the value of the gradient evaluated at each of the parameters in \\theta \\theta , in the same data structure as \\theta \\theta . # Put your answer here. # The actual dmseloss function is also present in the answers, # but _seriously_, go fill the one-liner to get dmseloss defined! # If you fill out the one-liner above, # remember to comment out the answer below # so that mine doesn't clobber over yours! from dl_workshop.answers import dmseloss I've provided an execution of the function below, so that you have an intuition of what's being returned. In my implementation, because theta are passed in as a 2-tuple, the gradients are returned as a 2-tuple as well. The return type will match up with how you pass in the parameters. from dl_workshop.answers import x , make_y , b_true , w_true # Create y by replacing my b_true and w_true with whatever you want y = make_y ( x , w_true , b_true ) dmseloss ( dict ( w = 0.3 , b = 0.5 ), linear_model , x , y )","title":"Exercise: Define the mean squared error loss function with linear model parameters as first argument"},{"location":"01-differential-programming/03-linear-model-optimization/#exercise-write-the-optimization-routine","text":"Finally, write the optimization routine! Make it run for 3,000 iterations, and record the loss on each iteration. Don't forget to update your parameters! (How you do so will depend on how you've set up the parameters.) # Write your optimization routine below. # And if you implemented your optimization loop, # feel free to comment out the next two lines from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , linear_model , mseloss , x , y , n_steps = 3000 ) Now, let's plot the loss score over time. It should be going downwards. import matplotlib.pyplot as plt plt . plot ( losses ) plt . xlabel ( 'iteration' ) plt . ylabel ( 'mse' ); Inspect your parameters to see if they've become close to the true values! print ( theta )","title":"Exercise: Write the optimization routine"},{"location":"01-differential-programming/03-linear-model-optimization/#summary","text":"","title":"Summary"},{"location":"01-differential-programming/03-linear-model-optimization/#ingredients-of-linear-model","text":"From these first three sections, have seen how the following components play inside a linear model: Model specification (\"equations\", e.g. y = wx + b y = wx + b ) and the parameters of the model to be optimized ( w w and b b , or more generally, \\theta \\theta ). Loss function: tells us how wrong our model parameters are w.r.t. the data ( MSE MSE ) Optimization routine (for-loop) Let's now explore a few pictorial representations of the model.","title":"Ingredients of  Linear Model"},{"location":"01-differential-programming/03-linear-model-optimization/#linear-regression-in-pictures","text":"Linear regression can be expressed pictorially, not just in equation form. Here are two ways of visualizing linear regression.","title":"Linear Regression In Pictures"},{"location":"01-differential-programming/03-linear-model-optimization/#matrix-form","text":"Linear regression in one dimension looks like this: Linear regression in higher dimensions looks like this: This is also known in the statistical world as \"multiple linear regression\". The general idea, though, should be pretty easy to catch. You can do linear regression that projects any arbitrary number of input dimensions to any arbitrary number of output dimensions.","title":"Matrix Form"},{"location":"01-differential-programming/03-linear-model-optimization/#neural-diagram","text":"We can draw a \"neural diagram\" based on the matrix view, with the implicit \"identity\" function included in orange. The neural diagram is one that we commonly see in the introductions to deep learning. As you can see here, linear regression, when visualized this way, can be conceptually thought of as the baseline model for understanding deep learning. The neural diagram also expresses the \"compute graph\" that transforms input variables to output variables.","title":"Neural Diagram"},{"location":"01-differential-programming/04-logistic-regression/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Logistic Regression Logistic regression builds upon linear regression. We use logistic regression to perform binary classification , that is, distinguishing between two classes. Typically, we label one of the classes with the integer 0, and the other class with the integer 1. What does the model look like? To help you build intuition, let's visualize logistic regression using pictures again. Matrix Form Here is logistic regression in matrix form. Neural Diagram Now, here's logistic regression in a neural diagram: Interactive Activity As should be evident from the pictures, logistic regression builds upon linear regression simply by changing the activation function from an \"identity\" function to a \"logistic\" function . In the one-dimensional case, it has the same two parameters as one-dimensional linear regression, w w and b b . Let's use an interactive visualization to visualize how the parameters w w and b b affect the shape of the curve. (Note: this exercise is best done in a live notebook!) from dl_workshop.answers import logistic logistic ?? import jax.numpy as np import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider @interact ( w = FloatSlider ( value = 0 , min =- 5 , max = 5 ), b = FloatSlider ( value = 0 , min =- 5 , max = 5 )) def plot_logistic ( w , b ): x = np . linspace ( - 10 , 10 , 1000 ) z = w * x + b # linear transform on x y = logistic ( z ) plt . plot ( x , y ) Parameters of the model As we can see, there are two parameters w w and b b . For those who may be encountering the model for the first time, this is what each of them control: w w controls how steep the step function is between 0 and 1 on the y-axis. Its sign also controls whether class 1 is associated with smaller values or larger values. b b controls the midpoint location of the curve. More negative values of b b shift it to the left; more positive values of b b shift it to the right. Make simulated data Once again, we are going to use simulated data to help us anchor our understanding. Along the way, we will see how logistic regression, once again, fits inside the same framework of \"model, loss, optimizer\". import numpy.random as npr x = np . linspace ( - 5 , 5 , 100 ) w = 2 b = 1 z = w * x + b + npr . random ( size = len ( x )) y_true = np . round ( logistic ( z )) plt . scatter ( x , y_true , alpha = 0.3 ); Here, we set w w to 2 and b b to 1, added some noise in there too, and rounded off the logistic-transformed values to between 0 and 1. Binary Classification Loss Function How would we quantify how good or bad our model is? In this case, we use the logistic loss function, also known as the binary cross entropy loss function. Expressed in equation form, it looks like this: L = -\\sum (y \\log(p) + (1-y)\\log(1-p) L = -\\sum (y \\log(p) + (1-y)\\log(1-p) Here: y y is the actual class, namely 1 1 or 0 0 . p p is the predicted class. If you're staring at this equation, and thinking that it looks a lot like the Bernoulli distribution log likelihood, you are right! Discussion Let's think about the loss function for a moment: What happens to the term y \\log(p) y \\log(p) when y=0 y=0 and y=1 y=1 ? What about the (1-y)\\log(1-p) (1-y)\\log(1-p) term? What happens to both terms when p \\approx 0 p \\approx 0 and when p \\approx 1 p \\approx 1 (but still bounded between 0 and 1)? The answers are as follows: When y=0 y=0 , $y \\log(p) = $, and when y=1 y=1 , (1-y)\\log(1-p) = 0 (1-y)\\log(1-p) = 0 . When p \\rightarrow 0 p \\rightarrow 0 , then \\log(p) \\log(p) approaches negative infinity. Likewise for \\log(1-p) \\log(1-p) when p \\rightarrow 1 p \\rightarrow 1 Exercise: Write down the logistic regression model Using the same patterns as you did before for the linear model, define a function called logistic_model , which accepts parameters theta and data x . # Exercise: Define logistic model def logistic_model ( theta , x ): pass from dl_workshop.answers import logistic_model Exercise: Write down the logistic loss function Now, write down the logistic loss function. It is defined as the negative binary cross entropy between the ground truth and the predicted. The binary cross entropy function is available for you to use: from dl_workshop.answers import binary_cross_entropy binary_cross_entropy ?? # Exercise: Define logistic loss function def logistic_loss ( params , model , x , y ): pass from dl_workshop.answers import logistic_loss logistic_loss ?? Now define the gradient of the loss function, using grad ! from jax import grad from dl_workshop.answers import dlogistic_loss # Exercise: Define gradient of loss function. # dlogistic_loss = ... Exercise: Initialize logistic regression model parameters using random numbers Because the parameters are identical to linear regression, you probably can use the same initialize_linear_params function. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params () theta Exercise: Write the training loop! This will look very similar to the linear model training loop, because the same two parameters are being optimized. The thing that should change is the loss function and gradient of the loss function. from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , logistic_model , logistic_loss , x , y_true , n_steps = 5000 , step_size = 0.0001 ) print ( theta ) You'll notice that the values are off from the true value. Why is this so? Partly it's because of the noise that we added, and we also rounded off values. Let's also print out the losses to check that \"learning\" has happened. plt . plot ( losses ); We might argue that the model hasn't yet converged, so we haven't yet figured out the parameters that best explain the data, given the model. And finally, checking the model against the actual values: plt . scatter ( x , y_true , alpha = 0.3 ) plt . plot ( x , logistic_model ( theta , x ), color = 'red' ); Indeed, we might say that the model parameters could have been optimized further, but as it stands, I'd say we did a pretty good job. Exercise What if we did not round off the values, and did not add noise to the original data? Try re-running the model without those two. Summary Let's recap what we learned here. We saw that logistic regression is nothing more than a natural extension of linear regression. We saw the introduction of the logistic loss function, and some of its properties. We finally saw that we could optimize a model, leveraging the same grad function from JAX. To reinforce point 1, let's look at logistic regression in matrix form again. See how there is an extra function g g (in yellow), which is the logistic function, that is tacked on. To further reinforce the ideas, we should look at the neural diagram once more. Once again, it's linear model + one more function. Remember this pattern: it will make neural networks much clearer in the next section!","title":"Logistic Regression"},{"location":"01-differential-programming/04-logistic-regression/#logistic-regression","text":"Logistic regression builds upon linear regression. We use logistic regression to perform binary classification , that is, distinguishing between two classes. Typically, we label one of the classes with the integer 0, and the other class with the integer 1. What does the model look like? To help you build intuition, let's visualize logistic regression using pictures again.","title":"Logistic Regression"},{"location":"01-differential-programming/04-logistic-regression/#matrix-form","text":"Here is logistic regression in matrix form.","title":"Matrix Form"},{"location":"01-differential-programming/04-logistic-regression/#neural-diagram","text":"Now, here's logistic regression in a neural diagram:","title":"Neural Diagram"},{"location":"01-differential-programming/04-logistic-regression/#interactive-activity","text":"As should be evident from the pictures, logistic regression builds upon linear regression simply by changing the activation function from an \"identity\" function to a \"logistic\" function . In the one-dimensional case, it has the same two parameters as one-dimensional linear regression, w w and b b . Let's use an interactive visualization to visualize how the parameters w w and b b affect the shape of the curve. (Note: this exercise is best done in a live notebook!) from dl_workshop.answers import logistic logistic ?? import jax.numpy as np import matplotlib.pyplot as plt from ipywidgets import interact , FloatSlider @interact ( w = FloatSlider ( value = 0 , min =- 5 , max = 5 ), b = FloatSlider ( value = 0 , min =- 5 , max = 5 )) def plot_logistic ( w , b ): x = np . linspace ( - 10 , 10 , 1000 ) z = w * x + b # linear transform on x y = logistic ( z ) plt . plot ( x , y )","title":"Interactive Activity"},{"location":"01-differential-programming/04-logistic-regression/#parameters-of-the-model","text":"As we can see, there are two parameters w w and b b . For those who may be encountering the model for the first time, this is what each of them control: w w controls how steep the step function is between 0 and 1 on the y-axis. Its sign also controls whether class 1 is associated with smaller values or larger values. b b controls the midpoint location of the curve. More negative values of b b shift it to the left; more positive values of b b shift it to the right.","title":"Parameters of the model"},{"location":"01-differential-programming/04-logistic-regression/#make-simulated-data","text":"Once again, we are going to use simulated data to help us anchor our understanding. Along the way, we will see how logistic regression, once again, fits inside the same framework of \"model, loss, optimizer\". import numpy.random as npr x = np . linspace ( - 5 , 5 , 100 ) w = 2 b = 1 z = w * x + b + npr . random ( size = len ( x )) y_true = np . round ( logistic ( z )) plt . scatter ( x , y_true , alpha = 0.3 ); Here, we set w w to 2 and b b to 1, added some noise in there too, and rounded off the logistic-transformed values to between 0 and 1.","title":"Make simulated data"},{"location":"01-differential-programming/04-logistic-regression/#binary-classification-loss-function","text":"How would we quantify how good or bad our model is? In this case, we use the logistic loss function, also known as the binary cross entropy loss function. Expressed in equation form, it looks like this: L = -\\sum (y \\log(p) + (1-y)\\log(1-p) L = -\\sum (y \\log(p) + (1-y)\\log(1-p) Here: y y is the actual class, namely 1 1 or 0 0 . p p is the predicted class. If you're staring at this equation, and thinking that it looks a lot like the Bernoulli distribution log likelihood, you are right!","title":"Binary Classification Loss Function"},{"location":"01-differential-programming/04-logistic-regression/#discussion","text":"Let's think about the loss function for a moment: What happens to the term y \\log(p) y \\log(p) when y=0 y=0 and y=1 y=1 ? What about the (1-y)\\log(1-p) (1-y)\\log(1-p) term? What happens to both terms when p \\approx 0 p \\approx 0 and when p \\approx 1 p \\approx 1 (but still bounded between 0 and 1)? The answers are as follows: When y=0 y=0 , $y \\log(p) = $, and when y=1 y=1 , (1-y)\\log(1-p) = 0 (1-y)\\log(1-p) = 0 . When p \\rightarrow 0 p \\rightarrow 0 , then \\log(p) \\log(p) approaches negative infinity. Likewise for \\log(1-p) \\log(1-p) when p \\rightarrow 1 p \\rightarrow 1","title":"Discussion"},{"location":"01-differential-programming/04-logistic-regression/#exercise-write-down-the-logistic-regression-model","text":"Using the same patterns as you did before for the linear model, define a function called logistic_model , which accepts parameters theta and data x . # Exercise: Define logistic model def logistic_model ( theta , x ): pass from dl_workshop.answers import logistic_model","title":"Exercise: Write down the logistic regression model"},{"location":"01-differential-programming/04-logistic-regression/#exercise-write-down-the-logistic-loss-function","text":"Now, write down the logistic loss function. It is defined as the negative binary cross entropy between the ground truth and the predicted. The binary cross entropy function is available for you to use: from dl_workshop.answers import binary_cross_entropy binary_cross_entropy ?? # Exercise: Define logistic loss function def logistic_loss ( params , model , x , y ): pass from dl_workshop.answers import logistic_loss logistic_loss ?? Now define the gradient of the loss function, using grad ! from jax import grad from dl_workshop.answers import dlogistic_loss # Exercise: Define gradient of loss function. # dlogistic_loss = ...","title":"Exercise: Write down the logistic loss function"},{"location":"01-differential-programming/04-logistic-regression/#exercise-initialize-logistic-regression-model-parameters-using-random-numbers","text":"Because the parameters are identical to linear regression, you probably can use the same initialize_linear_params function. from dl_workshop.answers import initialize_linear_params theta = initialize_linear_params () theta","title":"Exercise: Initialize logistic regression model parameters using random numbers"},{"location":"01-differential-programming/04-logistic-regression/#exercise-write-the-training-loop","text":"This will look very similar to the linear model training loop, because the same two parameters are being optimized. The thing that should change is the loss function and gradient of the loss function. from dl_workshop.answers import model_optimization_loop losses , theta = model_optimization_loop ( theta , logistic_model , logistic_loss , x , y_true , n_steps = 5000 , step_size = 0.0001 ) print ( theta ) You'll notice that the values are off from the true value. Why is this so? Partly it's because of the noise that we added, and we also rounded off values. Let's also print out the losses to check that \"learning\" has happened. plt . plot ( losses ); We might argue that the model hasn't yet converged, so we haven't yet figured out the parameters that best explain the data, given the model. And finally, checking the model against the actual values: plt . scatter ( x , y_true , alpha = 0.3 ) plt . plot ( x , logistic_model ( theta , x ), color = 'red' ); Indeed, we might say that the model parameters could have been optimized further, but as it stands, I'd say we did a pretty good job.","title":"Exercise: Write the training loop!"},{"location":"01-differential-programming/04-logistic-regression/#exercise","text":"What if we did not round off the values, and did not add noise to the original data? Try re-running the model without those two.","title":"Exercise"},{"location":"01-differential-programming/04-logistic-regression/#summary","text":"Let's recap what we learned here. We saw that logistic regression is nothing more than a natural extension of linear regression. We saw the introduction of the logistic loss function, and some of its properties. We finally saw that we could optimize a model, leveraging the same grad function from JAX. To reinforce point 1, let's look at logistic regression in matrix form again. See how there is an extra function g g (in yellow), which is the logistic function, that is tacked on. To further reinforce the ideas, we should look at the neural diagram once more. Once again, it's linear model + one more function. Remember this pattern: it will make neural networks much clearer in the next section!","title":"Summary"},{"location":"01-differential-programming/05-neural-networks/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Neural Networks Neural networks are basically very powerful versions of logistic regressions. Like linear and logistic regression, they also take our data and map it to some output, but does so without ever knowing what the true equation form is. That's all a neural network model is: an arbitrarily powerful model. How do feed forward neural networks look like? To give you an intuition for this, let's see one example of a deep neural network in pictures. Pictorial form Matrix diagram As usual, in a matrix diagram. If this looks like a stack of logistic regression-like models, then you're right! This is the essence of what a neural network model is underneath the hood. Neural diagram And for correspondence, let's visualize this in a neural diagram. There's some syntax/nomenclature that we need to introduce here. Notice how w_1 w_1 reshapes our data to go from 4 inputs to 3 outputs. In neural network lingo, we call the 4 inputs \"4 input nodes\", and the 3 outputs \"3 hidden nodes\". If you are familiar with linear algebra operations, you'll know that this operation is a projection of data from 4 dimensions to 3 dimensions. The second set of weights, w_2 w_2 , take us from 3 dimensions to 1, and the 1 dimension at the end of the relu function is called the \"output node\". The orange functions are called activation functions , and they are a transformation on the linear projection steps (red and blue) that precede them. We've drawn the computation graph in a very explicit fashion, documenting every math transform in there. However, in the literature, you'll find that most authors omit the blue and orange steps, and instead leave them as implicit in their model illustrations, especially when they have, as a modelling choice, opted for identical activation functions. Using neural networks on some real data We are going to try using some real data from the UCI Machine Learning Repository to something related to the work that I am engaged in: predicting molecular properties from molecule descriptors. With the dataset below, we want to predict whether a compound is biodegradable based on a series of 41 chemical descriptors. This is a classical \"classification\" problem, where the output is a 1/0 result, much like the logistic regression problem from before. The dataset was taken from: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation#. I have also prepared the data such that it is split into X (the predictors) and Y (the class that we are trying to predict), so that you do not have to worry about manipulating pandas DataFrames. Let's read in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 ) Neural network model definition Now, let's write a neural network model. The neural network model that we'll design must start with 41 input nodes, because there are 41 input values. As a modelling choice, we might decide to have 1 hidden layer with 20 nodes. Generally, this is arbitrary, but one general rule-of-thumb is to compress the data projection with fewer outputs than inputs. Finally, we must have an output layer with 1 node, as there is only column of data to predict on. Because this is a classification problem, we will use a logistic activation function right at the end. We'll start by instantiating a bunch of parameters. from dl_workshop.answers import noise params = dict () params [ 'w1' ] = noise (( 41 , 20 )) params [ 'b1' ] = noise (( 20 ,)) params [ 'w2' ] = noise (( 20 , 1 )) params [ 'b2' ] = noise (( 1 ,)) Now, let's define the model as a Python function: import jax.numpy as np from dl_workshop.answers import logistic def neural_network_model ( theta , x ): # \"a1\" is the activation from layer 1 a1 = np . tanh ( np . dot ( x , theta [ 'w1' ]) + theta [ 'b1' ]) # \"a2\" is the activation from layer 2. a2 = logistic ( np . dot ( a1 , theta [ 'w2' ]) + theta [ 'b2' ]) return a2 Why do we need a \"logistic\" function at the end, rather than follow what was in the diagram above (relu)? This is because we are doing a classification problem, therefore we must squash the output to be between 0 and 1. Optimization loop Now, write the optimization loop! It will look very similar to the optimization loop that we wrote for the logistic regression classification model. The difference here is the model that is used, as well as the initialized set of parameters. from dl_workshop.answers import model_optimization_loop , logistic_loss losses , params = model_optimization_loop ( params , neural_network_model , logistic_loss , X . values , y . values , step_size = 0.0001 ) import matplotlib.pyplot as plt plt . plot ( losses ) plt . title ( f \"final loss: { losses [ - 1 ] : .2f } \" ); Visualize trained model performance We can use a confusion matrix to see how \"confused\" a model was. Read more on Wikipedia . from sklearn.metrics import confusion_matrix y_pred = neural_network_model ( params , X . values ) confusion_matrix ( y , np . round ( y_pred )) import seaborn as sns sns . heatmap ( confusion_matrix ( y , np . round ( y_pred ))) plt . xlabel ( 'predicted' ) plt . ylabel ( 'actual' ); Recap Deep learning, and more generally any modelling, has the following ingredients: A model and its associated parameters to be optimized. A loss function against which we are optimizing parameters. An optimization routine. You have seen these three ingredients at play with 3 different models: a linear regression model, a logistic regression model, and a deep feed forward neural network model. In Pictures Here is a summary of what we've learned in this tutorial! Caveats thus far Deep learning is an active field of research. I have only shown you the basics here. In addition, I have also intentionally omitted certain aspects of machine learning practice, such as splitting our data into training and testing sets, performing model selection using cross-validation tuning hyperparameters, such as trying out optimizers regularizing the model, using L1/L2 regularization or dropout etc. Parting Thoughts Deep learning is nothing more than optimization of a model with a really large number of parameters. In its current state, it is not artificial intelligence. You should not be afraid of it; it is nothing more than a really powerful model that maps X to Y.","title":"Neural Networks"},{"location":"01-differential-programming/05-neural-networks/#neural-networks","text":"Neural networks are basically very powerful versions of logistic regressions. Like linear and logistic regression, they also take our data and map it to some output, but does so without ever knowing what the true equation form is. That's all a neural network model is: an arbitrarily powerful model. How do feed forward neural networks look like? To give you an intuition for this, let's see one example of a deep neural network in pictures.","title":"Neural Networks"},{"location":"01-differential-programming/05-neural-networks/#pictorial-form","text":"","title":"Pictorial form"},{"location":"01-differential-programming/05-neural-networks/#matrix-diagram","text":"As usual, in a matrix diagram. If this looks like a stack of logistic regression-like models, then you're right! This is the essence of what a neural network model is underneath the hood.","title":"Matrix diagram"},{"location":"01-differential-programming/05-neural-networks/#neural-diagram","text":"And for correspondence, let's visualize this in a neural diagram. There's some syntax/nomenclature that we need to introduce here. Notice how w_1 w_1 reshapes our data to go from 4 inputs to 3 outputs. In neural network lingo, we call the 4 inputs \"4 input nodes\", and the 3 outputs \"3 hidden nodes\". If you are familiar with linear algebra operations, you'll know that this operation is a projection of data from 4 dimensions to 3 dimensions. The second set of weights, w_2 w_2 , take us from 3 dimensions to 1, and the 1 dimension at the end of the relu function is called the \"output node\". The orange functions are called activation functions , and they are a transformation on the linear projection steps (red and blue) that precede them. We've drawn the computation graph in a very explicit fashion, documenting every math transform in there. However, in the literature, you'll find that most authors omit the blue and orange steps, and instead leave them as implicit in their model illustrations, especially when they have, as a modelling choice, opted for identical activation functions.","title":"Neural diagram"},{"location":"01-differential-programming/05-neural-networks/#using-neural-networks-on-some-real-data","text":"We are going to try using some real data from the UCI Machine Learning Repository to something related to the work that I am engaged in: predicting molecular properties from molecule descriptors. With the dataset below, we want to predict whether a compound is biodegradable based on a series of 41 chemical descriptors. This is a classical \"classification\" problem, where the output is a 1/0 result, much like the logistic regression problem from before. The dataset was taken from: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation#. I have also prepared the data such that it is split into X (the predictors) and Y (the class that we are trying to predict), so that you do not have to worry about manipulating pandas DataFrames. Let's read in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 )","title":"Using neural networks on some real data"},{"location":"01-differential-programming/05-neural-networks/#neural-network-model-definition","text":"Now, let's write a neural network model. The neural network model that we'll design must start with 41 input nodes, because there are 41 input values. As a modelling choice, we might decide to have 1 hidden layer with 20 nodes. Generally, this is arbitrary, but one general rule-of-thumb is to compress the data projection with fewer outputs than inputs. Finally, we must have an output layer with 1 node, as there is only column of data to predict on. Because this is a classification problem, we will use a logistic activation function right at the end. We'll start by instantiating a bunch of parameters. from dl_workshop.answers import noise params = dict () params [ 'w1' ] = noise (( 41 , 20 )) params [ 'b1' ] = noise (( 20 ,)) params [ 'w2' ] = noise (( 20 , 1 )) params [ 'b2' ] = noise (( 1 ,)) Now, let's define the model as a Python function: import jax.numpy as np from dl_workshop.answers import logistic def neural_network_model ( theta , x ): # \"a1\" is the activation from layer 1 a1 = np . tanh ( np . dot ( x , theta [ 'w1' ]) + theta [ 'b1' ]) # \"a2\" is the activation from layer 2. a2 = logistic ( np . dot ( a1 , theta [ 'w2' ]) + theta [ 'b2' ]) return a2 Why do we need a \"logistic\" function at the end, rather than follow what was in the diagram above (relu)? This is because we are doing a classification problem, therefore we must squash the output to be between 0 and 1.","title":"Neural network model definition"},{"location":"01-differential-programming/05-neural-networks/#optimization-loop","text":"Now, write the optimization loop! It will look very similar to the optimization loop that we wrote for the logistic regression classification model. The difference here is the model that is used, as well as the initialized set of parameters. from dl_workshop.answers import model_optimization_loop , logistic_loss losses , params = model_optimization_loop ( params , neural_network_model , logistic_loss , X . values , y . values , step_size = 0.0001 ) import matplotlib.pyplot as plt plt . plot ( losses ) plt . title ( f \"final loss: { losses [ - 1 ] : .2f } \" );","title":"Optimization loop"},{"location":"01-differential-programming/05-neural-networks/#visualize-trained-model-performance","text":"We can use a confusion matrix to see how \"confused\" a model was. Read more on Wikipedia . from sklearn.metrics import confusion_matrix y_pred = neural_network_model ( params , X . values ) confusion_matrix ( y , np . round ( y_pred )) import seaborn as sns sns . heatmap ( confusion_matrix ( y , np . round ( y_pred ))) plt . xlabel ( 'predicted' ) plt . ylabel ( 'actual' );","title":"Visualize trained model performance"},{"location":"01-differential-programming/05-neural-networks/#recap","text":"Deep learning, and more generally any modelling, has the following ingredients: A model and its associated parameters to be optimized. A loss function against which we are optimizing parameters. An optimization routine. You have seen these three ingredients at play with 3 different models: a linear regression model, a logistic regression model, and a deep feed forward neural network model.","title":"Recap"},{"location":"01-differential-programming/05-neural-networks/#in-pictures","text":"Here is a summary of what we've learned in this tutorial!","title":"In Pictures"},{"location":"01-differential-programming/05-neural-networks/#caveats-thus-far","text":"Deep learning is an active field of research. I have only shown you the basics here. In addition, I have also intentionally omitted certain aspects of machine learning practice, such as splitting our data into training and testing sets, performing model selection using cross-validation tuning hyperparameters, such as trying out optimizers regularizing the model, using L1/L2 regularization or dropout etc.","title":"Caveats thus far"},{"location":"01-differential-programming/05-neural-networks/#parting-thoughts","text":"Deep learning is nothing more than optimization of a model with a really large number of parameters. In its current state, it is not artificial intelligence. You should not be afraid of it; it is nothing more than a really powerful model that maps X to Y.","title":"Parting Thoughts"},{"location":"02-jax-idioms/00-introduction/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction In this chapter, we'll introduce you to some idiomatic JAX tools that will help you write performant numerical array programs. The main takeaways you should get from this notebook are how you can: Replace slow Python for loop constructs with fast, just-in-time compiled JAX loop constructs, Create deterministic random numbers (and yes, this is not an oxymoron!) for reproducibility, Freely mix-and-match through this idea of composable transforms . Because contrasts to what we might be used to doing are the the most effective way to teach and learn, in each section, we'll be explicit about what exactly we're replacing when we write these numerical array programs. In doing so, my hope is that you'll see very clearly that structuring your array programs in a composable and atomic fashion will help you take advantage of JAX's composable function transforms to write really fast and compiled functions. And for good measure, we'll contrast this against pure Python programs, so you can witness for yourself how powerful JAX's ideas are... and appreciate how much effort has gone into making the whole thing NumPy compatible! Prerequisites To get the most out of this notebook, you need only be familiar with the NumPy API, and writing functions. Having an appreciation of functools.partial , will help a bit, because we use it a lot in writing JAX programs. However, I know that not everybody has had prior experience with partial -ed functions, so we will introduce the idea mid-way, in a just-in-time fashion as well. If you've gone through tutorial.ipynb , which is the main tutorial notebook for this repository, then you'll have some appreciation of JAX's composable transforms. You'll also see how we wrote some loops in there, and hopefully have an appreciation of how much faster things will run when we use JAX's looping constructs instead.","title":"Introduction"},{"location":"02-jax-idioms/00-introduction/#introduction","text":"In this chapter, we'll introduce you to some idiomatic JAX tools that will help you write performant numerical array programs. The main takeaways you should get from this notebook are how you can: Replace slow Python for loop constructs with fast, just-in-time compiled JAX loop constructs, Create deterministic random numbers (and yes, this is not an oxymoron!) for reproducibility, Freely mix-and-match through this idea of composable transforms . Because contrasts to what we might be used to doing are the the most effective way to teach and learn, in each section, we'll be explicit about what exactly we're replacing when we write these numerical array programs. In doing so, my hope is that you'll see very clearly that structuring your array programs in a composable and atomic fashion will help you take advantage of JAX's composable function transforms to write really fast and compiled functions. And for good measure, we'll contrast this against pure Python programs, so you can witness for yourself how powerful JAX's ideas are... and appreciate how much effort has gone into making the whole thing NumPy compatible!","title":"Introduction"},{"location":"02-jax-idioms/00-introduction/#prerequisites","text":"To get the most out of this notebook, you need only be familiar with the NumPy API, and writing functions. Having an appreciation of functools.partial , will help a bit, because we use it a lot in writing JAX programs. However, I know that not everybody has had prior experience with partial -ed functions, so we will introduce the idea mid-way, in a just-in-time fashion as well. If you've gone through tutorial.ipynb , which is the main tutorial notebook for this repository, then you'll have some appreciation of JAX's composable transforms. You'll also see how we wrote some loops in there, and hopefully have an appreciation of how much faster things will run when we use JAX's looping constructs instead.","title":"Prerequisites"},{"location":"02-jax-idioms/01-loopless-loops/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Replace simple for-loops with vmap The first JAX thing we will look at is the vmap function. What does vmap do? From the JAX docs on vmap Vectorizing map. Creates a function which maps fun over argument axes. Basically the idea here is to take a function and apply it to every \"element\" along a particular array axis. The key skill to learn to use vmap is to be able to decompose a computation into its repeatable element. Let's take a look at a few examples to make this clear. Example: Squaring every element in an array This is the first example that we will walk through, which involves applying a function over every element in a vector. By default, vmap takes in a function f and returns a function f_prime that maps f over the leading axis of an array. The axis along which the array is mapped is configurable, and we'll see that in a moment. For now, let's explore what the default behaviour of vmap is. In the example below, we start with a function square that takes in scalars and returns scalars. (Whether they are float or integers doesn't really matter, but floats are the generalization of integers, so we'll work with that.) If we are being stringent about types, we won't allow ourselves to pass in an array into the square function, even though NumPy technically allows us to do so. vmap gives us the following equivalent function: def func ( x ): ... return result def vmapped_func ( array ): result = [] for element in array : result . append ( func ( x )) result = np . stack ( result ) return result Hence, we can apply a function across the leading (first) axis of an array. In the case of a vector, there is only one axis, so we simply apply the function to all elements on the array. import jax.numpy as np a = np . arange ( 20 ) # (20,) def square ( x : float ) -> float : return x ** 2 from jax import vmap mapped_sq = vmap ( square ) # this is a function! mapped_sq ( a ) vmap returns a function There is one very important thing to remember here! vmap takes in a function and returns another function. ( mapped_sq in the example above.) We still have to pass an array into the returned function, otherwise we won't get a result. Example: Summing every row of a 2D matrix In this next example, we will see how to apply a reduction function (e.g. summation) across every row or column in a matrix. This example will allow us to see how to configure the in_axes argument of vmap . def sum_vector ( x : np . ndarray ) -> np . ndarray : \"\"\"Assumes `x` is a vector\"\"\" return np . sum ( x ) a = np . arange ( 20 ) . reshape (( 4 , 5 )) a # Apply `sum_vector` across each element along the 0th axis. vmap ( sum_vector )( a ) # shape: (4,) # Apply `sum_vector` across each element along the 1st axis. vmap ( sum_vector , in_axes = 1 )( a ) # shape: (5,) Those of you who are experienced with NumPy are probably thinking, \"Couldn't we just specify the axis argument of np.sum , such as np.sum(axis=1) ?\" Yes, but there's more: Using vmap nudges us to think about the elementary and repeatable computation that is used. We practice this skill by thinking about it on a trivial example. Moreover, if we think carefully about the semantic meaning of our array data structures, we can avoid magic axis numbers showing up in our code. (An example of this is consistently keeping the time axis on the leading axis.) And as we all know, the fewer magic numbers there are inside code, the easier it is for us to read it. Example: Softmax function Here is another example involving the softmax function. (We have provided for you the softmax function.) This sort of operation is usually done when we want to take every row in a matrix, which might have negative numbers, and convert them into a stack of probability vectors. (To learn more about the softmax function, check out the Wikipedia article .) def softmax ( x : np . ndarray ) -> np . ndarray : \"\"\"Vector-wise softmax transform.\"\"\" return np . exp ( x ) / np . sum ( np . exp ( x )) a = np . arange ( 20 ) . reshape (( 4 , 5 )) a vmap ( softmax )( a ) Example: Solving for angle of a triangle When solving for an angle inside a right triangle, we need to know two of its lengths. Say we know the opposite and adjacent side lengths for a single triangle. We can then solve for the corresponding angle by taking np.arctan(opp/adj) . def angle ( opp : float , adj : float ): return np . arctan ( opp / adj ) angle ( 5 , 3 ) Imagine we had two vectors, one for opp s and the other for adj s. vmap can automatically transform the angle function into one that operates on vector pairs of opp and adj (assuming they both have the same length). opps = np . arange ( 20 ) adjs = np . linspace ( 3 , 30 , 20 ) vmap ( angle )( opps , adjs ) Exercises Let's go to some exercises to flex your newly-found vmap muscles! Everything you need to know you have picked up above; all that's left is getting practice creatively combining them together. Use the puzzles below, which are ordered in increasing complexity, to challenge your skillsets! Exercise: vmap -ed dot products Message passing is a fundamental operation in the network analysis and graph neural network worlds. It is defined by taking a square adjacency-like matrix (also known as the diffusion matrix) of a graph and matrix multiplying it against a node feature matrix (also known as the message matrix). In NumPy code: def mp ( a : np . ndarray , f : np . ndarray ) -> np . ndarray : \"\"\"Message passing operator. - `a`: An adjacency-like matrix of size (num_nodes, num_nodes). - `f`: A message matrix of size (num_nodes, num_feats). \"\"\" return np . dot ( a , f ) Suppose we have 13 graphs, each of size 7 nodes such that each node has a message vector of length 11. We'd like to perform a message passing operation on each of those graphs. Your task is to implement this using vmap . from jax import random num_nodes = 7 num_graphs = 13 num_feats = 11 key = random . PRNGKey ( 90 ) As = random . bernoulli ( key , p = 0.1 , shape = ( num_graphs , num_nodes , num_nodes )) Fs = random . normal ( key , shape = ( num_graphs , num_nodes , num_feats )) The naive implementation should look something like this: def naive_mp ( As , Fs ): res = [] for a , f in zip ( As , Fs ): res . append ( np . dot ( a , f )) return np . stack ( res ) # Your answer here. def vmapped_message_passing ( As , Fs ): \"\"\"Your answer here!\"\"\" result = vmap ( mp )( As , Fs ) return result Verify that your answer is correct. result = vmapped_message_passing ( As , Fs ) assert result . shape == naive_mp ( As , Fs ) . shape assert not np . allclose ( result , Fs ) Exercise: Chained vmap s We're going to try our hand at constructing a slightly more complex program. This program takes in one dataset of three dimensions, (n_datasets, n_rows, n_columns) . The program first calculates the cumulative product across each row in a dataset, then sums them up (collapsing the columns) across each dataset, and finally applies this same operation across all datasets stacked together. This one is a bit more challenging! To help you along here, the shape of the data are such: There are 11 stacks of data. Each stack of data has 31 rows, and 7 columns. The result of this program still should have 11 stacks and 31 rows, but now each column is not the original data, but the cumulative product of the previous columns. To get this answer right, no magic numbers are allows (e.g. for accessing particular axes). At least two vmap s are necessary here. from dl_workshop.jax_idioms import loopless_loops_ex2 data = random . normal ( key , shape = ( 11 , 31 , 7 )) def ex2_numpy_equivalent ( data ): result = [] for d in data : cp = np . cumprod ( d , axis =- 1 ) s = np . sum ( cp , axis = 1 ) result . append ( s ) return np . stack ( result ) def loopless_loops_ex2 ( data ): \"\"\"Your solution here!\"\"\" pass # Comment out the import if you want to test your answer. from dl_workshop.jax_idioms import loopless_loops_ex2 assert loopless_loops_ex2 ( data ) . shape == ex2_numpy_equivalent ( data ) . shape Exercise: Double for-loops This one is a favourite of mine, and took me an afternoon of on-and-off thinking to reason about clearly. Graphs, a.k.a. networks, are comprised of nodes and edges, and nodes can be represented by a vector of information (i.e. node features). Stacking all the nodes' vectors together gives us a node feature matrix . In graph attention networks, one step is needed where we pairwise concatenate every node to every other node together. For example, if every node had a length n_features feature vector, then concatenating two nodes' vectors together should give us a length 2 * n_features vector. Doing this pairwise across all nodes in a graph would give us an (n_nodes, n_nodes, 2 * n_features) tensor. Your challenge below is to write the vmapped version of the following: num_nodes = 13 num_feats = 17 node_feats = random . normal ( key , shape = ( 13 , 17 )) def ex3_numpy_equivalent ( node_feats ): result = [] for node1 in node_feats : node1_concats = [] for node2 in node_feats : cc = np . concatenate ([ node1 , node2 ]) node1_concats . append ( cc ) result . append ( np . stack ( node1_concats )) return np . stack ( result ) def loopless_loops_ex3 ( node_feats ): \"\"\"Your solution here!\"\"\" pass # Comment out the import if you want to test your answer. from dl_workshop.jax_idioms import loopless_loops_ex3 assert ( loopless_loops_ex3 ( node_feats ) . shape == ex3_numpy_equivalent ( node_feats ) . shape ) Summary To recap, the semantics of vmap basically follow this logic: Take an elementary computation and repeat it across the leading axis of an array. The elementary computation shouldn't know anything about the leading axis. You can then create the vmap -ed function that knows about the leading axis by passing the function through vmap and getting back another function.","title":"Loopless Loops"},{"location":"02-jax-idioms/01-loopless-loops/#replace-simple-for-loops-with-vmap","text":"The first JAX thing we will look at is the vmap function. What does vmap do? From the JAX docs on vmap Vectorizing map. Creates a function which maps fun over argument axes. Basically the idea here is to take a function and apply it to every \"element\" along a particular array axis. The key skill to learn to use vmap is to be able to decompose a computation into its repeatable element. Let's take a look at a few examples to make this clear.","title":"Replace simple for-loops with vmap"},{"location":"02-jax-idioms/01-loopless-loops/#example-squaring-every-element-in-an-array","text":"This is the first example that we will walk through, which involves applying a function over every element in a vector. By default, vmap takes in a function f and returns a function f_prime that maps f over the leading axis of an array. The axis along which the array is mapped is configurable, and we'll see that in a moment. For now, let's explore what the default behaviour of vmap is. In the example below, we start with a function square that takes in scalars and returns scalars. (Whether they are float or integers doesn't really matter, but floats are the generalization of integers, so we'll work with that.) If we are being stringent about types, we won't allow ourselves to pass in an array into the square function, even though NumPy technically allows us to do so. vmap gives us the following equivalent function: def func ( x ): ... return result def vmapped_func ( array ): result = [] for element in array : result . append ( func ( x )) result = np . stack ( result ) return result Hence, we can apply a function across the leading (first) axis of an array. In the case of a vector, there is only one axis, so we simply apply the function to all elements on the array. import jax.numpy as np a = np . arange ( 20 ) # (20,) def square ( x : float ) -> float : return x ** 2 from jax import vmap mapped_sq = vmap ( square ) # this is a function! mapped_sq ( a )","title":"Example: Squaring every element in an array"},{"location":"02-jax-idioms/01-loopless-loops/#vmap-returns-a-function","text":"There is one very important thing to remember here! vmap takes in a function and returns another function. ( mapped_sq in the example above.) We still have to pass an array into the returned function, otherwise we won't get a result.","title":"vmap returns a function"},{"location":"02-jax-idioms/01-loopless-loops/#example-summing-every-row-of-a-2d-matrix","text":"In this next example, we will see how to apply a reduction function (e.g. summation) across every row or column in a matrix. This example will allow us to see how to configure the in_axes argument of vmap . def sum_vector ( x : np . ndarray ) -> np . ndarray : \"\"\"Assumes `x` is a vector\"\"\" return np . sum ( x ) a = np . arange ( 20 ) . reshape (( 4 , 5 )) a # Apply `sum_vector` across each element along the 0th axis. vmap ( sum_vector )( a ) # shape: (4,) # Apply `sum_vector` across each element along the 1st axis. vmap ( sum_vector , in_axes = 1 )( a ) # shape: (5,) Those of you who are experienced with NumPy are probably thinking, \"Couldn't we just specify the axis argument of np.sum , such as np.sum(axis=1) ?\" Yes, but there's more: Using vmap nudges us to think about the elementary and repeatable computation that is used. We practice this skill by thinking about it on a trivial example. Moreover, if we think carefully about the semantic meaning of our array data structures, we can avoid magic axis numbers showing up in our code. (An example of this is consistently keeping the time axis on the leading axis.) And as we all know, the fewer magic numbers there are inside code, the easier it is for us to read it.","title":"Example: Summing every row of a 2D matrix"},{"location":"02-jax-idioms/01-loopless-loops/#example-softmax-function","text":"Here is another example involving the softmax function. (We have provided for you the softmax function.) This sort of operation is usually done when we want to take every row in a matrix, which might have negative numbers, and convert them into a stack of probability vectors. (To learn more about the softmax function, check out the Wikipedia article .) def softmax ( x : np . ndarray ) -> np . ndarray : \"\"\"Vector-wise softmax transform.\"\"\" return np . exp ( x ) / np . sum ( np . exp ( x )) a = np . arange ( 20 ) . reshape (( 4 , 5 )) a vmap ( softmax )( a )","title":"Example: Softmax function"},{"location":"02-jax-idioms/01-loopless-loops/#example-solving-for-angle-of-a-triangle","text":"When solving for an angle inside a right triangle, we need to know two of its lengths. Say we know the opposite and adjacent side lengths for a single triangle. We can then solve for the corresponding angle by taking np.arctan(opp/adj) . def angle ( opp : float , adj : float ): return np . arctan ( opp / adj ) angle ( 5 , 3 ) Imagine we had two vectors, one for opp s and the other for adj s. vmap can automatically transform the angle function into one that operates on vector pairs of opp and adj (assuming they both have the same length). opps = np . arange ( 20 ) adjs = np . linspace ( 3 , 30 , 20 ) vmap ( angle )( opps , adjs )","title":"Example: Solving for angle of a triangle"},{"location":"02-jax-idioms/01-loopless-loops/#exercises","text":"Let's go to some exercises to flex your newly-found vmap muscles! Everything you need to know you have picked up above; all that's left is getting practice creatively combining them together. Use the puzzles below, which are ordered in increasing complexity, to challenge your skillsets!","title":"Exercises"},{"location":"02-jax-idioms/01-loopless-loops/#exercise-vmap-ed-dot-products","text":"Message passing is a fundamental operation in the network analysis and graph neural network worlds. It is defined by taking a square adjacency-like matrix (also known as the diffusion matrix) of a graph and matrix multiplying it against a node feature matrix (also known as the message matrix). In NumPy code: def mp ( a : np . ndarray , f : np . ndarray ) -> np . ndarray : \"\"\"Message passing operator. - `a`: An adjacency-like matrix of size (num_nodes, num_nodes). - `f`: A message matrix of size (num_nodes, num_feats). \"\"\" return np . dot ( a , f ) Suppose we have 13 graphs, each of size 7 nodes such that each node has a message vector of length 11. We'd like to perform a message passing operation on each of those graphs. Your task is to implement this using vmap . from jax import random num_nodes = 7 num_graphs = 13 num_feats = 11 key = random . PRNGKey ( 90 ) As = random . bernoulli ( key , p = 0.1 , shape = ( num_graphs , num_nodes , num_nodes )) Fs = random . normal ( key , shape = ( num_graphs , num_nodes , num_feats )) The naive implementation should look something like this: def naive_mp ( As , Fs ): res = [] for a , f in zip ( As , Fs ): res . append ( np . dot ( a , f )) return np . stack ( res ) # Your answer here. def vmapped_message_passing ( As , Fs ): \"\"\"Your answer here!\"\"\" result = vmap ( mp )( As , Fs ) return result Verify that your answer is correct. result = vmapped_message_passing ( As , Fs ) assert result . shape == naive_mp ( As , Fs ) . shape assert not np . allclose ( result , Fs )","title":"Exercise: vmap-ed dot products"},{"location":"02-jax-idioms/01-loopless-loops/#exercise-chained-vmaps","text":"We're going to try our hand at constructing a slightly more complex program. This program takes in one dataset of three dimensions, (n_datasets, n_rows, n_columns) . The program first calculates the cumulative product across each row in a dataset, then sums them up (collapsing the columns) across each dataset, and finally applies this same operation across all datasets stacked together. This one is a bit more challenging! To help you along here, the shape of the data are such: There are 11 stacks of data. Each stack of data has 31 rows, and 7 columns. The result of this program still should have 11 stacks and 31 rows, but now each column is not the original data, but the cumulative product of the previous columns. To get this answer right, no magic numbers are allows (e.g. for accessing particular axes). At least two vmap s are necessary here. from dl_workshop.jax_idioms import loopless_loops_ex2 data = random . normal ( key , shape = ( 11 , 31 , 7 )) def ex2_numpy_equivalent ( data ): result = [] for d in data : cp = np . cumprod ( d , axis =- 1 ) s = np . sum ( cp , axis = 1 ) result . append ( s ) return np . stack ( result ) def loopless_loops_ex2 ( data ): \"\"\"Your solution here!\"\"\" pass # Comment out the import if you want to test your answer. from dl_workshop.jax_idioms import loopless_loops_ex2 assert loopless_loops_ex2 ( data ) . shape == ex2_numpy_equivalent ( data ) . shape","title":"Exercise: Chained vmaps"},{"location":"02-jax-idioms/01-loopless-loops/#exercise-double-for-loops","text":"This one is a favourite of mine, and took me an afternoon of on-and-off thinking to reason about clearly. Graphs, a.k.a. networks, are comprised of nodes and edges, and nodes can be represented by a vector of information (i.e. node features). Stacking all the nodes' vectors together gives us a node feature matrix . In graph attention networks, one step is needed where we pairwise concatenate every node to every other node together. For example, if every node had a length n_features feature vector, then concatenating two nodes' vectors together should give us a length 2 * n_features vector. Doing this pairwise across all nodes in a graph would give us an (n_nodes, n_nodes, 2 * n_features) tensor. Your challenge below is to write the vmapped version of the following: num_nodes = 13 num_feats = 17 node_feats = random . normal ( key , shape = ( 13 , 17 )) def ex3_numpy_equivalent ( node_feats ): result = [] for node1 in node_feats : node1_concats = [] for node2 in node_feats : cc = np . concatenate ([ node1 , node2 ]) node1_concats . append ( cc ) result . append ( np . stack ( node1_concats )) return np . stack ( result ) def loopless_loops_ex3 ( node_feats ): \"\"\"Your solution here!\"\"\" pass # Comment out the import if you want to test your answer. from dl_workshop.jax_idioms import loopless_loops_ex3 assert ( loopless_loops_ex3 ( node_feats ) . shape == ex3_numpy_equivalent ( node_feats ) . shape )","title":"Exercise: Double for-loops"},{"location":"02-jax-idioms/01-loopless-loops/#summary","text":"To recap, the semantics of vmap basically follow this logic: Take an elementary computation and repeat it across the leading axis of an array. The elementary computation shouldn't know anything about the leading axis. You can then create the vmap -ed function that knows about the leading axis by passing the function through vmap and getting back another function.","title":"Summary"},{"location":"02-jax-idioms/02-loopy-carry/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Eliminating for-loops that have carry-over using lax.scan We are now going to see how we can eliminate for-loops that have carry-over using lax.scan . From the JAX docs, lax.scan replaces a for-loop with carry-over, with some of my own annotations added in for clarity: Scan a function over leading array axes while carrying along state. The semantics are described as follows: def scan ( f , init , xs , length = None ): if xs is None : xs = [ None ] * length carry = init ys = [] for x in xs : carry , y = f ( carry , x ) # carry is the carryover ys . append ( y ) # the `y`s get accumulated into a stacked array return carry , np . stack ( ys ) A key requirement of the function f , which is the function that gets scanned over the array xs , is that it must have only two positional arguments in there, one for carry and one for x . You'll see how we can thus apply functools.partial to construct functions that have this signature from other functions that have more arguments present. Let's see some concrete examples of this in action. Example: Cumulative Summation One example where we might use a for-loop is in the cumulative sum or product of an array. Here, we need the current loop information to update the information from the previous loop. Let's see it in action for the cumulative sum: import jax.numpy as np a = np . array ([ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 ]) result = [] res = 0 for el in a : res += el result . append ( res ) np . array ( result ) This is identical to the cumulative sum: np . cumsum ( a ) Now, let's write it using lax.scan , so we can see the pattern in action: from jax import lax def cumsum ( res , el ): \"\"\" - `res`: The result from the previous loop. - `el`: The current array element. \"\"\" res = res + el return res , res # (\"carryover\", \"accumulated\") result_init = 0 final , result = lax . scan ( cumsum , result_init , a ) result As you can see, scanned function has to return two things: One object that gets carried over to the next loop ( carryover ), and Another object that gets \"accumulated\" into an array ( accumulated ). The starting initial value, result_init , is passed into the scanfunc as res on the first call of the scanfunc . On subsequent calls, the first res is passed back into the scanfunc as the new res . Exercise 1: Simulating compound interest We can use lax.scan to generate data that simulates the generation of wealth by compound interest. Here's an implementation using a plain vanilla for-loop: wealth_record = [] starting_wealth = 100.0 interest_factor = 1.01 num_timesteps = 100 prev_wealth = starting_wealth for t in range ( num_timesteps ): new_wealth = prev_wealth * interest_factor wealth_record . append ( prev_wealth ) prev_wealth = new_wealth wealth_record = np . array ( wealth_record ) Now, your challenge is to implement it in a lax.scan form. Implement the wealth_at_time function below. from functools import partial def wealth_at_time ( prev_wealth , time , interest_factor ): # The lax.scannable function to compute wealth at a given time. # your answer here pass # Comment out the import to test your answer from dl_workshop.jax_idioms import lax_scan_ex_1 as wealth_at_time wealth_func = partial ( wealth_at_time , interest_factor = interest_factor ) timesteps = np . arange ( num_timesteps ) final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) assert np . allclose ( wealth_record , result ) The two are equivalent, so we know we have the lax.scan implementation right. import matplotlib.pyplot as plt plt . plot ( wealth_record , label = \"for-loop\" ) plt . plot ( result , label = \"lax.scan\" ) plt . legend (); Example: Simulating compound interest from multiple starting points Previously, was one simulation of wealth generation by compound interest from one starting amount of money. Now, let's simulate the wealth generation for different starting wealth levels; onemay choose any 300 starting points however one likes. This will be a demonstration of how to compose lax.scan with vmap to do computation without loops. To do so, you'll likely want to start with a function that accepts a scalar starting wealth and generates the simulated time series from there, and then vmap that function across multiple starting points (which is an array itself). from jax import vmap def simulate_compound_interest ( starting_wealth : np . ndarray , timesteps : np . ndarray ): final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) return final , result num_timesteps = np . arange ( 200 ) starting_wealths = np . arange ( 300 ) . astype ( float ) simulation_func = partial ( simulate_compound_interest , timesteps = np . arange ( 200 )) final , growth = vmap ( simulation_func )( starting_wealths ) growth . shape plt . plot ( growth [ 1 ]) plt . plot ( growth [ 2 ]) plt . plot ( growth [ 3 ]); Exercise 2: Stick breaking process The stick breaking process is one that is important in Bayesian non-parametric modelling, where we want to model something that may have potentially an infinite number of components while being biased towards a smaller subset of components. The stick-breaking process uses the following generative process: Take a stick of length 1. Draw a number between 0 and 1 from a Beta distribution (we will modify this step for this notebook). Break that fraction of the stick, and leave it aside in a pile. Repeat steps 2 and 3 with the fraction leftover after breaking the stick. We repeat ad infinitum (in theory) or until a pre-specified large number of stick breaks have happened (in practice). In the exercise below, your task is to write the stick-breaking process in terms of a lax.scan operation. Because we have not yet covered drawing random numbers using JAX, the breaking fraction will be a fixed variable rather than a random variable. Here's the vanilla NumPy + Python equivalent for you to reference. # NumPy equivalent num_breaks = 30 breaking_fraction = 0.1 sticks = [] stick_length = 1.0 for i in range ( num_breaks ): stick = stick_length * breaking_fraction sticks . append ( stick ) stick_length = stick_length - stick sticks = np . array ( sticks ) sticks def lax_scan_ex_2 ( num_breaks : int , frac : float ): # Your answer goes here! pass # Comment out the import if you want to test your answer. from dl_workshop.jax_idioms import lax_scan_ex_2 sticksres = lax_scan_ex_2 ( num_breaks , breaking_fraction ) assert np . allclose ( sticksres , sticks )","title":"Loopy Carry"},{"location":"02-jax-idioms/02-loopy-carry/#eliminating-for-loops-that-have-carry-over-using-laxscan","text":"We are now going to see how we can eliminate for-loops that have carry-over using lax.scan . From the JAX docs, lax.scan replaces a for-loop with carry-over, with some of my own annotations added in for clarity: Scan a function over leading array axes while carrying along state. The semantics are described as follows: def scan ( f , init , xs , length = None ): if xs is None : xs = [ None ] * length carry = init ys = [] for x in xs : carry , y = f ( carry , x ) # carry is the carryover ys . append ( y ) # the `y`s get accumulated into a stacked array return carry , np . stack ( ys ) A key requirement of the function f , which is the function that gets scanned over the array xs , is that it must have only two positional arguments in there, one for carry and one for x . You'll see how we can thus apply functools.partial to construct functions that have this signature from other functions that have more arguments present. Let's see some concrete examples of this in action.","title":"Eliminating for-loops that have carry-over using lax.scan"},{"location":"02-jax-idioms/02-loopy-carry/#example-cumulative-summation","text":"One example where we might use a for-loop is in the cumulative sum or product of an array. Here, we need the current loop information to update the information from the previous loop. Let's see it in action for the cumulative sum: import jax.numpy as np a = np . array ([ 1 , 2 , 3 , 5 , 7 , 11 , 13 , 17 ]) result = [] res = 0 for el in a : res += el result . append ( res ) np . array ( result ) This is identical to the cumulative sum: np . cumsum ( a ) Now, let's write it using lax.scan , so we can see the pattern in action: from jax import lax def cumsum ( res , el ): \"\"\" - `res`: The result from the previous loop. - `el`: The current array element. \"\"\" res = res + el return res , res # (\"carryover\", \"accumulated\") result_init = 0 final , result = lax . scan ( cumsum , result_init , a ) result As you can see, scanned function has to return two things: One object that gets carried over to the next loop ( carryover ), and Another object that gets \"accumulated\" into an array ( accumulated ). The starting initial value, result_init , is passed into the scanfunc as res on the first call of the scanfunc . On subsequent calls, the first res is passed back into the scanfunc as the new res .","title":"Example: Cumulative Summation"},{"location":"02-jax-idioms/02-loopy-carry/#exercise-1-simulating-compound-interest","text":"We can use lax.scan to generate data that simulates the generation of wealth by compound interest. Here's an implementation using a plain vanilla for-loop: wealth_record = [] starting_wealth = 100.0 interest_factor = 1.01 num_timesteps = 100 prev_wealth = starting_wealth for t in range ( num_timesteps ): new_wealth = prev_wealth * interest_factor wealth_record . append ( prev_wealth ) prev_wealth = new_wealth wealth_record = np . array ( wealth_record ) Now, your challenge is to implement it in a lax.scan form. Implement the wealth_at_time function below. from functools import partial def wealth_at_time ( prev_wealth , time , interest_factor ): # The lax.scannable function to compute wealth at a given time. # your answer here pass # Comment out the import to test your answer from dl_workshop.jax_idioms import lax_scan_ex_1 as wealth_at_time wealth_func = partial ( wealth_at_time , interest_factor = interest_factor ) timesteps = np . arange ( num_timesteps ) final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) assert np . allclose ( wealth_record , result ) The two are equivalent, so we know we have the lax.scan implementation right. import matplotlib.pyplot as plt plt . plot ( wealth_record , label = \"for-loop\" ) plt . plot ( result , label = \"lax.scan\" ) plt . legend ();","title":"Exercise 1: Simulating compound interest"},{"location":"02-jax-idioms/02-loopy-carry/#example-simulating-compound-interest-from-multiple-starting-points","text":"Previously, was one simulation of wealth generation by compound interest from one starting amount of money. Now, let's simulate the wealth generation for different starting wealth levels; onemay choose any 300 starting points however one likes. This will be a demonstration of how to compose lax.scan with vmap to do computation without loops. To do so, you'll likely want to start with a function that accepts a scalar starting wealth and generates the simulated time series from there, and then vmap that function across multiple starting points (which is an array itself). from jax import vmap def simulate_compound_interest ( starting_wealth : np . ndarray , timesteps : np . ndarray ): final , result = lax . scan ( wealth_func , init = starting_wealth , xs = timesteps ) return final , result num_timesteps = np . arange ( 200 ) starting_wealths = np . arange ( 300 ) . astype ( float ) simulation_func = partial ( simulate_compound_interest , timesteps = np . arange ( 200 )) final , growth = vmap ( simulation_func )( starting_wealths ) growth . shape plt . plot ( growth [ 1 ]) plt . plot ( growth [ 2 ]) plt . plot ( growth [ 3 ]);","title":"Example: Simulating compound interest from multiple starting points"},{"location":"02-jax-idioms/02-loopy-carry/#exercise-2-stick-breaking-process","text":"The stick breaking process is one that is important in Bayesian non-parametric modelling, where we want to model something that may have potentially an infinite number of components while being biased towards a smaller subset of components. The stick-breaking process uses the following generative process: Take a stick of length 1. Draw a number between 0 and 1 from a Beta distribution (we will modify this step for this notebook). Break that fraction of the stick, and leave it aside in a pile. Repeat steps 2 and 3 with the fraction leftover after breaking the stick. We repeat ad infinitum (in theory) or until a pre-specified large number of stick breaks have happened (in practice). In the exercise below, your task is to write the stick-breaking process in terms of a lax.scan operation. Because we have not yet covered drawing random numbers using JAX, the breaking fraction will be a fixed variable rather than a random variable. Here's the vanilla NumPy + Python equivalent for you to reference. # NumPy equivalent num_breaks = 30 breaking_fraction = 0.1 sticks = [] stick_length = 1.0 for i in range ( num_breaks ): stick = stick_length * breaking_fraction sticks . append ( stick ) stick_length = stick_length - stick sticks = np . array ( sticks ) sticks def lax_scan_ex_2 ( num_breaks : int , frac : float ): # Your answer goes here! pass # Comment out the import if you want to test your answer. from dl_workshop.jax_idioms import lax_scan_ex_2 sticksres = lax_scan_ex_2 ( num_breaks , breaking_fraction ) assert np . allclose ( sticksres , sticks )","title":"Exercise 2: Stick breaking process"},{"location":"02-jax-idioms/03-deterministic-randomness/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Deterministic Randomness In this section, we'll explore how to create programs that use random number generation in a fashion that is fully deterministic. If that sounds weird to you, fret not: it sounded weird to me too when I first started using random numbers. My goal here is to demystify this foundational piece for you. Random number generation before JAX Before JAX came along, we used NumPy's stateful random number generation system. Let's quickly recap how it works. import numpy as onp # original numpy Let's draw a random number from a Gaussian in NumPy. onp . random . seed ( 42 ) a = onp . random . normal () a And for good measure, let's draw another one. b = onp . random . normal () b This is intuitive behaviour, because we expect that each time we call on a random number generator, we should get back a different number from before. However, this behaviour is problematic when we are trying to debug programs. When debugging, one desirable property is determinism. Executing the same line of code twice should produce exactly the same result. Otherwise, debugging what happens at that particular line would be extremely difficult. The core problem here is that stochastically, we might hit a setting where we encounter an error in our program, and we are unable to reproduce it because we are relying on a random number generator that relies on global state, and hence that doesn't behave in a fully controllable fashion. I don't know about you, but if I am going to encounter problems, I'd like to encounter them reliably ! Random number generation with JAX How then can we get \"the best of both worlds\": random number generation that is controllable? Explicit PRNGKeys control random number generation The way that JAX's developers went about doing this is to use pseudo-random number generators that require explicit passing in of a pseudo-random number generation key, rather than relying on a global state being set. Each unique key will deterministically give a unique drawn value explicitly. Let's see that in action: from jax import random key = random . PRNGKey ( 42 ) a = random . normal ( key = key ) a To show you that passing in the same key gives us the same values as before: b = random . normal ( key = key ) b That should already be a stark difference from what you're used to with vanilla NumPy, and this is one key crucial difference between JAX's random module and NumPy's random module. Everything else about the API is very similar, but this is a key difference, and for good reason -- this should hint to you the idea that we can have explicit reproducibility, rather than merely implicit, over our stochastic programs within the same session. Splitting keys to generate new draws How, then, do we get a new draw from JAX? Well, we can either create a new key manually, or we can programmatically split the key into two, and use one of the newly split keys to generate a new random number. Let's see that in action: k1 , k2 = random . split ( key ) c = random . normal ( key = k2 ) c k3 , k4 , k5 = random . split ( k2 , num = 3 ) d = random . normal ( key = k3 ) d Generating multiple draws from a Gaussian, two ways To show you how we can combine random keys together with vmap , here's two ways we can generate random draws from a Normal distribution. The first way is to split the key into K (say, 20) pieces and then vmap random.normal over the split keys. from jax import vmap key = random . PRNGKey ( 44 ) ks = random . split ( key , 20 ) # we want to generate 20 draws draws = vmap ( random . normal )( ks ) draws Of course, the second way is to simply specify the shape of the draws. random . normal ( key , shape = ( 20 ,)) By splitting the key into two, three, or even 1000 parts, we can get new keys that are derived from a parent key that generate different random numbers from the same random number generating function. Let's explore how we can use this in the generation of a Gaussian random walk. Example: Simulating a Gaussian random walk A Gaussian random walk is one where we start at a point that is drawn from a Gaussian, and then we draw another point from a Gausian using the first point as the starting Gaussian point. Does that loop structure sound familiar? Well... yeah, it sounds like a classic lax.scan setup! Here's how we might set it up. Firstly, JAX's random.normal function doesn't allow us to specify the location and scale, and only gives us a draw from a unit Gaussian. We can work around this, because any unit Gaussian draw can be shifted and scaled to a N(\\mu, \\sigma) N(\\mu, \\sigma) by multiplying the draw by \\sigma \\sigma and adding \\mu \\mu . Knowing this, let's see how we can write a Gaussian random walk using JAX's idioms, building up from a vanilla Python implementation. Vanilla Python implementation For those who might not be too familiar with Gaussian random walks, here is an annotated version in vanilla Python code (plus some use of the JAX PRNGKey system added in). num_timesteps = 100 mu = 0.0 # starting mean. observations = [ mu ] key = random . PRNGKey ( 44 ) # Split the key num_timesteps number of times keys = random . split ( key , num_timesteps ) # Gaussian Random Walk goes here for k in keys : mu = mu + random . normal ( k ) observations . append ( mu ) import matplotlib.pyplot as plt plt . plot ( observations ) Implementation using JAX Now, let's see how we can write a Gaussian random walk using lax.scan . The strategy we'll go for is as follows: We'll instantiate an array of PRNG keys. We'll then scan a function across the PRNG keys. We'll finally collect the observations together. from jax import lax def new_draw ( prev_val , key ): new = prev_val + random . normal ( key ) return new , prev_val final , draws = lax . scan ( new_draw , 0.0 , keys ) plt . plot ( draws ) Looks like we did it! Definitely looks like a proper Gaussian random walk to me. Let's encapsulate the code inside a function that gives us one random walk draw, as I will show you how next to generate multiple random walk draws. def grw_draw ( key , num_steps ): keys = random . split ( key , num_steps ) final , draws = lax . scan ( new_draw , 0.0 , keys ) return final , draws final , draw = grw_draw ( key , num_steps = 100 ) plt . plot ( draw ) A note on reproducibility Now, note how if you were to re-run the entire program from top-to-bottom again, you would get exactly the same plot . This is what we might call strictly reproducible . Traditional array programs are not always written in a strictly reproducible way; the sloppy programmer would set a global state at the top of a notebook and then call it a day. By contrast, with JAX's random number generation paradigm, any random number generation program is 100% reproducible, down to the level of the exact sequence of random number draws, as long as the seed(s) controlling the program are 100% identical. Because JAX's stochastic programs always require an explicit key to be provided, as long as you write your stochastic programs to depend on keys passed into it, rather than keys instantiated from within it, any errors you get can be fully reproduced by passing in exactly the same key. When an error shows up in a program, as long as its stochastic components are controlled by explicitly passed in seeds, that error is 100% reproducible. For those who have tried working with stochastic programs before, this is an extremely desirable property, as it means we gain the ability to reliably debug our program -- absolutely crucial especially when it comes to working with probabilistic models. Also notice how we finally wrote our first productive for-loop -- but it was only to plot something, not for some form of calculations :). Exercise 1: Brownian motion on a grid In this exercise, the goal is to simulate the random walk of a single particle on a 2D grid. The particle's (x, y) position can be represented by a vector of length 2. At each time step, the particle moves either in the x- or y- direction, and when it moves, it either goes +1 or -1 along that axis. Here is the NumPy + Python loopy equivalent that you'll be simulating. import jax.numpy as np starting_position = onp . array ([ 0 , 0 ]) n_steps = 1000 positions = [ starting_position ] keys = random . split ( key , n_steps ) for k in keys : k1 , k2 = random . split ( k ) axis = random . choice ( k1 , np . array ([ 0 , 1 ])) direction = random . choice ( k2 , np . array ([ - 1 , 1 ])) x , y = positions [ - 1 ] if axis == 0 : x += direction else : y += direction new_position = np . array ([ x , y ]) positions . append ( new_position ) positions = np . stack ( positions ) plt . plot ( positions [:, 0 ], positions [:, 1 ], alpha = 0.5 ) Your challenge is to replicate the brownian motion on a grid using JAX's random module. Some hints that may help you get started include: JAX arrays are immutable, so you definitely cannot do arr[:, 0] += 1 . random.permutation can be used to identify which axis to move. random.choice can be used to identify which direction to go in. Together, the axis to move in and the direction to proceed can give you something to loop over... ...but without looping explicitly :), for which you have all of the tricks in the book. def randomness_ex_1 ( keys ): # Your answer here! pass from dl_workshop.jax_idioms import randomness_ex_1 final , history = randomness_ex_1 ( keys , starting_position ) plt . plot ( history [:, 0 ], history [:, 1 ], alpha = 0.5 ) Exercise 2: Stochastic stick breaking In the previous notebook, we introduced you to the stick-breaking process, and we asked you to write it in a non-stochastic fashion. We're now going to have you write it using a stochastic draw. To do so, however, you need to be familiar with the Beta distribution , which models a random draw from the interval x \\in (0, 1) x \\in (0, 1) . Here is how you can draw numbers from the Beta distribution: betadraw = random . beta ( key , a = 1 , b = 2 ) betadraw Now, I'm going to show you the NumPy + Python equivalent of the real (i.e. stochastic) stick-breaking process: import jax.numpy as np num_breaks = 30 keys = random . split ( key , num_breaks ) concentration = 5 sticks = [] stick_length = 1.0 for k in keys : breaking_fraction = random . beta ( k , a = 1 , b = concentration ) stick = stick_length * breaking_fraction sticks . append ( stick ) stick_length = stick_length - stick result = np . array ( sticks ) result Now, your task is to implement it using lax.scan . def randomness_ex_2 ( key , num_breaks , concentration : float ): # Your answer here! pass # Comment out the import to test your answer! from dl_workshop.jax_idioms import randomness_ex_2 final , sticks = randomness_ex_2 ( key , num_breaks , concentration ) assert np . allclose ( sticks , result ) Exercise 3: Multiple GRWs Now, what if we wanted to generate multiple realizations of the Gaussian random walk? Does this sound familiar? If so... yeah, it's a vanilla for-loop, which directly brings us to vmap ! And that's what we're going to try to implement in this exercise. from functools import partial from jax import vmap The key idea here is to vmap the grw_draw function across multiple PRNGKeys. That way, you can avoid doing a for-loop, which is the goal of this exercise too. You get to decide how many realizations of the GRW you'd like to create. def randomness_ex_3 ( key , num_realizations = 20 , grw_draw = grw_draw ): # Your answer here pass from dl_workshop.jax_idioms import randomness_ex_3 final , trajectories = randomness_ex_3 ( key , num_realizations = 20 , grw_draw = grw_draw ) trajectories . shape We did it! We have 20 trajectories of a 1000-step Gaussian random walk. Notice also how the program is structured very nicely: Each layer of abstraction in the program corresponds to a new axis dimension along which we are working. The onion layering of the program has very natural structure for the problem at hand. Effectively, we have planned out, or perhaps staged out, our computation using Python before actually executing it. Let's visualize the trajectories to make sure they are really GRW-like. import seaborn as sns fig , ax = plt . subplots () for trajectory in trajectories [ 0 : 20 ]: ax . plot ( trajectory ) sns . despine ()","title":"Deterministic Randomness"},{"location":"02-jax-idioms/03-deterministic-randomness/#deterministic-randomness","text":"In this section, we'll explore how to create programs that use random number generation in a fashion that is fully deterministic. If that sounds weird to you, fret not: it sounded weird to me too when I first started using random numbers. My goal here is to demystify this foundational piece for you.","title":"Deterministic Randomness"},{"location":"02-jax-idioms/03-deterministic-randomness/#random-number-generation-before-jax","text":"Before JAX came along, we used NumPy's stateful random number generation system. Let's quickly recap how it works. import numpy as onp # original numpy Let's draw a random number from a Gaussian in NumPy. onp . random . seed ( 42 ) a = onp . random . normal () a And for good measure, let's draw another one. b = onp . random . normal () b This is intuitive behaviour, because we expect that each time we call on a random number generator, we should get back a different number from before. However, this behaviour is problematic when we are trying to debug programs. When debugging, one desirable property is determinism. Executing the same line of code twice should produce exactly the same result. Otherwise, debugging what happens at that particular line would be extremely difficult. The core problem here is that stochastically, we might hit a setting where we encounter an error in our program, and we are unable to reproduce it because we are relying on a random number generator that relies on global state, and hence that doesn't behave in a fully controllable fashion. I don't know about you, but if I am going to encounter problems, I'd like to encounter them reliably !","title":"Random number generation before JAX"},{"location":"02-jax-idioms/03-deterministic-randomness/#random-number-generation-with-jax","text":"How then can we get \"the best of both worlds\": random number generation that is controllable?","title":"Random number generation with JAX"},{"location":"02-jax-idioms/03-deterministic-randomness/#explicit-prngkeys-control-random-number-generation","text":"The way that JAX's developers went about doing this is to use pseudo-random number generators that require explicit passing in of a pseudo-random number generation key, rather than relying on a global state being set. Each unique key will deterministically give a unique drawn value explicitly. Let's see that in action: from jax import random key = random . PRNGKey ( 42 ) a = random . normal ( key = key ) a To show you that passing in the same key gives us the same values as before: b = random . normal ( key = key ) b That should already be a stark difference from what you're used to with vanilla NumPy, and this is one key crucial difference between JAX's random module and NumPy's random module. Everything else about the API is very similar, but this is a key difference, and for good reason -- this should hint to you the idea that we can have explicit reproducibility, rather than merely implicit, over our stochastic programs within the same session.","title":"Explicit PRNGKeys control random number generation"},{"location":"02-jax-idioms/03-deterministic-randomness/#splitting-keys-to-generate-new-draws","text":"How, then, do we get a new draw from JAX? Well, we can either create a new key manually, or we can programmatically split the key into two, and use one of the newly split keys to generate a new random number. Let's see that in action: k1 , k2 = random . split ( key ) c = random . normal ( key = k2 ) c k3 , k4 , k5 = random . split ( k2 , num = 3 ) d = random . normal ( key = k3 ) d","title":"Splitting keys to generate new draws"},{"location":"02-jax-idioms/03-deterministic-randomness/#generating-multiple-draws-from-a-gaussian-two-ways","text":"To show you how we can combine random keys together with vmap , here's two ways we can generate random draws from a Normal distribution. The first way is to split the key into K (say, 20) pieces and then vmap random.normal over the split keys. from jax import vmap key = random . PRNGKey ( 44 ) ks = random . split ( key , 20 ) # we want to generate 20 draws draws = vmap ( random . normal )( ks ) draws Of course, the second way is to simply specify the shape of the draws. random . normal ( key , shape = ( 20 ,)) By splitting the key into two, three, or even 1000 parts, we can get new keys that are derived from a parent key that generate different random numbers from the same random number generating function. Let's explore how we can use this in the generation of a Gaussian random walk.","title":"Generating multiple draws from a Gaussian, two ways"},{"location":"02-jax-idioms/03-deterministic-randomness/#example-simulating-a-gaussian-random-walk","text":"A Gaussian random walk is one where we start at a point that is drawn from a Gaussian, and then we draw another point from a Gausian using the first point as the starting Gaussian point. Does that loop structure sound familiar? Well... yeah, it sounds like a classic lax.scan setup! Here's how we might set it up. Firstly, JAX's random.normal function doesn't allow us to specify the location and scale, and only gives us a draw from a unit Gaussian. We can work around this, because any unit Gaussian draw can be shifted and scaled to a N(\\mu, \\sigma) N(\\mu, \\sigma) by multiplying the draw by \\sigma \\sigma and adding \\mu \\mu . Knowing this, let's see how we can write a Gaussian random walk using JAX's idioms, building up from a vanilla Python implementation.","title":"Example: Simulating a Gaussian random walk"},{"location":"02-jax-idioms/03-deterministic-randomness/#vanilla-python-implementation","text":"For those who might not be too familiar with Gaussian random walks, here is an annotated version in vanilla Python code (plus some use of the JAX PRNGKey system added in). num_timesteps = 100 mu = 0.0 # starting mean. observations = [ mu ] key = random . PRNGKey ( 44 ) # Split the key num_timesteps number of times keys = random . split ( key , num_timesteps ) # Gaussian Random Walk goes here for k in keys : mu = mu + random . normal ( k ) observations . append ( mu ) import matplotlib.pyplot as plt plt . plot ( observations )","title":"Vanilla Python implementation"},{"location":"02-jax-idioms/03-deterministic-randomness/#implementation-using-jax","text":"Now, let's see how we can write a Gaussian random walk using lax.scan . The strategy we'll go for is as follows: We'll instantiate an array of PRNG keys. We'll then scan a function across the PRNG keys. We'll finally collect the observations together. from jax import lax def new_draw ( prev_val , key ): new = prev_val + random . normal ( key ) return new , prev_val final , draws = lax . scan ( new_draw , 0.0 , keys ) plt . plot ( draws ) Looks like we did it! Definitely looks like a proper Gaussian random walk to me. Let's encapsulate the code inside a function that gives us one random walk draw, as I will show you how next to generate multiple random walk draws. def grw_draw ( key , num_steps ): keys = random . split ( key , num_steps ) final , draws = lax . scan ( new_draw , 0.0 , keys ) return final , draws final , draw = grw_draw ( key , num_steps = 100 ) plt . plot ( draw )","title":"Implementation using JAX"},{"location":"02-jax-idioms/03-deterministic-randomness/#a-note-on-reproducibility","text":"Now, note how if you were to re-run the entire program from top-to-bottom again, you would get exactly the same plot . This is what we might call strictly reproducible . Traditional array programs are not always written in a strictly reproducible way; the sloppy programmer would set a global state at the top of a notebook and then call it a day. By contrast, with JAX's random number generation paradigm, any random number generation program is 100% reproducible, down to the level of the exact sequence of random number draws, as long as the seed(s) controlling the program are 100% identical. Because JAX's stochastic programs always require an explicit key to be provided, as long as you write your stochastic programs to depend on keys passed into it, rather than keys instantiated from within it, any errors you get can be fully reproduced by passing in exactly the same key. When an error shows up in a program, as long as its stochastic components are controlled by explicitly passed in seeds, that error is 100% reproducible. For those who have tried working with stochastic programs before, this is an extremely desirable property, as it means we gain the ability to reliably debug our program -- absolutely crucial especially when it comes to working with probabilistic models. Also notice how we finally wrote our first productive for-loop -- but it was only to plot something, not for some form of calculations :).","title":"A note on reproducibility"},{"location":"02-jax-idioms/03-deterministic-randomness/#exercise-1-brownian-motion-on-a-grid","text":"In this exercise, the goal is to simulate the random walk of a single particle on a 2D grid. The particle's (x, y) position can be represented by a vector of length 2. At each time step, the particle moves either in the x- or y- direction, and when it moves, it either goes +1 or -1 along that axis. Here is the NumPy + Python loopy equivalent that you'll be simulating. import jax.numpy as np starting_position = onp . array ([ 0 , 0 ]) n_steps = 1000 positions = [ starting_position ] keys = random . split ( key , n_steps ) for k in keys : k1 , k2 = random . split ( k ) axis = random . choice ( k1 , np . array ([ 0 , 1 ])) direction = random . choice ( k2 , np . array ([ - 1 , 1 ])) x , y = positions [ - 1 ] if axis == 0 : x += direction else : y += direction new_position = np . array ([ x , y ]) positions . append ( new_position ) positions = np . stack ( positions ) plt . plot ( positions [:, 0 ], positions [:, 1 ], alpha = 0.5 ) Your challenge is to replicate the brownian motion on a grid using JAX's random module. Some hints that may help you get started include: JAX arrays are immutable, so you definitely cannot do arr[:, 0] += 1 . random.permutation can be used to identify which axis to move. random.choice can be used to identify which direction to go in. Together, the axis to move in and the direction to proceed can give you something to loop over... ...but without looping explicitly :), for which you have all of the tricks in the book. def randomness_ex_1 ( keys ): # Your answer here! pass from dl_workshop.jax_idioms import randomness_ex_1 final , history = randomness_ex_1 ( keys , starting_position ) plt . plot ( history [:, 0 ], history [:, 1 ], alpha = 0.5 )","title":"Exercise 1: Brownian motion on a grid"},{"location":"02-jax-idioms/03-deterministic-randomness/#exercise-2-stochastic-stick-breaking","text":"In the previous notebook, we introduced you to the stick-breaking process, and we asked you to write it in a non-stochastic fashion. We're now going to have you write it using a stochastic draw. To do so, however, you need to be familiar with the Beta distribution , which models a random draw from the interval x \\in (0, 1) x \\in (0, 1) . Here is how you can draw numbers from the Beta distribution: betadraw = random . beta ( key , a = 1 , b = 2 ) betadraw Now, I'm going to show you the NumPy + Python equivalent of the real (i.e. stochastic) stick-breaking process: import jax.numpy as np num_breaks = 30 keys = random . split ( key , num_breaks ) concentration = 5 sticks = [] stick_length = 1.0 for k in keys : breaking_fraction = random . beta ( k , a = 1 , b = concentration ) stick = stick_length * breaking_fraction sticks . append ( stick ) stick_length = stick_length - stick result = np . array ( sticks ) result Now, your task is to implement it using lax.scan . def randomness_ex_2 ( key , num_breaks , concentration : float ): # Your answer here! pass # Comment out the import to test your answer! from dl_workshop.jax_idioms import randomness_ex_2 final , sticks = randomness_ex_2 ( key , num_breaks , concentration ) assert np . allclose ( sticks , result )","title":"Exercise 2: Stochastic stick breaking"},{"location":"02-jax-idioms/03-deterministic-randomness/#exercise-3-multiple-grws","text":"Now, what if we wanted to generate multiple realizations of the Gaussian random walk? Does this sound familiar? If so... yeah, it's a vanilla for-loop, which directly brings us to vmap ! And that's what we're going to try to implement in this exercise. from functools import partial from jax import vmap The key idea here is to vmap the grw_draw function across multiple PRNGKeys. That way, you can avoid doing a for-loop, which is the goal of this exercise too. You get to decide how many realizations of the GRW you'd like to create. def randomness_ex_3 ( key , num_realizations = 20 , grw_draw = grw_draw ): # Your answer here pass from dl_workshop.jax_idioms import randomness_ex_3 final , trajectories = randomness_ex_3 ( key , num_realizations = 20 , grw_draw = grw_draw ) trajectories . shape We did it! We have 20 trajectories of a 1000-step Gaussian random walk. Notice also how the program is structured very nicely: Each layer of abstraction in the program corresponds to a new axis dimension along which we are working. The onion layering of the program has very natural structure for the problem at hand. Effectively, we have planned out, or perhaps staged out, our computation using Python before actually executing it. Let's visualize the trajectories to make sure they are really GRW-like. import seaborn as sns fig , ax = plt . subplots () for trajectory in trajectories [ 0 : 20 ]: ax . plot ( trajectory ) sns . despine ()","title":"Exercise 3: Multiple GRWs"},{"location":"02-jax-idioms/04-optimized-learning/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Optimized Learning In this notebook, we will take a look at how to transform our numerical programs into their derivatives . Autograd to JAX Before they worked on JAX, there was another Python package called autograd that some of the JAX developers worked on. That was where the original idea of building an automatic differentiation system on top of NumPy started. Example: Transforming a function into its derivative Just like vmap , grad takes in a function and transforms it into another function. By default, the returned function from grad is the derivative of the function with respect to the first argument. Let's see an example of it in action using the simple math function: f(x) = 3x + 1 f(x) = 3x + 1 # Example 1: from jax import grad def func ( x ): return 3 * x + 1 df = grad ( func ) # Pass in any float value of x, you should get back 3.0 as the _gradient_. df ( 4.0 ) Here's another example using a polynomial function: f(x) = 3x^2 + 4x -3 f(x) = 3x^2 + 4x -3 Its derivative function is: f'(x) = 6x + 4 f'(x) = 6x + 4 <span><span class=\"MathJax_Preview\">f'(x) = 6x + 4</span><script type=\"math/tex\">f'(x) = 6x + 4 . # Example 2: def polynomial ( x ): return 3 * x ** 2 + 4 * x - 3 dpolynomial = grad ( polynomial ) # pass in any float value of x # the result will be evaluated at 6x + 4, # which is the gradient of the polynomial function. dpolynomial ( 3.0 ) Using grad to solve minimization problems Once we have access to the derivative function that we can evaluate, we can use it to solve optimization problems. Optimization problems are where one wishes to find the maxima or minima of a function. For example, if we take the polynomial function above, we can calculate its derivative function analytically as: f'(x) = 6x + 4 f'(x) = 6x + 4 At the minima, f'(x) f'(x) is zero, and solving for the value of x x , we get x = -\\frac{2}{3} x = -\\frac{2}{3} . # Example: find the minima of the polynomial function. start = 3.0 for i in range ( 200 ): start -= dpolynomial ( start ) * 0.01 start We know from calculus that the sign of the second derivative tells us whether we have a minima or maxima at a point. Analytically, the second derivative of our polynomial is: f''(x) = 6 f''(x) = 6 We can verify that the point is a minima by calling grad again on the derivative function. ddpolynomial = grad ( dpolynomial ) ddpolynomial ( start ) Grad is composable an arbitrary number of times. You can keep calling grad as many times as you like. Maximum likelihood estimation In statistics, maximum likelihood estimation is used to estimate the most likely value of a distribution's parameters. Usually, analytical solutions can be found; however, for difficult cases, we can always fall back on grad . Let's see this in action. Say we draw 1000 random numbers from a Gaussian with \\mu=-3 \\mu=-3 and \\sigma=2 \\sigma=2 . Our task is to pretend we don't know the actual \\mu \\mu and \\sigma \\sigma and instead estimate it from the observed data. from jax import random import jax.numpy as np from functools import partial key = random . PRNGKey ( 44 ) real_mu = - 3.0 real_log_sigma = np . log ( 2.0 ) # the real sigma is 2.0 data = random . normal ( key , shape = ( 1000 ,)) * np . exp ( real_log_sigma ) + real_mu Our estimation task will necessitate calculating the total joint log likelihood of our data under a Gaussian model. What we then need to do is to estimate \\mu \\mu and \\sigma \\sigma that maximizes the log likelihood of observing our data. Since we have been operating in a function minimization paradigm, we can instead minimize the negative log likelihood. from jax.scipy.stats import norm def negloglike ( mu , log_sigma , data ): return - np . sum ( norm . logpdf ( data , loc = mu , scale = np . exp ( log_sigma ))) If you're wondering why we use log_sigma rather than sigma , it is a choice made for practical reasons. When doing optimizations, we can possibly run into negative values, or more generally, values that are \"out of bounds\" for a parameter. Operating in log-space for a positive-only value allows us to optimize that value in an unbounded space, and we can use the log/exp transformations to bring our parameter into the correct space when necessary. Whenever doing likelihood calculations, it's always good practice to ensure that we have no NaN issues first. Let's check: mu = - 6.0 log_sigma = np . log ( 2.0 ) negloglike ( mu , log_sigma , data ) Now, we can create the gradient function of our negative log likelihood. But there's a snag! Doesn't grad take the derivative w.r.t. the first argument? We need it w.r.t. two arguments, mu and log_sigma . Well, grad has an argnums argument that we can use to specify with respect to which arguments of the function we wish to take the derivative for. dnegloglike = grad ( negloglike , argnums = ( 0 , 1 )) # condition on data dnegloglike = partial ( dnegloglike , data = data ) dnegloglike ( mu , log_sigma ) Now, we can do the gradient descent step! # gradient descent for i in range ( 300 ): dmu , dlog_sigma = dnegloglike ( mu , log_sigma ) mu -= dmu * 0.0001 log_sigma -= dlog_sigma * 0.0001 mu , np . exp ( log_sigma ) And voila! We have gradient descended our way to the maximum likelihood parameters :). Exercise: Where is the gold? It's at the minima! We're now going to attempt an exercise. The task here is to program a robot to find the gold in a field that is defined by a math function. def goldfield ( x , y ): \"\"\"All credit to https://www.analyzemath.com/calculus/multivariable/maxima_minima.html for this function.\"\"\" return ( 2 * x ** 2 ) - ( 4 * x * y ) + ( y ** 4 + 2 ) It should be evident from here that there are two minima in the function. Let's find out where they are. dgoldfield = grad ( goldfield , argnums = [ 0 , 1 ]) dgoldfield ( 3.0 , 4.0 ) # Start somewhere x , y = 0.1 , - 0.1 for i in range ( 300 ): dx , dy = dgoldfield ( x , y ) x -= dx * 0.01 y -= dy * 0.01 x , y Exercise: programming a robot that only moves along one axis Our robot has had a malfunction, and it now can only flow along one axis. Can you help it find the minima nonetheless? (This is effectively a problem of finding the partial derivative! You can fix either the x or y to your value of choice.) dgoldfield = grad ( partial ( goldfield , y = 1.2 )) # Start somewhere x = 0.1 for i in range ( 300 ): dx = dgoldfield ( x ) x -= dx * 0.01 x import matplotlib.pyplot as plt from matplotlib import cm fig , ax = plt . subplots ( subplot_kw = { \"projection\" : \"3d\" }) # Make data. X = np . arange ( - 1.5 , 1.5 , 0.01 ) Y = np . arange ( - 1.5 , 1.5 , 0.01 ) X , Y = np . meshgrid ( X , Y ) Z = goldfield ( X , Y ) # Plot the surface. surf = ax . plot_surface ( X , Y , Z , cmap = cm . coolwarm , linewidth = 0 , antialiased = False , ) ax . view_init ( elev = 20.0 , azim = 20 )","title":"Optimized Learning"},{"location":"02-jax-idioms/04-optimized-learning/#optimized-learning","text":"In this notebook, we will take a look at how to transform our numerical programs into their derivatives .","title":"Optimized Learning"},{"location":"02-jax-idioms/04-optimized-learning/#autograd-to-jax","text":"Before they worked on JAX, there was another Python package called autograd that some of the JAX developers worked on. That was where the original idea of building an automatic differentiation system on top of NumPy started.","title":"Autograd to JAX"},{"location":"02-jax-idioms/04-optimized-learning/#example-transforming-a-function-into-its-derivative","text":"Just like vmap , grad takes in a function and transforms it into another function. By default, the returned function from grad is the derivative of the function with respect to the first argument. Let's see an example of it in action using the simple math function: f(x) = 3x + 1 f(x) = 3x + 1 # Example 1: from jax import grad def func ( x ): return 3 * x + 1 df = grad ( func ) # Pass in any float value of x, you should get back 3.0 as the _gradient_. df ( 4.0 ) Here's another example using a polynomial function: f(x) = 3x^2 + 4x -3 f(x) = 3x^2 + 4x -3 Its derivative function is: f'(x) = 6x + 4 f'(x) = 6x + 4 <span><span class=\"MathJax_Preview\">f'(x) = 6x + 4</span><script type=\"math/tex\">f'(x) = 6x + 4 . # Example 2: def polynomial ( x ): return 3 * x ** 2 + 4 * x - 3 dpolynomial = grad ( polynomial ) # pass in any float value of x # the result will be evaluated at 6x + 4, # which is the gradient of the polynomial function. dpolynomial ( 3.0 )","title":"Example: Transforming a function into its derivative"},{"location":"02-jax-idioms/04-optimized-learning/#using-grad-to-solve-minimization-problems","text":"Once we have access to the derivative function that we can evaluate, we can use it to solve optimization problems. Optimization problems are where one wishes to find the maxima or minima of a function. For example, if we take the polynomial function above, we can calculate its derivative function analytically as: f'(x) = 6x + 4 f'(x) = 6x + 4 At the minima, f'(x) f'(x) is zero, and solving for the value of x x , we get x = -\\frac{2}{3} x = -\\frac{2}{3} . # Example: find the minima of the polynomial function. start = 3.0 for i in range ( 200 ): start -= dpolynomial ( start ) * 0.01 start We know from calculus that the sign of the second derivative tells us whether we have a minima or maxima at a point. Analytically, the second derivative of our polynomial is: f''(x) = 6 f''(x) = 6 We can verify that the point is a minima by calling grad again on the derivative function. ddpolynomial = grad ( dpolynomial ) ddpolynomial ( start ) Grad is composable an arbitrary number of times. You can keep calling grad as many times as you like.","title":"Using grad to solve minimization problems"},{"location":"02-jax-idioms/04-optimized-learning/#maximum-likelihood-estimation","text":"In statistics, maximum likelihood estimation is used to estimate the most likely value of a distribution's parameters. Usually, analytical solutions can be found; however, for difficult cases, we can always fall back on grad . Let's see this in action. Say we draw 1000 random numbers from a Gaussian with \\mu=-3 \\mu=-3 and \\sigma=2 \\sigma=2 . Our task is to pretend we don't know the actual \\mu \\mu and \\sigma \\sigma and instead estimate it from the observed data. from jax import random import jax.numpy as np from functools import partial key = random . PRNGKey ( 44 ) real_mu = - 3.0 real_log_sigma = np . log ( 2.0 ) # the real sigma is 2.0 data = random . normal ( key , shape = ( 1000 ,)) * np . exp ( real_log_sigma ) + real_mu Our estimation task will necessitate calculating the total joint log likelihood of our data under a Gaussian model. What we then need to do is to estimate \\mu \\mu and \\sigma \\sigma that maximizes the log likelihood of observing our data. Since we have been operating in a function minimization paradigm, we can instead minimize the negative log likelihood. from jax.scipy.stats import norm def negloglike ( mu , log_sigma , data ): return - np . sum ( norm . logpdf ( data , loc = mu , scale = np . exp ( log_sigma ))) If you're wondering why we use log_sigma rather than sigma , it is a choice made for practical reasons. When doing optimizations, we can possibly run into negative values, or more generally, values that are \"out of bounds\" for a parameter. Operating in log-space for a positive-only value allows us to optimize that value in an unbounded space, and we can use the log/exp transformations to bring our parameter into the correct space when necessary. Whenever doing likelihood calculations, it's always good practice to ensure that we have no NaN issues first. Let's check: mu = - 6.0 log_sigma = np . log ( 2.0 ) negloglike ( mu , log_sigma , data ) Now, we can create the gradient function of our negative log likelihood. But there's a snag! Doesn't grad take the derivative w.r.t. the first argument? We need it w.r.t. two arguments, mu and log_sigma . Well, grad has an argnums argument that we can use to specify with respect to which arguments of the function we wish to take the derivative for. dnegloglike = grad ( negloglike , argnums = ( 0 , 1 )) # condition on data dnegloglike = partial ( dnegloglike , data = data ) dnegloglike ( mu , log_sigma ) Now, we can do the gradient descent step! # gradient descent for i in range ( 300 ): dmu , dlog_sigma = dnegloglike ( mu , log_sigma ) mu -= dmu * 0.0001 log_sigma -= dlog_sigma * 0.0001 mu , np . exp ( log_sigma ) And voila! We have gradient descended our way to the maximum likelihood parameters :).","title":"Maximum likelihood estimation"},{"location":"02-jax-idioms/04-optimized-learning/#exercise-where-is-the-gold-its-at-the-minima","text":"We're now going to attempt an exercise. The task here is to program a robot to find the gold in a field that is defined by a math function. def goldfield ( x , y ): \"\"\"All credit to https://www.analyzemath.com/calculus/multivariable/maxima_minima.html for this function.\"\"\" return ( 2 * x ** 2 ) - ( 4 * x * y ) + ( y ** 4 + 2 ) It should be evident from here that there are two minima in the function. Let's find out where they are. dgoldfield = grad ( goldfield , argnums = [ 0 , 1 ]) dgoldfield ( 3.0 , 4.0 ) # Start somewhere x , y = 0.1 , - 0.1 for i in range ( 300 ): dx , dy = dgoldfield ( x , y ) x -= dx * 0.01 y -= dy * 0.01 x , y","title":"Exercise: Where is the gold? It's at the minima!"},{"location":"02-jax-idioms/04-optimized-learning/#exercise-programming-a-robot-that-only-moves-along-one-axis","text":"Our robot has had a malfunction, and it now can only flow along one axis. Can you help it find the minima nonetheless? (This is effectively a problem of finding the partial derivative! You can fix either the x or y to your value of choice.) dgoldfield = grad ( partial ( goldfield , y = 1.2 )) # Start somewhere x = 0.1 for i in range ( 300 ): dx = dgoldfield ( x ) x -= dx * 0.01 x import matplotlib.pyplot as plt from matplotlib import cm fig , ax = plt . subplots ( subplot_kw = { \"projection\" : \"3d\" }) # Make data. X = np . arange ( - 1.5 , 1.5 , 0.01 ) Y = np . arange ( - 1.5 , 1.5 , 0.01 ) X , Y = np . meshgrid ( X , Y ) Z = goldfield ( X , Y ) # Plot the surface. surf = ax . plot_surface ( X , Y , Z , cmap = cm . coolwarm , linewidth = 0 , antialiased = False , ) ax . view_init ( elev = 20.0 , azim = 20 )","title":"Exercise: programming a robot that only moves along one axis"},{"location":"02-jax-idioms/05-jit/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Introduction JAX has a jit function that allows us to just-in-time compile functions written using JAX's NumPy and SciPy-wrapped functions. JIT stands for \"just-in-time\" compilation, which stands in contrast to AOT (ahead-of-time). Using jit should give you speed-ups compared to not using it. In this notebook, we are going to explore the gains that we expect to get by using JAX's just-in-time compilation function jit . Because JIT compilation is usually simply applied on top of existing functions, we'll explore its primarily by examples rather than by exercises. JIT example from the JAX docs Coming up with an example where JIT compilation could be useful is quite a challenge, so let's start off with an examplee from the JAX docs. The function in question is the SELU function, which is an activation function applied elementwise to the outputs of a neural network layer. import jax.numpy as np def selu ( x , alpha = 1.67 , lmbda = 1.05 ): return lmbda * np . where ( x > 0 , x , alpha * np . exp ( x ) - alpha ) Timing the function without JIT compilation: from time import time from jax import random key = random . PRNGKey ( 44 ) x = random . normal ( key , ( 1000000 ,)) % timeit selu ( x ) . block_until_ready () Now, let's try JIT-compiling the function. from jax import jit selu_jit = jit ( selu ) % timeit selu_jit ( x ) . block_until_ready () As we can see, the JIT-compiled function is about 3X faster than the non-JIT compiled function. More importantly, any function that you write using JAX-wrapped NumPy, JAX-wrapped SciPy, and its own provided lax submodule, can be JIT-compiled to gain speed-ups. Re-examining the Gaussian random walk Let's revisit the Gaussian random walk that we implemented as a case study in what happens when we use JAX's idioms to write our code. Pure Python version of the Gaussian random walk import numpy as onp def gaussian_random_walk_python ( num_realizations , num_timesteps ): rws = [] for i in range ( num_realizations ): rw = [] prev_draw = 0 for t in range ( num_timesteps ): prev_draw = onp . random . normal ( loc = prev_draw ) rw . append ( prev_draw ) rws . append ( rw ) return rws from time import time N_REALIZATIONS = 1_000 N_TIMESTEPS = 10_000 start = time () trajectories_python = gaussian_random_walk_python ( N_REALIZATIONS , N_TIMESTEPS ) end = time () print ( f \" { end - start : .2f } seconds\" ) import matplotlib.pyplot as plt import seaborn as sns for trajectory in trajectories_python [: 20 ]: plt . plot ( trajectory ) sns . despine () JAX implementation without JIT Now, let's take a look at the JAX-based implementation. from jax import lax , random key = random . PRNGKey ( 44 ) keys = random . split ( key , N_TIMESTEPS ) def new_draw ( prev_val , key ): \"\"\"lax.scannable function for drawing a new draw from the GRW.\"\"\" new = prev_val + random . normal ( key ) return new , prev_val def grw_draw ( key , num_steps ): \"\"\"One GRW draw over a bunch of steps.\"\"\" keys = random . split ( key , num_steps ) final , draws = lax . scan ( new_draw , 0.0 , keys ) return final , draws from functools import partial from jax import vmap def gaussian_random_walk_jax ( num_realizations , num_timesteps ): \"\"\"Multiple GRW draws.\"\"\" keys = random . split ( key , num_realizations ) grw_k_steps = partial ( grw_draw , num_steps = num_timesteps ) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories from jax import random key = random . PRNGKey ( 42 ) start = time () final_jax , trajectories_jax = gaussian_random_walk_jax ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jax . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) % timeit gaussian_random_walk_jax ( N_REALIZATIONS , N_TIMESTEPS )[ 1 ] . block_until_ready () for trajectory in trajectories_jax [: 20 ]: plt . plot ( trajectory ) sns . despine () JAX implementation with JIT compilation Now we're going to JIT-compile our Gaussian Random Walk function and see how long it takes for the program to run. from jax import jit def gaussian_random_walk_jit ( num_realizations , num_timesteps ): keys = random . split ( key , num_realizations ) grw_k_steps = jit ( partial ( grw_draw , num_steps = num_timesteps )) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories start = time () final_jit , trajectories_jit = gaussian_random_walk_jit ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jit . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) % timeit gaussian_random_walk_jit ( N_REALIZATIONS , N_TIMESTEPS )[ 1 ] . block_until_ready () for trajectory in trajectories_jit [: 20 ]: plt . plot ( trajectory ) sns . despine () It may appear that JIT-compilation doesn't appear to do much, but we can assure you that there's a great explanation for this phenomena. Within the Gaussian random walk, we used lax.scan , which itself gives us a fairly compiled operation already. The docs spell it out in jargon: Also unlike that Python version, scan is a JAX primitive and is lowered to a single XLA While HLO. That makes it useful for reducing compilation times for jit-compiled functions, since native Python loop constructs in an @jit function are unrolled, leading to large XLA computations. If we were to use a for-loop instead of lax.scan , then we would be missing out on te performance gain. So when we add in JIT-compilation on top of using lax.scan , the added gain is not as much as if we didn't use lax.scan . In both cases, the runtime is essentially constant JIT-compilation gave us about a 1-2X speedup over non-JIT compiled code, and was approximately at least 20X faster than the pure Python version. That shouldn't surprise you one bit :). A few pointers on syntax Firstly, if we subscribe to the Zen of Python's notion that \"flat is better than nested\", then by following the idioms listed here -- closures/partials, vmap and lax.scan , we'll likely only ever go one closure deep into our programs. Notice how we basically never wrote any for-loops in our array code; they were handled elegantly by the looping constructs vmap and lax.scan . Secondly, using jit , we get further optimizations on our code for free. A pre-requisite of jit is that the every function call made in the program function being jit -ed is required to be written in a \"pure functional\" style, i.e. there are no side effects, no mutation of global state. Put plainly, everything that you use inside the function should be passed in (with the exception of imports, of course). If you write a program using the idioms used here (closures to wrap state, vmap / lax.scan in lieu of loops, explicit random number generation using PRNGKeys), then you will be able to JIT compile the program with ease.","title":"05 jit"},{"location":"02-jax-idioms/05-jit/#introduction","text":"JAX has a jit function that allows us to just-in-time compile functions written using JAX's NumPy and SciPy-wrapped functions. JIT stands for \"just-in-time\" compilation, which stands in contrast to AOT (ahead-of-time). Using jit should give you speed-ups compared to not using it. In this notebook, we are going to explore the gains that we expect to get by using JAX's just-in-time compilation function jit . Because JIT compilation is usually simply applied on top of existing functions, we'll explore its primarily by examples rather than by exercises.","title":"Introduction"},{"location":"02-jax-idioms/05-jit/#jit-example-from-the-jax-docs","text":"Coming up with an example where JIT compilation could be useful is quite a challenge, so let's start off with an examplee from the JAX docs. The function in question is the SELU function, which is an activation function applied elementwise to the outputs of a neural network layer. import jax.numpy as np def selu ( x , alpha = 1.67 , lmbda = 1.05 ): return lmbda * np . where ( x > 0 , x , alpha * np . exp ( x ) - alpha ) Timing the function without JIT compilation: from time import time from jax import random key = random . PRNGKey ( 44 ) x = random . normal ( key , ( 1000000 ,)) % timeit selu ( x ) . block_until_ready () Now, let's try JIT-compiling the function. from jax import jit selu_jit = jit ( selu ) % timeit selu_jit ( x ) . block_until_ready () As we can see, the JIT-compiled function is about 3X faster than the non-JIT compiled function. More importantly, any function that you write using JAX-wrapped NumPy, JAX-wrapped SciPy, and its own provided lax submodule, can be JIT-compiled to gain speed-ups.","title":"JIT example from the JAX docs"},{"location":"02-jax-idioms/05-jit/#re-examining-the-gaussian-random-walk","text":"Let's revisit the Gaussian random walk that we implemented as a case study in what happens when we use JAX's idioms to write our code.","title":"Re-examining the Gaussian random walk"},{"location":"02-jax-idioms/05-jit/#pure-python-version-of-the-gaussian-random-walk","text":"import numpy as onp def gaussian_random_walk_python ( num_realizations , num_timesteps ): rws = [] for i in range ( num_realizations ): rw = [] prev_draw = 0 for t in range ( num_timesteps ): prev_draw = onp . random . normal ( loc = prev_draw ) rw . append ( prev_draw ) rws . append ( rw ) return rws from time import time N_REALIZATIONS = 1_000 N_TIMESTEPS = 10_000 start = time () trajectories_python = gaussian_random_walk_python ( N_REALIZATIONS , N_TIMESTEPS ) end = time () print ( f \" { end - start : .2f } seconds\" ) import matplotlib.pyplot as plt import seaborn as sns for trajectory in trajectories_python [: 20 ]: plt . plot ( trajectory ) sns . despine ()","title":"Pure Python version of the Gaussian random walk"},{"location":"02-jax-idioms/05-jit/#jax-implementation-without-jit","text":"Now, let's take a look at the JAX-based implementation. from jax import lax , random key = random . PRNGKey ( 44 ) keys = random . split ( key , N_TIMESTEPS ) def new_draw ( prev_val , key ): \"\"\"lax.scannable function for drawing a new draw from the GRW.\"\"\" new = prev_val + random . normal ( key ) return new , prev_val def grw_draw ( key , num_steps ): \"\"\"One GRW draw over a bunch of steps.\"\"\" keys = random . split ( key , num_steps ) final , draws = lax . scan ( new_draw , 0.0 , keys ) return final , draws from functools import partial from jax import vmap def gaussian_random_walk_jax ( num_realizations , num_timesteps ): \"\"\"Multiple GRW draws.\"\"\" keys = random . split ( key , num_realizations ) grw_k_steps = partial ( grw_draw , num_steps = num_timesteps ) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories from jax import random key = random . PRNGKey ( 42 ) start = time () final_jax , trajectories_jax = gaussian_random_walk_jax ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jax . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) % timeit gaussian_random_walk_jax ( N_REALIZATIONS , N_TIMESTEPS )[ 1 ] . block_until_ready () for trajectory in trajectories_jax [: 20 ]: plt . plot ( trajectory ) sns . despine ()","title":"JAX implementation without JIT"},{"location":"02-jax-idioms/05-jit/#jax-implementation-with-jit-compilation","text":"Now we're going to JIT-compile our Gaussian Random Walk function and see how long it takes for the program to run. from jax import jit def gaussian_random_walk_jit ( num_realizations , num_timesteps ): keys = random . split ( key , num_realizations ) grw_k_steps = jit ( partial ( grw_draw , num_steps = num_timesteps )) final , trajectories = vmap ( grw_k_steps )( keys ) return final , trajectories start = time () final_jit , trajectories_jit = gaussian_random_walk_jit ( N_REALIZATIONS , N_TIMESTEPS ) trajectories_jit . block_until_ready () end = time () print ( f \" { end - start : .2f } seconds\" ) % timeit gaussian_random_walk_jit ( N_REALIZATIONS , N_TIMESTEPS )[ 1 ] . block_until_ready () for trajectory in trajectories_jit [: 20 ]: plt . plot ( trajectory ) sns . despine () It may appear that JIT-compilation doesn't appear to do much, but we can assure you that there's a great explanation for this phenomena. Within the Gaussian random walk, we used lax.scan , which itself gives us a fairly compiled operation already. The docs spell it out in jargon: Also unlike that Python version, scan is a JAX primitive and is lowered to a single XLA While HLO. That makes it useful for reducing compilation times for jit-compiled functions, since native Python loop constructs in an @jit function are unrolled, leading to large XLA computations. If we were to use a for-loop instead of lax.scan , then we would be missing out on te performance gain. So when we add in JIT-compilation on top of using lax.scan , the added gain is not as much as if we didn't use lax.scan . In both cases, the runtime is essentially constant JIT-compilation gave us about a 1-2X speedup over non-JIT compiled code, and was approximately at least 20X faster than the pure Python version. That shouldn't surprise you one bit :).","title":"JAX implementation with JIT compilation"},{"location":"02-jax-idioms/05-jit/#a-few-pointers-on-syntax","text":"Firstly, if we subscribe to the Zen of Python's notion that \"flat is better than nested\", then by following the idioms listed here -- closures/partials, vmap and lax.scan , we'll likely only ever go one closure deep into our programs. Notice how we basically never wrote any for-loops in our array code; they were handled elegantly by the looping constructs vmap and lax.scan . Secondly, using jit , we get further optimizations on our code for free. A pre-requisite of jit is that the every function call made in the program function being jit -ed is required to be written in a \"pure functional\" style, i.e. there are no side effects, no mutation of global state. Put plainly, everything that you use inside the function should be passed in (with the exception of imports, of course). If you write a program using the idioms used here (closures to wrap state, vmap / lax.scan in lieu of loops, explicit random number generation using PRNGKeys), then you will be able to JIT compile the program with ease.","title":"A few pointers on syntax"},{"location":"02-jax-idioms/05-tips/","text":"Carefully design your array data structure semantics! Think about what your elementary operations are - they should be as close to a vector as possible, and build them out from there.","title":"05 tips"},{"location":"03-stax/01-linear/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Writing linear models with stax In this notebook, I'll show the code for how to use JAX's stax submodule to write arbitrary models. Prerequisites I'm assuming you have read through the jax-programming.ipynb notebook, as well as the tutorial.ipynb notebook. The main tutorial.ipynb notebook gives you a general introduction to differential programming using grad , while the jax-programming.ipynb notebook gives you a flavour of the other four main JAX idioms: vmap , lax.scan , random.PRNGKey , and jit . What is stax ? Most deep learning libraries use objects as the data structure for a neural network layer. As such, the tunable parameters of the layer, for example w and b for a linear (\"dense\") layer are class attributes associated with the forward function. In some sense, because a neural network layer is nothing more than a math function, specifying the layer in terms of a function might also make sense. stax , then, is a new take on writing neural network models using pure functions rather than objects. How does stax work? The way that stax layers work is as follows. Every neural network layer is nothing more than a math function with a \"forward\" pass. Neural network models typically have their parameters initialized into the right shapes using random number generators. Put these two together, and we have a pair of functions that specify a layer: An init_fun function, that initializes parameters into the correct shapes, and An apply_fun function, that applies the specified math transformations onto incoming data, using parameters of the correct shape. Example: Linear layer Let's see an example of this in action, by studying the implementation of the linear (\"dense\") layer in stax from jax.experimental import stax stax . Dense ?? As you can see, the apply_fun specifies the linear transformation. It accepts a parameter called params , which gets tuple-unpacked into the appropriate W and b . Notice how the params argument matches up with the second output of init_fun ! The init_fun always accepts an rng parameter, which is returned from JAX's jax.random.PRNGKey() . It also accepts an input_shape parameter, which specifies what the elementary shape of one sample of data is. So if your entire dataset is of shape (n_samples, n_columns) , then you would put in (n_columns,) inside there, as you would want to ignore the sample dimension, thus allowing us to take advantage of vmap to map our model function over each and every i.i.d. sample in our dataset. The init_fun also returns the output_shape , which is used later when we chain layers together. Let's see how we can use the Dense layer to specify a linear regression model. Create the initialization and application function pairs Firstly, we create the init_fun and apply_fun pair: init_fun , apply_fun = stax . Dense ( 1 ) Initialize the parameters Now, let's initialize parameters using the init_fun . Let's assume that we have data that is of 4 columns only. from jax import random , numpy as np key = random . PRNGKey ( 42 ) output_shape , params_initial = init_fun ( key , input_shape = ( 4 ,)) params_initial Apply parameters and data through function We'll create some randomly generated data. X = random . normal ( key , shape = ( 200 , 4 )) X [ 0 : 5 ], X . shape Here's some y_true values that I've snuck in. y_true = np . dot ( X , np . array ([ 1 , 2 , 3 , 4 ])) + 5 y_true = y_true . reshape ( - 1 , 1 ) y_true [ 0 : 5 ], y_true . shape Now, we'll pass data through the linear model! apply_fun ?? from jax import vmap from functools import partial y_pred = vmap ( partial ( apply_fun , params_initial ))( X ) y_pred [ 0 : 5 ], y_pred . shape Voil\u00e0! We have a simple linear model implemented just like that. Optimization Next question: how do we optimize the parameters using JAX? Instead of writing a training loop on our own, we can take advantage of JAX's optimizers, which are also written in a functional paradigm! JAX's optimizers are constructed as a \"triplet\" set of functions: init : Takes params and initializes them in as a state , which is structured in a fashion that update can operate on. update : Takes in i , g , and state , which respectively are: i : The current loop iteration g : Gradients calculated from grad ! state : The current state of the parameters. get_params : Takes in the state at a given point, and returns the parameters structured correctly. from jax import jit , grad from jax.experimental.optimizers import adam init , update , get_params = adam ( step_size = 1e-1 ) update = jit ( update ) get_params = jit ( get_params ) Loss Function We're still missing a piece here, that is the loss function. For illustration purposes, let's use the mean squared error. def mseloss ( params , model , x , y_true ): y_preds = vmap ( partial ( model , params ))( x ) return np . mean ( np . power ( y_preds - y_true , 2 )) dmseloss = grad ( mseloss ) \"Step\" portion of update loop Now, we're going to define the \"step\" portion of the update loop. from dl_workshop.stax_models import step step ?? JIT compilation Because it takes so many parameters (in order to remain pure, and not rely on notebook state), we're going to bind some of them using functools.partial . I'm also going to show you what happens when we JIT-compile vs. don't JIT-compile the function. step_partial = partial ( step , get_params = get_params , dlossfunc = dmseloss , update = update , model = apply_fun , x = X , y_true = y_true ) step_partial_jit = jit ( step_partial ) Explicit loops Firstly, let's see what kind of code we'd write if we did write the loop explicitly. from time import time start = time () state = init ( params_initial ) for i in range ( 1000 ): params = get_params ( state ) g = dmseloss ( params , apply_fun , X , y_true ) state = update ( i , g , state ) end = time () print ( end - start ) Partialled out loop step Now, let's run the loop with the partialled out function. start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial ( i , state ) end = time () print ( end - start ) JIT-compiled loop! This is much cleaner of a loop, but we did have to do some work up-front. What happens if we now use the JIT-ed function? start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial_jit ( i , state ) end = time () print ( end - start ) Whoa, holy smokes, that's fast! At least 10X faster using JIT-compilation. lax.scan loop Now we'll use some JAX trickery ot write a training loop without ever writing a for-loop. from dl_workshop.stax_models import make_scannable_step make_scannable_step ?? from jax import lax scannable_step = make_scannable_step ( step_partial_jit ) start = time () initial_state = init ( params_initial ) final_state , states_history = lax . scan ( scannable_step , initial_state , np . arange ( 1000 )) end = time () print ( end - start ) get_params ( final_state ) vmap -ed training loop over multiple starting points Now, we're going to do the ultimate: we'll create at least 100 different parameter initializations and run our training loop over each of them. from dl_workshop.stax_models import make_training_start make_training_start ?? from jax import lax train_linear = make_training_start ( partial ( init_fun , input_shape = ( - 1 , 4 )), init , scannable_step , 1000 ) start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) final_states , states_histories = vmap ( train_linear )( initialization_keys ) end = time () print ( end - start ) w_final , b_final = vmap ( get_params )( final_states ) w_final . squeeze ()[ 0 : 5 ] b_final . squeeze ()[ 0 : 5 ] Looks like we were also able to run the whole optimization pretty fast, and recover the correct parameters over multiple training starts. JIT-compiled training loop What happens if we JIT-compile the vmapped initialization? start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) train_linear_jit = jit ( train_linear ) final_states , states_histories = vmap ( train_linear_jit )( initialization_keys ) vmap ( get_params )( final_states ) # this line exists to just block the computation until it completes. end = time () print ( end - start ) HOOOOOLY SMOKES! Did you see that? With JIT-compilation, we essentially took the training time down to be identical to training on one starting point. Naturally, I don't expect this result to hold 100% of the time, but it's pretty darn rad to see that live. The craziest piece here is that we could vmap our training loop over multiple starting points and get massive speedups there.","title":"Linear Models with stax"},{"location":"03-stax/01-linear/#writing-linear-models-with-stax","text":"In this notebook, I'll show the code for how to use JAX's stax submodule to write arbitrary models.","title":"Writing linear models with stax"},{"location":"03-stax/01-linear/#prerequisites","text":"I'm assuming you have read through the jax-programming.ipynb notebook, as well as the tutorial.ipynb notebook. The main tutorial.ipynb notebook gives you a general introduction to differential programming using grad , while the jax-programming.ipynb notebook gives you a flavour of the other four main JAX idioms: vmap , lax.scan , random.PRNGKey , and jit .","title":"Prerequisites"},{"location":"03-stax/01-linear/#what-is-stax","text":"Most deep learning libraries use objects as the data structure for a neural network layer. As such, the tunable parameters of the layer, for example w and b for a linear (\"dense\") layer are class attributes associated with the forward function. In some sense, because a neural network layer is nothing more than a math function, specifying the layer in terms of a function might also make sense. stax , then, is a new take on writing neural network models using pure functions rather than objects.","title":"What is stax?"},{"location":"03-stax/01-linear/#how-does-stax-work","text":"The way that stax layers work is as follows. Every neural network layer is nothing more than a math function with a \"forward\" pass. Neural network models typically have their parameters initialized into the right shapes using random number generators. Put these two together, and we have a pair of functions that specify a layer: An init_fun function, that initializes parameters into the correct shapes, and An apply_fun function, that applies the specified math transformations onto incoming data, using parameters of the correct shape.","title":"How does stax work?"},{"location":"03-stax/01-linear/#example-linear-layer","text":"Let's see an example of this in action, by studying the implementation of the linear (\"dense\") layer in stax from jax.experimental import stax stax . Dense ?? As you can see, the apply_fun specifies the linear transformation. It accepts a parameter called params , which gets tuple-unpacked into the appropriate W and b . Notice how the params argument matches up with the second output of init_fun ! The init_fun always accepts an rng parameter, which is returned from JAX's jax.random.PRNGKey() . It also accepts an input_shape parameter, which specifies what the elementary shape of one sample of data is. So if your entire dataset is of shape (n_samples, n_columns) , then you would put in (n_columns,) inside there, as you would want to ignore the sample dimension, thus allowing us to take advantage of vmap to map our model function over each and every i.i.d. sample in our dataset. The init_fun also returns the output_shape , which is used later when we chain layers together. Let's see how we can use the Dense layer to specify a linear regression model.","title":"Example: Linear layer"},{"location":"03-stax/01-linear/#create-the-initialization-and-application-function-pairs","text":"Firstly, we create the init_fun and apply_fun pair: init_fun , apply_fun = stax . Dense ( 1 )","title":"Create the initialization and application function pairs"},{"location":"03-stax/01-linear/#initialize-the-parameters","text":"Now, let's initialize parameters using the init_fun . Let's assume that we have data that is of 4 columns only. from jax import random , numpy as np key = random . PRNGKey ( 42 ) output_shape , params_initial = init_fun ( key , input_shape = ( 4 ,)) params_initial","title":"Initialize the parameters"},{"location":"03-stax/01-linear/#apply-parameters-and-data-through-function","text":"We'll create some randomly generated data. X = random . normal ( key , shape = ( 200 , 4 )) X [ 0 : 5 ], X . shape Here's some y_true values that I've snuck in. y_true = np . dot ( X , np . array ([ 1 , 2 , 3 , 4 ])) + 5 y_true = y_true . reshape ( - 1 , 1 ) y_true [ 0 : 5 ], y_true . shape Now, we'll pass data through the linear model! apply_fun ?? from jax import vmap from functools import partial y_pred = vmap ( partial ( apply_fun , params_initial ))( X ) y_pred [ 0 : 5 ], y_pred . shape Voil\u00e0! We have a simple linear model implemented just like that.","title":"Apply parameters and data through function"},{"location":"03-stax/01-linear/#optimization","text":"Next question: how do we optimize the parameters using JAX? Instead of writing a training loop on our own, we can take advantage of JAX's optimizers, which are also written in a functional paradigm! JAX's optimizers are constructed as a \"triplet\" set of functions: init : Takes params and initializes them in as a state , which is structured in a fashion that update can operate on. update : Takes in i , g , and state , which respectively are: i : The current loop iteration g : Gradients calculated from grad ! state : The current state of the parameters. get_params : Takes in the state at a given point, and returns the parameters structured correctly. from jax import jit , grad from jax.experimental.optimizers import adam init , update , get_params = adam ( step_size = 1e-1 ) update = jit ( update ) get_params = jit ( get_params )","title":"Optimization"},{"location":"03-stax/01-linear/#loss-function","text":"We're still missing a piece here, that is the loss function. For illustration purposes, let's use the mean squared error. def mseloss ( params , model , x , y_true ): y_preds = vmap ( partial ( model , params ))( x ) return np . mean ( np . power ( y_preds - y_true , 2 )) dmseloss = grad ( mseloss )","title":"Loss Function"},{"location":"03-stax/01-linear/#step-portion-of-update-loop","text":"Now, we're going to define the \"step\" portion of the update loop. from dl_workshop.stax_models import step step ??","title":"\"Step\" portion of update loop"},{"location":"03-stax/01-linear/#jit-compilation","text":"Because it takes so many parameters (in order to remain pure, and not rely on notebook state), we're going to bind some of them using functools.partial . I'm also going to show you what happens when we JIT-compile vs. don't JIT-compile the function. step_partial = partial ( step , get_params = get_params , dlossfunc = dmseloss , update = update , model = apply_fun , x = X , y_true = y_true ) step_partial_jit = jit ( step_partial )","title":"JIT compilation"},{"location":"03-stax/01-linear/#explicit-loops","text":"Firstly, let's see what kind of code we'd write if we did write the loop explicitly. from time import time start = time () state = init ( params_initial ) for i in range ( 1000 ): params = get_params ( state ) g = dmseloss ( params , apply_fun , X , y_true ) state = update ( i , g , state ) end = time () print ( end - start )","title":"Explicit loops"},{"location":"03-stax/01-linear/#partialled-out-loop-step","text":"Now, let's run the loop with the partialled out function. start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial ( i , state ) end = time () print ( end - start )","title":"Partialled out loop step"},{"location":"03-stax/01-linear/#jit-compiled-loop","text":"This is much cleaner of a loop, but we did have to do some work up-front. What happens if we now use the JIT-ed function? start = time () state = init ( params_initial ) for i in range ( 1000 ): state = step_partial_jit ( i , state ) end = time () print ( end - start ) Whoa, holy smokes, that's fast! At least 10X faster using JIT-compilation.","title":"JIT-compiled loop!"},{"location":"03-stax/01-linear/#laxscan-loop","text":"Now we'll use some JAX trickery ot write a training loop without ever writing a for-loop. from dl_workshop.stax_models import make_scannable_step make_scannable_step ?? from jax import lax scannable_step = make_scannable_step ( step_partial_jit ) start = time () initial_state = init ( params_initial ) final_state , states_history = lax . scan ( scannable_step , initial_state , np . arange ( 1000 )) end = time () print ( end - start ) get_params ( final_state )","title":"lax.scan loop"},{"location":"03-stax/01-linear/#vmap-ed-training-loop-over-multiple-starting-points","text":"Now, we're going to do the ultimate: we'll create at least 100 different parameter initializations and run our training loop over each of them. from dl_workshop.stax_models import make_training_start make_training_start ?? from jax import lax train_linear = make_training_start ( partial ( init_fun , input_shape = ( - 1 , 4 )), init , scannable_step , 1000 ) start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) final_states , states_histories = vmap ( train_linear )( initialization_keys ) end = time () print ( end - start ) w_final , b_final = vmap ( get_params )( final_states ) w_final . squeeze ()[ 0 : 5 ] b_final . squeeze ()[ 0 : 5 ] Looks like we were also able to run the whole optimization pretty fast, and recover the correct parameters over multiple training starts.","title":"vmap-ed training loop over multiple starting points"},{"location":"03-stax/01-linear/#jit-compiled-training-loop","text":"What happens if we JIT-compile the vmapped initialization? start = time () N_INITIALIZATIONS = 100 initialization_keys = random . split ( key , N_INITIALIZATIONS ) train_linear_jit = jit ( train_linear ) final_states , states_histories = vmap ( train_linear_jit )( initialization_keys ) vmap ( get_params )( final_states ) # this line exists to just block the computation until it completes. end = time () print ( end - start ) HOOOOOLY SMOKES! Did you see that? With JIT-compilation, we essentially took the training time down to be identical to training on one starting point. Naturally, I don't expect this result to hold 100% of the time, but it's pretty darn rad to see that live. The craziest piece here is that we could vmap our training loop over multiple starting points and get massive speedups there.","title":"JIT-compiled training loop"},{"location":"03-stax/02-neural/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Writing neural network models using stax We're now going to try rewriting the neural network model that we had earlier on, now using stax syntax, and traing it using the syntax that we have learned above. Using stax.serial Firstly, let's replicate the model using stax.serial . It's a serial composition of a Dense+Tanh layer, followed by a Dense+Sigmoid layer. from jax.experimental import stax nn_init , nn_apply = stax . serial ( stax . Dense ( 20 ), stax . Tanh , stax . Dense ( 1 ), stax . Sigmoid ) def nn_init_wrapper ( input_shape ): def inner ( key ): return nn_init ( key , input_shape ) return inner nn_initializer = nn_init_wrapper ( input_shape = ( - 1 , 41 )) nn_initializer Now, we initialize one instance of the parameters. from jax import random key = random . PRNGKey ( 42 ) output_shape , params_init = nn_initializer ( key ) We'll need a loss funciton to optimize as well. from jax import grad , numpy as np , vmap from functools import partial def binary_cross_entropy ( y_true , y_pred , tol = 1e-6 ): return y_true * np . log ( y_pred + tol ) + ( 1 - y_true ) * np . log ( 1 - y_pred + tol ) def logistic_loss ( params , model , x , y ): preds = vmap ( partial ( model , params ))( x ) bces = vmap ( binary_cross_entropy )( y , preds ) return - np . sum ( bces ) dlogistic_loss = grad ( logistic_loss ) Load in data Now, we load in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 ) Test-drive functions to make sure they work Always important. It'll reveal whether there's anything wrong with our code. logistic_loss ( params_init , nn_apply , X . values , y . values ) Progressively construct our training functions Firstly, we make sure the step function works with our logistic loss, model func, and actual data. from jax.experimental.optimizers import adam adam_init , update , get_params = adam ( 0.0005 ) from dl_workshop.stax_models import step , make_scannable_step , make_training_start from time import time stepfunc_nn = partial ( step , dlossfunc = dlogistic_loss , get_params = get_params , update = update , model = nn_apply , x = X . values , y_true = y . values ) scannable_step = make_scannable_step ( stepfunc_nn ) train_nn = make_training_start ( nn_initializer , adam_init , scannable_step , n_steps = 3000 ) start = time () final_state , states_history = train_nn ( key ) end = time () print ( end - start ) Friends, if you remember where we started in the tutorial.ipynb notebook, the original neural network took approximately a minute to train on a GPU (and longer if on a CPU). Let's now start by ploting the loss over training iterations. We start first with a function that returns the loss from a given state object. import matplotlib.pyplot as plt def calculate_loss ( state , get_params , model , lossfunc , x , y ): params = get_params ( state ) return lossfunc ( params , model , x , y ) calculate_loss ( final_state , get_params , nn_apply , logistic_loss , X . values , y . values ) Now, we need to vmap it over all states in the states history, to get back the loss score. calc_loss_vmap = partial ( calculate_loss , get_params = get_params , model = nn_apply , lossfunc = logistic_loss , x = X . values , y = y . values ) start = time () losses = vmap ( calc_loss_vmap )( states_history ) end = time () print ( end - start ) plt . plot ( losses ) Training with multiple starting points Just as above, we can also train the neural network with multiple starting points, again by vmap -ing our training function across split PRNGKeys. keys = random . split ( key , 5 ) start = time () final_states , state_histories = vmap ( train_nn )( keys ) end = time () print ( end - start ) get_params ( final_states )[ 0 ][ 0 ] . shape Let's plot the losses over each of the state histories. Our last function calc_loss_vmap calculates loss score for one time point, which we then vmap over a single states_history , so we need another function that encapsulates this behaviour and vmap s over all state histories. def state_history_loss ( state_history ): losses = vmap ( calc_loss_vmap )( state_history ) return losses losses = vmap ( state_history_loss )( state_histories ) losses . shape losses Correctly-shaped! And now plotting it... plt . plot ( losses . T ) Now that's pretty cool! We were able to see the loss from three independent runs. With sufficient memory, one would be able to do more runs; when I was writing this notebook early on, I saw that it was getting difficult to do on the order of tens of runs due to memory allocation issues. Summary In this notebook, we saw a few things in action. Firstly, we saw how to use the stax module on a linear model. Anytime we have a new framework for doing differential programming, it's super important to be able to explore it in the context of a linear model, which is basically the foundation of all deep learning. Secondly, we also explored how to leverage the JAX idioms to create fast parallelized training loops. We mixed-and-matched together jit , vmap , lax.scan , and grad into a performant training loop that was minimally nested. A corollary of this programming style is that every piece of the code can, in principle, be properly tested , because they are properly isolated. Have you written training loops where you modify a little piece here and a little piece there, until you lost what your original working one looked like? With training functions that are minimally nested, we can control the behaviour explicitly using closures/partials easily. Even when doing experimenation, our code can run reliably and fast. Thirdly, we saw how to apply the same lessons to training a neural network really fast with multiple starting points. The essence of the solution was to properly structure our program in progressively higher level layers of abstraction. We carefully wrote the program to go from the inner most layer out until we hit our goal of allowing for a set of multiple starts. The key here is that each level of abstraction is very natural, and corresponds to a \"unit computation\" being applied consistently across an \"array\" of things. Once we identify that \"unit computation\", writing the vmap -able or lax.scan -able function becomes very easy.","title":"Neural Networks with stax"},{"location":"03-stax/02-neural/#writing-neural-network-models-using-stax","text":"We're now going to try rewriting the neural network model that we had earlier on, now using stax syntax, and traing it using the syntax that we have learned above.","title":"Writing neural network models using stax"},{"location":"03-stax/02-neural/#using-staxserial","text":"Firstly, let's replicate the model using stax.serial . It's a serial composition of a Dense+Tanh layer, followed by a Dense+Sigmoid layer. from jax.experimental import stax nn_init , nn_apply = stax . serial ( stax . Dense ( 20 ), stax . Tanh , stax . Dense ( 1 ), stax . Sigmoid ) def nn_init_wrapper ( input_shape ): def inner ( key ): return nn_init ( key , input_shape ) return inner nn_initializer = nn_init_wrapper ( input_shape = ( - 1 , 41 )) nn_initializer Now, we initialize one instance of the parameters. from jax import random key = random . PRNGKey ( 42 ) output_shape , params_init = nn_initializer ( key ) We'll need a loss funciton to optimize as well. from jax import grad , numpy as np , vmap from functools import partial def binary_cross_entropy ( y_true , y_pred , tol = 1e-6 ): return y_true * np . log ( y_pred + tol ) + ( 1 - y_true ) * np . log ( 1 - y_pred + tol ) def logistic_loss ( params , model , x , y ): preds = vmap ( partial ( model , params ))( x ) bces = vmap ( binary_cross_entropy )( y , preds ) return - np . sum ( bces ) dlogistic_loss = grad ( logistic_loss )","title":"Using stax.serial"},{"location":"03-stax/02-neural/#load-in-data","text":"Now, we load in the data. import pandas as pd from pyprojroot import here X = pd . read_csv ( here () / 'data/biodeg_X.csv' , index_col = 0 ) y = pd . read_csv ( here () / 'data/biodeg_y.csv' , index_col = 0 )","title":"Load in data"},{"location":"03-stax/02-neural/#test-drive-functions-to-make-sure-they-work","text":"Always important. It'll reveal whether there's anything wrong with our code. logistic_loss ( params_init , nn_apply , X . values , y . values )","title":"Test-drive functions to make sure they work"},{"location":"03-stax/02-neural/#progressively-construct-our-training-functions","text":"Firstly, we make sure the step function works with our logistic loss, model func, and actual data. from jax.experimental.optimizers import adam adam_init , update , get_params = adam ( 0.0005 ) from dl_workshop.stax_models import step , make_scannable_step , make_training_start from time import time stepfunc_nn = partial ( step , dlossfunc = dlogistic_loss , get_params = get_params , update = update , model = nn_apply , x = X . values , y_true = y . values ) scannable_step = make_scannable_step ( stepfunc_nn ) train_nn = make_training_start ( nn_initializer , adam_init , scannable_step , n_steps = 3000 ) start = time () final_state , states_history = train_nn ( key ) end = time () print ( end - start ) Friends, if you remember where we started in the tutorial.ipynb notebook, the original neural network took approximately a minute to train on a GPU (and longer if on a CPU). Let's now start by ploting the loss over training iterations. We start first with a function that returns the loss from a given state object. import matplotlib.pyplot as plt def calculate_loss ( state , get_params , model , lossfunc , x , y ): params = get_params ( state ) return lossfunc ( params , model , x , y ) calculate_loss ( final_state , get_params , nn_apply , logistic_loss , X . values , y . values ) Now, we need to vmap it over all states in the states history, to get back the loss score. calc_loss_vmap = partial ( calculate_loss , get_params = get_params , model = nn_apply , lossfunc = logistic_loss , x = X . values , y = y . values ) start = time () losses = vmap ( calc_loss_vmap )( states_history ) end = time () print ( end - start ) plt . plot ( losses )","title":"Progressively construct our training functions"},{"location":"03-stax/02-neural/#training-with-multiple-starting-points","text":"Just as above, we can also train the neural network with multiple starting points, again by vmap -ing our training function across split PRNGKeys. keys = random . split ( key , 5 ) start = time () final_states , state_histories = vmap ( train_nn )( keys ) end = time () print ( end - start ) get_params ( final_states )[ 0 ][ 0 ] . shape Let's plot the losses over each of the state histories. Our last function calc_loss_vmap calculates loss score for one time point, which we then vmap over a single states_history , so we need another function that encapsulates this behaviour and vmap s over all state histories. def state_history_loss ( state_history ): losses = vmap ( calc_loss_vmap )( state_history ) return losses losses = vmap ( state_history_loss )( state_histories ) losses . shape losses Correctly-shaped! And now plotting it... plt . plot ( losses . T ) Now that's pretty cool! We were able to see the loss from three independent runs. With sufficient memory, one would be able to do more runs; when I was writing this notebook early on, I saw that it was getting difficult to do on the order of tens of runs due to memory allocation issues.","title":"Training with multiple starting points"},{"location":"03-stax/02-neural/#summary","text":"In this notebook, we saw a few things in action. Firstly, we saw how to use the stax module on a linear model. Anytime we have a new framework for doing differential programming, it's super important to be able to explore it in the context of a linear model, which is basically the foundation of all deep learning. Secondly, we also explored how to leverage the JAX idioms to create fast parallelized training loops. We mixed-and-matched together jit , vmap , lax.scan , and grad into a performant training loop that was minimally nested. A corollary of this programming style is that every piece of the code can, in principle, be properly tested , because they are properly isolated. Have you written training loops where you modify a little piece here and a little piece there, until you lost what your original working one looked like? With training functions that are minimally nested, we can control the behaviour explicitly using closures/partials easily. Even when doing experimenation, our code can run reliably and fast. Thirdly, we saw how to apply the same lessons to training a neural network really fast with multiple starting points. The essence of the solution was to properly structure our program in progressively higher level layers of abstraction. We carefully wrote the program to go from the inner most layer out until we hit our goal of allowing for a set of multiple starts. The key here is that each level of abstraction is very natural, and corresponds to a \"unit computation\" being applied consistently across an \"array\" of things. Once we identify that \"unit computation\", writing the vmap -able or lax.scan -able function becomes very easy.","title":"Summary"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Gaussian mixture model-based clustering In this notebook, we are going to take a look at how to cluster Gaussian-distributed data. Imagine you have data that are multi-modal, something that looks like the following: import jax.numpy as np from jax import random import matplotlib.pyplot as plt weights_true = np . array ([ 1 , 5 ]) # 1:5 ratio locs_true = np . array ([ - 2. , 5. ]) # different means scale_true = np . array ([ 1.1 , 2 ]) # different variances base_n_draws = 1000 key = random . PRNGKey ( 100 ) k1 , k2 = random . split ( key ) draws_1 = scale_true [ 0 ] * random . normal ( k1 , shape = ( base_n_draws * weights_true [ 0 ],)) + locs_true [ 0 ] draws_2 = scale_true [ 1 ] * random . normal ( k2 , shape = ( base_n_draws * weights_true [ 1 ],)) + locs_true [ 1 ] data_mixture = np . concatenate ([ draws_1 , draws_2 ]) plt . hist ( data_mixture , bins = 40 ); Likelihoods of Mixture Data We might look at this data and say, \"I think there's two clusters of data here.\" One that belongs to the left mode, and one that belongs to the right mode. By visual inspection, the relative weighting might be about 1:3 to 1:6, or somewhere in between. What might be the \"data generating process\" here? Well, we could claim that when a data point is drawn from the mixture distribution, it could have come from either of the modes. By basic probability logic, the joint likelihood of observing the data point is: The likelihood that the datum came from the left Gaussian, times the probability of drawing a number from the left Gaussian, plus... The likelihood that the datum came from the right Gaussian, times the probability of drawing a number from the right Gaussian. Phrased more generally: The sum over \"components j j of the likelihood that the datum x_i x_i came from Gaussian j j with parameters \\mu_j, \\sigma_j \\mu_j, \\sigma_j times the likelihood of observing a draw from component j j .\" In math, we would need to calculate: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) Now, we can make the middle term P(\\mu_j, \\sigma_j|w_j) P(\\mu_j, \\sigma_j|w_j) is always 1, by assuming that the \\mu_j \\mu_j and \\sigma_j \\sigma_j chosen are always fixed given the component weight chosen. The expression then simplifies to: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) Log Likelihood of One Datum under One Component Because this is a summation, let's work out the elementary steps first. from dl_workshop.gaussian_mixture import loglike_one_component loglike_one_component ?? The summation here is because we are operating in logarithmic space. You might ask, why do we use \"log\" of the component scale? This is a math trick that helps us whenever we are doing computations in an unbounded space. When doing gradient descent, we can never guarantee that a gradient update on a parameter that ought to be positive-only will give us a positive number. Thus, for positive numbers, we operate in logarithmic space. We can quickly write a test here. If the component probability is 1.0, the component \\mu \\mu is 0, and the observed datum is also 0, it should equal to the log-likelihood of 0 under a unit Gaussian. from jax.scipy import stats our_test = loglike_one_component ( component_weight = 1.0 , component_mu = 0. , log_component_scale = np . log ( 1. ), datum = 0. ) ground_truth = ( stats . norm . logpdf ( x = 0 , loc = 0 , scale = 1 ) ) our_test , ground_truth Log Likelihood of One Datum under All Components Now that we are done with the elementary computation of one datum under one component, we can vmap the log-likelihood calculation over all components, thereby giving us the loglikelihood of a datum under any of the possible given components. Firstly, we need a function that normalizes component weights to sum to 1. This is enforced just in case during the gradient descent procedure, we end up with weights that do not sum to 1. from dl_workshop.gaussian_mixture import normalize_weights , loglike_across_components normalize_weights ?? Next, we leverage the normalize_weights function inside a loglike_across_components function, which vmap s the log likelihood calculation across components: loglike_across_components ?? Inside that function, we first calculated elementwise the log-likelihood of observing that data under each component. That only gives us per-component log-likelihoods though. Because our data could have been drawn from any of those components, the total likelihood is a sum of the per-component likelihoods. Thus, we have to elementwise exponentiate the log-likelihoods first. Because we have sum up each of those probability components together, a shortcut function we have access to is the logsumexp function, which first exponentiates each of the probabilities, sums them up, and then takes their log again, thereby accomplishing what we need. We could have written our own version of the function, but I think it makes a ton of sense to trust the numerically-stable, professionally-implemented version provided in SciPy! The choice to pass in log_component_weights rather than weights is because the normalize_weights function assumes that all numbers in the vector are positive, but in gradient descent, we operate in an unbounded space, which may bring us into negative numbers. To make things safe, we assume the numbers come to us from an unbounded space, and then use an exponential transform first before normalizing. Let us now test-drive our loglike_across_components function, which should give us a scalar value at the end. loglike_across_components ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), datum = data_mixture [ 1 ], ) Great, that worked! Log Likelihood of All Data under All Components Now that we've got the log-likelihood of each datum under each component, we can now vmap the function across all data given to us. Mathematically, this would be: \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) Or in prose: The total likelihood of all datum x_i x_i together under all components j j is given by first summing the likelihoods of each datum x_i x_i under each component j j , and then taking the product of likelihoods for each data point x_i x_i , assuming data are i.i.d. from the mixture distribution. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? Notice how we vmap -ed the loglike_across_components function over all data points provided in the function above. This helped us eliminate a for-loop, basically! If we execute the function, we should get a scalar value. mixture_loglike ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), data = data_mixture , ) Log Likelihood of Weighting The final thing we are missing is a generative story for the weights. In other words, we are asking the question, \"How did the weights come about?\" We might say that the weights were drawn from a Dirichlet distribution (the generalization of a Beta distribution to multiple dimensions), and as a na\u00efve first pass, were drawn with equal probability. from dl_workshop.gaussian_mixture import weights_loglike weights_loglike ?? alpha_prior = 2 * np . ones_like ( weights_true ) weights_loglike ( np . log ( weights_true ), alpha_prior = alpha_prior ) Review thus far Now that we have composed together our generative story for the data, let's pause for a moment and break down our model a bit. This will serve as a review of what we've done. Firstly, we have our \"model\", i.e. the log-likelihood of our data conditioned on some parameter set and their values. Secondly, our parameters of the model are: Component weights. Component central tendencies/means Component scales/variances. What we're going to attempt next is to use gradient based optimization to learn what those parameters are, conditioned on data, leveraging the JAX idioms that we've learned before. Gradient descent to find maximum likelihood values Given a mixture Gaussian dataset, one natural task we might want to do is estimate the weights, central tendencies/means and scales/variances from data. This corresponds naturally to a maximum likelihood estimation task. Now, one thing we know is that JAX's optimizers assume we are minimizing a function, so to use JAX's optimizers with a maximum likelihood function, we simply take the negative of the log likelihood and minimize that. Loss function Let's first take a look at the loss function. from dl_workshop.gaussian_mixture import loss_mixture_weights loss_mixture_weights ?? As you can see, our function is designed to be compatible with JAX's grad . We are taking derivatives w.r.t. the first argument, the parameters, which we unpack into our likelihood function parameters. The two likelihood functions are used inside there too: mixture_loglike weights_loglike The alpha_prior is hard-coded; it's not the most ideal. For convenience, I have just hard-coded it, but the principled way to handle this is to add it as a keyword argument that gets passed in. Gradient of loss function As usual, we now define the gradient function of loss_mixture_weights by calling grad on it: from jax import grad dloss_mixture_weights = grad ( loss_mixture_weights ) Parameter Initialization Next up, we initialize our parameters randomly. For convenience, we'll use Gaussian draws. N_MIXTURE_COMPONENTS = 2 k1 , k2 , k3 , k4 = random . split ( key , 4 ) log_component_weights_init = random . normal ( k1 , shape = ( N_MIXTURE_COMPONENTS ,)) component_mus_init = random . normal ( k2 , shape = ( N_MIXTURE_COMPONENTS ,)) log_component_scales_init = random . normal ( k3 , shape = ( N_MIXTURE_COMPONENTS ,)) params_init = log_component_weights_init , component_mus_init , log_component_scales_init params_true = np . log ( weights_true ), locs_true , np . log ( scale_true ) Here, you see JAX's controllable handling of random numbers. Our parameters are always going to be initialized in exactly the same way on each notebook cell re-run, since we have explicit keys passed in. Test-drive functions Let's test-drive the functions to make sure that they work correctly. For the loss function, we should expect to get back a scalar. If we pass in initialized parameters, it should also have a higher value (corresponding to more lower log likelihood) than if we pass in true parameters. loss_mixture_weights ( params_true , data_mixture ) loss_mixture_weights ( params_init , data_mixture ) Indeed, both criteria are satisfied. Test-driving the gradient function should give us a tuple of gradients evaluated. dloss_mixture_weights ( params_init , data_mixture ) Defining performant training loops Now, we are going to use JAX's optimizers inside a lax.scan -ed training loop to get fast training going. We begin with the elementary \"step\" function. from dl_workshop.gaussian_mixture import step step ?? This should look familiar to you. At each step of the loop, we unpack params from a JAX optimizer state, obtain gradients, and then update the state using the gradients. We then make the elementary step function a scannable one using lax.scan . This will allow us to \"scan\" the function across an array that represents the number of optimization steps we will be using. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? Recall that the inner function that gets returned here has the API that we require for using lax.scan : previous_state corresponds to the carry , and iteration corresponds to the x . Now we actually instantiate the scannable step. from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.5 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_mixture_weights , update_func = adam_update , data = data_mixture , ) Then, we lax.scan step_scannable over 1000 iterations (constructed as an np.arange() array). from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 )) Sanity-checking whether learning has happened We can sanity check whether learning has happened. The loss function value for optimized parameters should be pretty close to the loss function when we put in true params. (Do keep in mind that because we have data that are an imperfect sample of the ground truth distribution, it is possible that our optimized params' negative log likelihood will be different than that of the true params.) Firstly, we unpack the parameters of the final state: params_opt = adam_get_params ( final_state ) log_component_weights_opt , component_mus_opt , log_component_scales_opt = params_opt Then, we look at the loss for the optimized params: loss_mixture_weights ( params_opt , data_mixture ) It should be lower than the loss for the initialized params loss_mixture_weights ( params_init , data_mixture ) Indeed that is so! And if we inspect the component weights: np . exp ( log_component_weights_opt ), weights_true Indeed, we have optimized our parameters such that they are close to the original 1:5 ratio! And for our component means? component_mus_opt , locs_true Really close too! Finally, for the component scales: np . exp ( log_component_scales_opt ), scale_true Very nice, really close to the ground truth too. Visualizing training dynamics Let's now visualize how training went. I have created a function called animate_training , which will provide for us a visual representation. animate_training ?? animate_training leverages celluloid to make easy matplotlib animations. You can check out the package here . We can now call on animate_training to give us an animation of the mixture Gaussian PDFs as we trained the model. %% capture from dl_workshop.gaussian_mixture import animate_training params_history = adam_get_params ( state_history ) animation = animate_training ( params_history , 10 , data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) There's some comments to be said on the dynamics here: At first, one Gaussian is used to approximate over the entire distribution. It's not a good fit, but approximates it fine enough. However, our optimization routine continues to push forward, eventually finding the bimodal pattern. Once this happens, the PDFs fit very nicely to the data samples. This phenomena is also reflected in the loss: from dl_workshop.gaussian_mixture import get_loss get_loss ?? Because states_history is the result of lax.scan -ing, we can vmap our get_loss function over the states_history object to get back an array of losses that can then be plotted: from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_mixture_weights , data = data_mixture ))( state_history ) plt . plot ( losses ) plt . yscale ( \"log\" ); You should notice the first plateau, followed by the second plateau. This corresponds to the two phases of learning. Now, thus far, we have set up the problem in a fashion that is essentially \"trivial\". What if, however, we wanted to try fitting a mixture Gaussian where we didn't know exactly how many mixture components there ought to be? To check that out, head over to the next section in this chapter.","title":"Introduction"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#gaussian-mixture-model-based-clustering","text":"In this notebook, we are going to take a look at how to cluster Gaussian-distributed data. Imagine you have data that are multi-modal, something that looks like the following: import jax.numpy as np from jax import random import matplotlib.pyplot as plt weights_true = np . array ([ 1 , 5 ]) # 1:5 ratio locs_true = np . array ([ - 2. , 5. ]) # different means scale_true = np . array ([ 1.1 , 2 ]) # different variances base_n_draws = 1000 key = random . PRNGKey ( 100 ) k1 , k2 = random . split ( key ) draws_1 = scale_true [ 0 ] * random . normal ( k1 , shape = ( base_n_draws * weights_true [ 0 ],)) + locs_true [ 0 ] draws_2 = scale_true [ 1 ] * random . normal ( k2 , shape = ( base_n_draws * weights_true [ 1 ],)) + locs_true [ 1 ] data_mixture = np . concatenate ([ draws_1 , draws_2 ]) plt . hist ( data_mixture , bins = 40 );","title":"Gaussian mixture model-based clustering"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#likelihoods-of-mixture-data","text":"We might look at this data and say, \"I think there's two clusters of data here.\" One that belongs to the left mode, and one that belongs to the right mode. By visual inspection, the relative weighting might be about 1:3 to 1:6, or somewhere in between. What might be the \"data generating process\" here? Well, we could claim that when a data point is drawn from the mixture distribution, it could have come from either of the modes. By basic probability logic, the joint likelihood of observing the data point is: The likelihood that the datum came from the left Gaussian, times the probability of drawing a number from the left Gaussian, plus... The likelihood that the datum came from the right Gaussian, times the probability of drawing a number from the right Gaussian. Phrased more generally: The sum over \"components j j of the likelihood that the datum x_i x_i came from Gaussian j j with parameters \\mu_j, \\sigma_j \\mu_j, \\sigma_j times the likelihood of observing a draw from component j j .\" In math, we would need to calculate: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j) Now, we can make the middle term P(\\mu_j, \\sigma_j|w_j) P(\\mu_j, \\sigma_j|w_j) is always 1, by assuming that the \\mu_j \\mu_j and \\sigma_j \\sigma_j chosen are always fixed given the component weight chosen. The expression then simplifies to: \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j)","title":"Likelihoods of Mixture Data"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-one-datum-under-one-component","text":"Because this is a summation, let's work out the elementary steps first. from dl_workshop.gaussian_mixture import loglike_one_component loglike_one_component ?? The summation here is because we are operating in logarithmic space. You might ask, why do we use \"log\" of the component scale? This is a math trick that helps us whenever we are doing computations in an unbounded space. When doing gradient descent, we can never guarantee that a gradient update on a parameter that ought to be positive-only will give us a positive number. Thus, for positive numbers, we operate in logarithmic space. We can quickly write a test here. If the component probability is 1.0, the component \\mu \\mu is 0, and the observed datum is also 0, it should equal to the log-likelihood of 0 under a unit Gaussian. from jax.scipy import stats our_test = loglike_one_component ( component_weight = 1.0 , component_mu = 0. , log_component_scale = np . log ( 1. ), datum = 0. ) ground_truth = ( stats . norm . logpdf ( x = 0 , loc = 0 , scale = 1 ) ) our_test , ground_truth","title":"Log Likelihood of One Datum under One Component"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-one-datum-under-all-components","text":"Now that we are done with the elementary computation of one datum under one component, we can vmap the log-likelihood calculation over all components, thereby giving us the loglikelihood of a datum under any of the possible given components. Firstly, we need a function that normalizes component weights to sum to 1. This is enforced just in case during the gradient descent procedure, we end up with weights that do not sum to 1. from dl_workshop.gaussian_mixture import normalize_weights , loglike_across_components normalize_weights ?? Next, we leverage the normalize_weights function inside a loglike_across_components function, which vmap s the log likelihood calculation across components: loglike_across_components ?? Inside that function, we first calculated elementwise the log-likelihood of observing that data under each component. That only gives us per-component log-likelihoods though. Because our data could have been drawn from any of those components, the total likelihood is a sum of the per-component likelihoods. Thus, we have to elementwise exponentiate the log-likelihoods first. Because we have sum up each of those probability components together, a shortcut function we have access to is the logsumexp function, which first exponentiates each of the probabilities, sums them up, and then takes their log again, thereby accomplishing what we need. We could have written our own version of the function, but I think it makes a ton of sense to trust the numerically-stable, professionally-implemented version provided in SciPy! The choice to pass in log_component_weights rather than weights is because the normalize_weights function assumes that all numbers in the vector are positive, but in gradient descent, we operate in an unbounded space, which may bring us into negative numbers. To make things safe, we assume the numbers come to us from an unbounded space, and then use an exponential transform first before normalizing. Let us now test-drive our loglike_across_components function, which should give us a scalar value at the end. loglike_across_components ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), datum = data_mixture [ 1 ], ) Great, that worked!","title":"Log Likelihood of One Datum under All Components"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-all-data-under-all-components","text":"Now that we've got the log-likelihood of each datum under each component, we can now vmap the function across all data given to us. Mathematically, this would be: \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) \\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j) Or in prose: The total likelihood of all datum x_i x_i together under all components j j is given by first summing the likelihoods of each datum x_i x_i under each component j j , and then taking the product of likelihoods for each data point x_i x_i , assuming data are i.i.d. from the mixture distribution. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? Notice how we vmap -ed the loglike_across_components function over all data points provided in the function above. This helped us eliminate a for-loop, basically! If we execute the function, we should get a scalar value. mixture_loglike ( log_component_weights = np . log ( weights_true ), component_mus = locs_true , log_component_scales = np . log ( scale_true ), data = data_mixture , )","title":"Log Likelihood of All Data under All Components"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#log-likelihood-of-weighting","text":"The final thing we are missing is a generative story for the weights. In other words, we are asking the question, \"How did the weights come about?\" We might say that the weights were drawn from a Dirichlet distribution (the generalization of a Beta distribution to multiple dimensions), and as a na\u00efve first pass, were drawn with equal probability. from dl_workshop.gaussian_mixture import weights_loglike weights_loglike ?? alpha_prior = 2 * np . ones_like ( weights_true ) weights_loglike ( np . log ( weights_true ), alpha_prior = alpha_prior )","title":"Log Likelihood of Weighting"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#review-thus-far","text":"Now that we have composed together our generative story for the data, let's pause for a moment and break down our model a bit. This will serve as a review of what we've done. Firstly, we have our \"model\", i.e. the log-likelihood of our data conditioned on some parameter set and their values. Secondly, our parameters of the model are: Component weights. Component central tendencies/means Component scales/variances. What we're going to attempt next is to use gradient based optimization to learn what those parameters are, conditioned on data, leveraging the JAX idioms that we've learned before.","title":"Review thus far"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#gradient-descent-to-find-maximum-likelihood-values","text":"Given a mixture Gaussian dataset, one natural task we might want to do is estimate the weights, central tendencies/means and scales/variances from data. This corresponds naturally to a maximum likelihood estimation task. Now, one thing we know is that JAX's optimizers assume we are minimizing a function, so to use JAX's optimizers with a maximum likelihood function, we simply take the negative of the log likelihood and minimize that.","title":"Gradient descent to find maximum likelihood values"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#loss-function","text":"Let's first take a look at the loss function. from dl_workshop.gaussian_mixture import loss_mixture_weights loss_mixture_weights ?? As you can see, our function is designed to be compatible with JAX's grad . We are taking derivatives w.r.t. the first argument, the parameters, which we unpack into our likelihood function parameters. The two likelihood functions are used inside there too: mixture_loglike weights_loglike The alpha_prior is hard-coded; it's not the most ideal. For convenience, I have just hard-coded it, but the principled way to handle this is to add it as a keyword argument that gets passed in.","title":"Loss function"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#gradient-of-loss-function","text":"As usual, we now define the gradient function of loss_mixture_weights by calling grad on it: from jax import grad dloss_mixture_weights = grad ( loss_mixture_weights )","title":"Gradient of loss function"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#parameter-initialization","text":"Next up, we initialize our parameters randomly. For convenience, we'll use Gaussian draws. N_MIXTURE_COMPONENTS = 2 k1 , k2 , k3 , k4 = random . split ( key , 4 ) log_component_weights_init = random . normal ( k1 , shape = ( N_MIXTURE_COMPONENTS ,)) component_mus_init = random . normal ( k2 , shape = ( N_MIXTURE_COMPONENTS ,)) log_component_scales_init = random . normal ( k3 , shape = ( N_MIXTURE_COMPONENTS ,)) params_init = log_component_weights_init , component_mus_init , log_component_scales_init params_true = np . log ( weights_true ), locs_true , np . log ( scale_true ) Here, you see JAX's controllable handling of random numbers. Our parameters are always going to be initialized in exactly the same way on each notebook cell re-run, since we have explicit keys passed in.","title":"Parameter Initialization"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#test-drive-functions","text":"Let's test-drive the functions to make sure that they work correctly. For the loss function, we should expect to get back a scalar. If we pass in initialized parameters, it should also have a higher value (corresponding to more lower log likelihood) than if we pass in true parameters. loss_mixture_weights ( params_true , data_mixture ) loss_mixture_weights ( params_init , data_mixture ) Indeed, both criteria are satisfied. Test-driving the gradient function should give us a tuple of gradients evaluated. dloss_mixture_weights ( params_init , data_mixture )","title":"Test-drive functions"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#defining-performant-training-loops","text":"Now, we are going to use JAX's optimizers inside a lax.scan -ed training loop to get fast training going. We begin with the elementary \"step\" function. from dl_workshop.gaussian_mixture import step step ?? This should look familiar to you. At each step of the loop, we unpack params from a JAX optimizer state, obtain gradients, and then update the state using the gradients. We then make the elementary step function a scannable one using lax.scan . This will allow us to \"scan\" the function across an array that represents the number of optimization steps we will be using. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? Recall that the inner function that gets returned here has the API that we require for using lax.scan : previous_state corresponds to the carry , and iteration corresponds to the x . Now we actually instantiate the scannable step. from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.5 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_mixture_weights , update_func = adam_update , data = data_mixture , ) Then, we lax.scan step_scannable over 1000 iterations (constructed as an np.arange() array). from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 ))","title":"Defining performant training loops"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#sanity-checking-whether-learning-has-happened","text":"We can sanity check whether learning has happened. The loss function value for optimized parameters should be pretty close to the loss function when we put in true params. (Do keep in mind that because we have data that are an imperfect sample of the ground truth distribution, it is possible that our optimized params' negative log likelihood will be different than that of the true params.) Firstly, we unpack the parameters of the final state: params_opt = adam_get_params ( final_state ) log_component_weights_opt , component_mus_opt , log_component_scales_opt = params_opt Then, we look at the loss for the optimized params: loss_mixture_weights ( params_opt , data_mixture ) It should be lower than the loss for the initialized params loss_mixture_weights ( params_init , data_mixture ) Indeed that is so! And if we inspect the component weights: np . exp ( log_component_weights_opt ), weights_true Indeed, we have optimized our parameters such that they are close to the original 1:5 ratio! And for our component means? component_mus_opt , locs_true Really close too! Finally, for the component scales: np . exp ( log_component_scales_opt ), scale_true Very nice, really close to the ground truth too.","title":"Sanity-checking whether learning has happened"},{"location":"04-gaussian-clustering/01-intro-gaussian-clustering/#visualizing-training-dynamics","text":"Let's now visualize how training went. I have created a function called animate_training , which will provide for us a visual representation. animate_training ?? animate_training leverages celluloid to make easy matplotlib animations. You can check out the package here . We can now call on animate_training to give us an animation of the mixture Gaussian PDFs as we trained the model. %% capture from dl_workshop.gaussian_mixture import animate_training params_history = adam_get_params ( state_history ) animation = animate_training ( params_history , 10 , data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) There's some comments to be said on the dynamics here: At first, one Gaussian is used to approximate over the entire distribution. It's not a good fit, but approximates it fine enough. However, our optimization routine continues to push forward, eventually finding the bimodal pattern. Once this happens, the PDFs fit very nicely to the data samples. This phenomena is also reflected in the loss: from dl_workshop.gaussian_mixture import get_loss get_loss ?? Because states_history is the result of lax.scan -ing, we can vmap our get_loss function over the states_history object to get back an array of losses that can then be plotted: from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_mixture_weights , data = data_mixture ))( state_history ) plt . plot ( losses ) plt . yscale ( \"log\" ); You should notice the first plateau, followed by the second plateau. This corresponds to the two phases of learning. Now, thus far, we have set up the problem in a fashion that is essentially \"trivial\". What if, however, we wanted to try fitting a mixture Gaussian where we didn't know exactly how many mixture components there ought to be? To check that out, head over to the next section in this chapter.","title":"Visualizing training dynamics"},{"location":"04-gaussian-clustering/02-dirichlet-processes/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Dirichlet Processes: A simulated guide Introduction In the previous section, we saw how we could fit a two-component Gaussian mixture model to data that looked like it had just two components. In many real-world settings, though, we oftentimes do not know exactly how many components are present, so one way we can approach the problem is to assume that there are an infinite (or \"countably large\") number of components available for our model to pick from, but we \"guide\" our model to focus its attention on only a small number of components provided. Does that sound magical? It sure did for me when I first heard about this possibility. The key modelling component that we need is a process for creating infinite numbers of mixture weight components from a single controllable parameter, and that naturally gives us a Dirichlet process , which we will look at in this section. What are Dirichlet processes? To quote from Wikipedia's article on DPs : In probability theory, Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions. Hmm, now that doesn't look very concrete. Is there a more concrete way to think about DPs? Turns out, the answer is yes! At its core, each realization/draw from a DP provides an infinite (or, in computing world, a \"large\") set of weights that sum to 1. Remember that: A long vector of numbers that sum to 1, which we can interpret as a probability distribution over sets of weights. Simulating a Dirichlet Process using \"stick-breaking\" We're going to look at one way to construct a probability vector, the \"stick-breaking\" process. How does it work? At its core, it looks like this, a very simple idea. We take a length 1 stick, draw a probability value from a Beta distribution, break the length 1 stick into two at the point drawn, and record the left side's value. We then take the right side, draw another probability value from a Beta distribution again, break that stick proportionally into two portions at the point drawn, and record the absolute length of the left side's value We then braek the right side again, using the same process. We repeat this until we have the countably large number of states that we desire. In code, this looks like a loop with a carryover from the previous iteration, which means it is a lax.scan -able function! from dl_workshop.gaussian_mixture import stick_breaking_weights stick_breaking_weights ?? As you can see, in the inner function weighting , we first calculate the weight associated with the \"left side\" of the stick, which we record down and accumulate as the \"history\" (second tuple element of the return). Our carry is the occupied_probability + weight , which we can use to calculate the length of the right side of the stick ( 1 - occupied_probability ). Because each beta_i is an i.i.d. draw from beta_draws , we can pre-instantiate a vector of beta_draws and then lax.scan the weighting function over the vector. Beta distribution crash-course Because on computers it's hard to deal with infinitely-long arrays, we can instead instantiate a \"countably large\" array of beta_draws . Now, the beta_draws , need to be i.i.d. from a source Beta distribution, which has two parameters, a and b , and gives us a continuous distribution over the interval (0, 1) (0, 1) . Because of the nature of a and b corresponding to success and failure weights: higher a at constant b shifts the distribution closer to 1, higher b at constant a shifts the distribution closer to 0, higher magnitudes of a and b narrow the distribution width. Visualizing stick-breaking For our purposes, we are going to hold a constant at 1.0 while varying b . We'll then see how our weight vectors are generated as a function of b . As you will see, b becomes a \"concentration\" parameter, which governs how \"concentrated\" our probability mass is allocated. Let's see how one draw from a Dirichlet process looks like. def dp_draw ( key , concentration , vector_length ): beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( vector_length ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) return occupied_probability , weights from jax import random import matplotlib.pyplot as plt key = random . PRNGKey ( 42 ) occupied_probability , weights = dp_draw ( key , 3 , 50 ) plt . plot ( weights ) plt . xlabel ( \"Vector slot\" ) plt . ylabel ( \"Probability\" ); Now, what if we took 20 draws from the Dirichlet process? To do so, we can vmap dp_draw over split PRNGKey s. from jax import vmap from functools import partial import seaborn as sns keys = random . split ( key , 20 ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = 3 , vector_length = 50 ))( keys ) sns . heatmap ( weights_draws ); Effect of concentration on Dirichlet weights draws As is visible here, when concentration = 3 , most of our probability mass is concentrated across roughly the first 5-8 states. What happens if we varied the concentration? How does that parameter affect the distribution of weights? import jax.numpy as np concentrations = np . array ([ 0.5 , 1 , 3 , 5 , 10 , 20 ]) def dirichlet_one_concentration ( key , concentration , num_draws ): keys = random . split ( key , num_draws ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = concentration , vector_length = 50 ))( keys ) return occupied_probabilities , weights_draws keys = random . split ( key , len ( concentrations )) occupied_probabilities , weights_draws = vmap ( partial ( dirichlet_one_concentration , num_draws = 20 ))( keys , concentrations ) weights_draws . shape fig , axes = plt . subplots ( nrows = 2 , ncols = 3 , figsize = ( 3 * 3 , 3 * 2 ), sharex = True , sharey = True ) for ax , weights_mat , conc in zip ( axes . flatten (), weights_draws , concentrations ): sns . heatmap ( weights_mat , ax = ax ) ax . set_title ( f \"Concentration = { conc } \" ) ax . set_xlabel ( \"Component\" ) ax . set_ylabel ( \"Draw\" ) plt . tight_layout () As we increase the concentration value, the probabilities get more diffuse. This is evident from the above heatmaps in the following ways. Over each draw, as we increase the value of the concentration parameter, the probability mass allocated to the components that have significant probability mass decreases. Additionally, more components have \"significant\" amounts of probability mass allocated. Running stick-breaking backwards From this forward process of generating Dirichlet-distributed weights, instead of evaluating the log likelihood of the component weights under a \"fixed\" Dirichlet distribution prior, we can instead evaluate it under a Dirichlet process with a \"concentration\" prior. The requirement here is that we be able to recover correctly the i.i.d. Beta draws that generated the Dirichlet process weights. Let's try that out. from dl_workshop.gaussian_mixture import beta_draw_from_weights beta_draw_from_weights ?? We essentially run the process backwards, taking advantage of the fact that we know the first weight exactly. Let's try to see how well we can recover the weights. concentration = 3 beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( 50 ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) final , beta_hat = beta_draw_from_weights ( weights ) plt . plot ( beta_draws , label = \"original\" ) plt . plot ( beta_hat , label = \"inferred\" ) plt . legend () plt . xlabel ( \"Component\" ) plt . ylabel ( \"Beta Draw\" ); As is visible from the plot above, we were able to recover about 1/2 to 2/3 of the weights before the divergence in the two curves shows up. One of the difficulties that we have is that when we get back the observed weights in real life, we have no access to how much of the length 1 \"stick\" is leftover. This, alongside numerical underflow issues arising from small numbers, means we can only use about 1/2 of the drawn weights to recover the Beta-distributed draws from which we can evaluate our log likelihoods. Evaluating log-likelihood of recovered Beta-distributed weights So putting things all together, we can take a weights vector, run the stick-breaking process backwards (up to a certain point) to recover Beta-distributed draws that would have generated the weights vector, and then evaluate the log-likelihood of the Beta-disributed draws under a Beta distribution. Let's see that in action: from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? And evaluating our draws should give us a scalar likelihood: component_probs_loglike ( np . log ( weights ), log_concentration = 1.0 , num_components = 25 ) Log likelihood as a function of concentration Once again, let's build up our understanding by seeing how the log likelihood of our weights under an assumed Dirichlet process from a Beta distribution changes as we vary the concentration parameter. log_concentration = np . linspace ( - 3 , 3 , 1000 ) def make_vmappable_loglike ( log_component_probs , num_components ): def inner ( log_concentration ): return component_probs_loglike ( log_component_probs , log_concentration , num_components ) return inner component_probs_loglike_vmappable = make_vmappable_loglike ( log_component_probs = np . log ( weights ), num_components = 25 ) lls = vmap ( component_probs_loglike_vmappable )( log_concentration ) plt . plot ( log_concentration , lls ) plt . xlabel ( \"Concentration\" ) plt . ylabel ( \"Log likelihood\" ); As you can see above, we first constructed the vmappable log-likelihood function using a closure. The shape of the curve tells us that it is an optimizable problem with one optimal point, at least within bounds of possible concentrations that we're interested in. Optimizing the log-likelihood Once again, we're going to see how we can use gradient-based optimization to see how we can identify the most likely concentration value that generated a Dirichlet process weights vector. Define loss function As always, we start with the loss function definition. Because our component_probs_loglike function operates only on a single draw, we need a function that will allow us to operate on multiple draws. We can do this by using a closure. from jax import grad def make_loss_dp ( num_components ): def loss_dp ( log_concentration , log_component_probs ): \"\"\"Log-likelihood of component_probabilities of dirichlet process. :param log_concentration: Scalar value. :param log_component_probs: One or more component probability vectors. \"\"\" vm_func = partial ( component_probs_loglike , log_concentration = log_concentration , num_components = num_components , ) ll = vmap ( vm_func , in_axes = 0 )( log_component_probs ) return - np . sum ( ll ) return loss_dp loss_dp = make_loss_dp ( num_components = 25 ) dloss_dp = grad ( loss_dp ) loss_dp ( np . log ( 3 ), log_component_probs = np . log ( weights_draws [ 3 ] + 1e-6 )) I have opted for a closure pattern here because we are going to require that the Dirichlet-process log likelihood loss function accept log_concentration (parameter to optimize) as the first argument, and log_component_probs (data) as the second. However, we need to specify the number of components we are going to allow for evaluating the Beta-distributed log likelihood, so that goes on the outside. Moreover, we are assuming i.i.d. draws of weights, therefore, we also vmap over all of the log_component_probs . Define training loop Just as with the previous sections, we are going to define the training loops. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? For our demonstration here, we are going to use draws from the weights_draws matrix defined above, specifically the one at index 3, which had a concentration value of 5. Just to remind ourselves what that heatmapt looks like: sns . heatmap ( weights_draws [ 3 ]); Now, we set up the scannable step function: from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_dp , update_func = adam_update , data = np . log ( weights_draws [ 3 ] + 1e-6 ), ) And then we initialize our parameters log_concentration_init = random . normal ( key ) params_init = log_concentration_init And finally, we run the training loop as a lax.scan function. from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 )) Now, we can calculate the losses over history. from dl_workshop.gaussian_mixture import get_loss get_loss ?? from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_dp , data = np . log ( weights_draws [ 1 ] + 1e-6 ) ) )( state_history ) plt . plot ( losses ) What is the final value that we obtain? params_opt = adam_get_params ( final_state ) params_opt np . exp ( params_opt ) This is pretty darn close to what we started with! Summary Here, we took a detour through Dirichlet processes to help you get a grounding onto how its math works. Through code, we saw how to: Use the Beta distribution, Write the stick-breaking process using Beta-distributed draws to generate large vectors of weights that correspond to categorical probabilities, Run the stick-breaking process backwards from a vector of categorical probabilities to get back Beta-distributed draws Infer the maximum likelihood concentration value given a set of draws. The primary purpose of this section was to get you primed for the next section, in which we try to simulatenously infer the number of prominent mixture components and their distribution parameters. A (ahem!) derivative outcome here was that I hopefully showed you how it is possible to use gradient-based optimization on seemingly discrete problems.","title":"Dirichlet Processes"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#dirichlet-processes-a-simulated-guide","text":"","title":"Dirichlet Processes: A simulated guide"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#introduction","text":"In the previous section, we saw how we could fit a two-component Gaussian mixture model to data that looked like it had just two components. In many real-world settings, though, we oftentimes do not know exactly how many components are present, so one way we can approach the problem is to assume that there are an infinite (or \"countably large\") number of components available for our model to pick from, but we \"guide\" our model to focus its attention on only a small number of components provided. Does that sound magical? It sure did for me when I first heard about this possibility. The key modelling component that we need is a process for creating infinite numbers of mixture weight components from a single controllable parameter, and that naturally gives us a Dirichlet process , which we will look at in this section.","title":"Introduction"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#what-are-dirichlet-processes","text":"To quote from Wikipedia's article on DPs : In probability theory, Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions. Hmm, now that doesn't look very concrete. Is there a more concrete way to think about DPs? Turns out, the answer is yes! At its core, each realization/draw from a DP provides an infinite (or, in computing world, a \"large\") set of weights that sum to 1. Remember that: A long vector of numbers that sum to 1, which we can interpret as a probability distribution over sets of weights.","title":"What are Dirichlet processes?"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#simulating-a-dirichlet-process-using-stick-breaking","text":"We're going to look at one way to construct a probability vector, the \"stick-breaking\" process. How does it work? At its core, it looks like this, a very simple idea. We take a length 1 stick, draw a probability value from a Beta distribution, break the length 1 stick into two at the point drawn, and record the left side's value. We then take the right side, draw another probability value from a Beta distribution again, break that stick proportionally into two portions at the point drawn, and record the absolute length of the left side's value We then braek the right side again, using the same process. We repeat this until we have the countably large number of states that we desire. In code, this looks like a loop with a carryover from the previous iteration, which means it is a lax.scan -able function! from dl_workshop.gaussian_mixture import stick_breaking_weights stick_breaking_weights ?? As you can see, in the inner function weighting , we first calculate the weight associated with the \"left side\" of the stick, which we record down and accumulate as the \"history\" (second tuple element of the return). Our carry is the occupied_probability + weight , which we can use to calculate the length of the right side of the stick ( 1 - occupied_probability ). Because each beta_i is an i.i.d. draw from beta_draws , we can pre-instantiate a vector of beta_draws and then lax.scan the weighting function over the vector.","title":"Simulating a Dirichlet Process using \"stick-breaking\""},{"location":"04-gaussian-clustering/02-dirichlet-processes/#beta-distribution-crash-course","text":"Because on computers it's hard to deal with infinitely-long arrays, we can instead instantiate a \"countably large\" array of beta_draws . Now, the beta_draws , need to be i.i.d. from a source Beta distribution, which has two parameters, a and b , and gives us a continuous distribution over the interval (0, 1) (0, 1) . Because of the nature of a and b corresponding to success and failure weights: higher a at constant b shifts the distribution closer to 1, higher b at constant a shifts the distribution closer to 0, higher magnitudes of a and b narrow the distribution width.","title":"Beta distribution crash-course"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#visualizing-stick-breaking","text":"For our purposes, we are going to hold a constant at 1.0 while varying b . We'll then see how our weight vectors are generated as a function of b . As you will see, b becomes a \"concentration\" parameter, which governs how \"concentrated\" our probability mass is allocated. Let's see how one draw from a Dirichlet process looks like. def dp_draw ( key , concentration , vector_length ): beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( vector_length ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) return occupied_probability , weights from jax import random import matplotlib.pyplot as plt key = random . PRNGKey ( 42 ) occupied_probability , weights = dp_draw ( key , 3 , 50 ) plt . plot ( weights ) plt . xlabel ( \"Vector slot\" ) plt . ylabel ( \"Probability\" ); Now, what if we took 20 draws from the Dirichlet process? To do so, we can vmap dp_draw over split PRNGKey s. from jax import vmap from functools import partial import seaborn as sns keys = random . split ( key , 20 ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = 3 , vector_length = 50 ))( keys ) sns . heatmap ( weights_draws );","title":"Visualizing stick-breaking"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#effect-of-concentration-on-dirichlet-weights-draws","text":"As is visible here, when concentration = 3 , most of our probability mass is concentrated across roughly the first 5-8 states. What happens if we varied the concentration? How does that parameter affect the distribution of weights? import jax.numpy as np concentrations = np . array ([ 0.5 , 1 , 3 , 5 , 10 , 20 ]) def dirichlet_one_concentration ( key , concentration , num_draws ): keys = random . split ( key , num_draws ) occupied_probabilities , weights_draws = vmap ( partial ( dp_draw , concentration = concentration , vector_length = 50 ))( keys ) return occupied_probabilities , weights_draws keys = random . split ( key , len ( concentrations )) occupied_probabilities , weights_draws = vmap ( partial ( dirichlet_one_concentration , num_draws = 20 ))( keys , concentrations ) weights_draws . shape fig , axes = plt . subplots ( nrows = 2 , ncols = 3 , figsize = ( 3 * 3 , 3 * 2 ), sharex = True , sharey = True ) for ax , weights_mat , conc in zip ( axes . flatten (), weights_draws , concentrations ): sns . heatmap ( weights_mat , ax = ax ) ax . set_title ( f \"Concentration = { conc } \" ) ax . set_xlabel ( \"Component\" ) ax . set_ylabel ( \"Draw\" ) plt . tight_layout () As we increase the concentration value, the probabilities get more diffuse. This is evident from the above heatmaps in the following ways. Over each draw, as we increase the value of the concentration parameter, the probability mass allocated to the components that have significant probability mass decreases. Additionally, more components have \"significant\" amounts of probability mass allocated.","title":"Effect of concentration on Dirichlet weights draws"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#running-stick-breaking-backwards","text":"From this forward process of generating Dirichlet-distributed weights, instead of evaluating the log likelihood of the component weights under a \"fixed\" Dirichlet distribution prior, we can instead evaluate it under a Dirichlet process with a \"concentration\" prior. The requirement here is that we be able to recover correctly the i.i.d. Beta draws that generated the Dirichlet process weights. Let's try that out. from dl_workshop.gaussian_mixture import beta_draw_from_weights beta_draw_from_weights ?? We essentially run the process backwards, taking advantage of the fact that we know the first weight exactly. Let's try to see how well we can recover the weights. concentration = 3 beta_draws = random . beta ( key = key , a = 1 , b = concentration , shape = ( 50 ,)) occupied_probability , weights = stick_breaking_weights ( beta_draws ) final , beta_hat = beta_draw_from_weights ( weights ) plt . plot ( beta_draws , label = \"original\" ) plt . plot ( beta_hat , label = \"inferred\" ) plt . legend () plt . xlabel ( \"Component\" ) plt . ylabel ( \"Beta Draw\" ); As is visible from the plot above, we were able to recover about 1/2 to 2/3 of the weights before the divergence in the two curves shows up. One of the difficulties that we have is that when we get back the observed weights in real life, we have no access to how much of the length 1 \"stick\" is leftover. This, alongside numerical underflow issues arising from small numbers, means we can only use about 1/2 of the drawn weights to recover the Beta-distributed draws from which we can evaluate our log likelihoods.","title":"Running stick-breaking backwards"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#evaluating-log-likelihood-of-recovered-beta-distributed-weights","text":"So putting things all together, we can take a weights vector, run the stick-breaking process backwards (up to a certain point) to recover Beta-distributed draws that would have generated the weights vector, and then evaluate the log-likelihood of the Beta-disributed draws under a Beta distribution. Let's see that in action: from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? And evaluating our draws should give us a scalar likelihood: component_probs_loglike ( np . log ( weights ), log_concentration = 1.0 , num_components = 25 )","title":"Evaluating log-likelihood of recovered Beta-distributed weights"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#log-likelihood-as-a-function-of-concentration","text":"Once again, let's build up our understanding by seeing how the log likelihood of our weights under an assumed Dirichlet process from a Beta distribution changes as we vary the concentration parameter. log_concentration = np . linspace ( - 3 , 3 , 1000 ) def make_vmappable_loglike ( log_component_probs , num_components ): def inner ( log_concentration ): return component_probs_loglike ( log_component_probs , log_concentration , num_components ) return inner component_probs_loglike_vmappable = make_vmappable_loglike ( log_component_probs = np . log ( weights ), num_components = 25 ) lls = vmap ( component_probs_loglike_vmappable )( log_concentration ) plt . plot ( log_concentration , lls ) plt . xlabel ( \"Concentration\" ) plt . ylabel ( \"Log likelihood\" ); As you can see above, we first constructed the vmappable log-likelihood function using a closure. The shape of the curve tells us that it is an optimizable problem with one optimal point, at least within bounds of possible concentrations that we're interested in.","title":"Log likelihood as a function of concentration"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#optimizing-the-log-likelihood","text":"Once again, we're going to see how we can use gradient-based optimization to see how we can identify the most likely concentration value that generated a Dirichlet process weights vector.","title":"Optimizing the log-likelihood"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#define-loss-function","text":"As always, we start with the loss function definition. Because our component_probs_loglike function operates only on a single draw, we need a function that will allow us to operate on multiple draws. We can do this by using a closure. from jax import grad def make_loss_dp ( num_components ): def loss_dp ( log_concentration , log_component_probs ): \"\"\"Log-likelihood of component_probabilities of dirichlet process. :param log_concentration: Scalar value. :param log_component_probs: One or more component probability vectors. \"\"\" vm_func = partial ( component_probs_loglike , log_concentration = log_concentration , num_components = num_components , ) ll = vmap ( vm_func , in_axes = 0 )( log_component_probs ) return - np . sum ( ll ) return loss_dp loss_dp = make_loss_dp ( num_components = 25 ) dloss_dp = grad ( loss_dp ) loss_dp ( np . log ( 3 ), log_component_probs = np . log ( weights_draws [ 3 ] + 1e-6 )) I have opted for a closure pattern here because we are going to require that the Dirichlet-process log likelihood loss function accept log_concentration (parameter to optimize) as the first argument, and log_component_probs (data) as the second. However, we need to specify the number of components we are going to allow for evaluating the Beta-distributed log likelihood, so that goes on the outside. Moreover, we are assuming i.i.d. draws of weights, therefore, we also vmap over all of the log_component_probs .","title":"Define loss function"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#define-training-loop","text":"Just as with the previous sections, we are going to define the training loops. from dl_workshop.gaussian_mixture import make_step_scannable make_step_scannable ?? For our demonstration here, we are going to use draws from the weights_draws matrix defined above, specifically the one at index 3, which had a concentration value of 5. Just to remind ourselves what that heatmapt looks like: sns . heatmap ( weights_draws [ 3 ]); Now, we set up the scannable step function: from jax.experimental.optimizers import adam adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = dloss_dp , update_func = adam_update , data = np . log ( weights_draws [ 3 ] + 1e-6 ), ) And then we initialize our parameters log_concentration_init = random . normal ( key ) params_init = log_concentration_init And finally, we run the training loop as a lax.scan function. from jax import lax initial_state = adam_init ( params_init ) final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( 1000 )) Now, we can calculate the losses over history. from dl_workshop.gaussian_mixture import get_loss get_loss ?? from jax import vmap from functools import partial losses = vmap ( partial ( get_loss , get_params_func = adam_get_params , loss_func = loss_dp , data = np . log ( weights_draws [ 1 ] + 1e-6 ) ) )( state_history ) plt . plot ( losses ) What is the final value that we obtain? params_opt = adam_get_params ( final_state ) params_opt np . exp ( params_opt ) This is pretty darn close to what we started with!","title":"Define training loop"},{"location":"04-gaussian-clustering/02-dirichlet-processes/#summary","text":"Here, we took a detour through Dirichlet processes to help you get a grounding onto how its math works. Through code, we saw how to: Use the Beta distribution, Write the stick-breaking process using Beta-distributed draws to generate large vectors of weights that correspond to categorical probabilities, Run the stick-breaking process backwards from a vector of categorical probabilities to get back Beta-distributed draws Infer the maximum likelihood concentration value given a set of draws. The primary purpose of this section was to get you primed for the next section, in which we try to simulatenously infer the number of prominent mixture components and their distribution parameters. A (ahem!) derivative outcome here was that I hopefully showed you how it is possible to use gradient-based optimization on seemingly discrete problems.","title":"Summary"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Applying Dirichlet-processes to mixture-model clustering Over the previous two sections, we learned about Dirichlet processes and Gaussian Mixture Model-based clustering. In this section, we're going to put the two concepts together! A data problem with unclear number of modes Let's start with a data problem that is a bit trickier to solve: one that has multiple numbers of modes, but for which the mixture distribution visually obscures the true number of modes present. from jax import numpy as np , random , vmap , jit , grad , lax import matplotlib.pyplot as plt weights_true = np . array ([ 2 , 10 , 1 , 6 ]) locs_true = np . array ([ - 2. , - 5. , 3. , 8. ]) scale_true = np . array ([ 1.1 , 2 , 1. , 1.5 ,]) base_n_draws = 1000 key = random . PRNGKey ( 42 ) keys = random . split ( key , 4 ) draws = [] for i in range ( 4 ): shape = int ( base_n_draws * weights_true [ i ]), draw = scale_true [ i ] * random . normal ( keys [ i ], shape = shape ) + locs_true [ i ] draws . append ( draw ) data_mixture = np . concatenate ( draws ) plt . hist ( data_mixture ); From the histogram, it should be easy to tell that this is not going to be an easy problem to solve. Firstly, the mixture distributions in reality have 4 components. But what we get looks more like 2 components... or really? Could it be that we're lying by using a histogram? plt . hist ( data_mixture , bins = 100 ); Aha! The case against histograms reveals itself. Turns out there's lots of problems using histograms, and I shan't go deeper into them here, but obscuring data is one of those issues. To learn more, I wrote a blog post on the matter . In any case, this situation is a clear one where the distribution shape clearly masks the number of mixture components. How can we get around this? Here, we can turn to Dirichlet processes as a tool to help us. Because DPs don't impose an exact number of significant categories on us, but instead allow us to control their number probabilistically with a single \"concentration\" parameter, we can instead write down a model to learn the: concentration parameters, optimal relative weighting of components, conditioned on concentration parameters, distribution parameters for each component, conditioned on data. This effectively forms a Dirichlet-Process Gaussian Mixture Model . Let's see this in action! Dirichlet-Process Gaussian Mixture Model (DP-GMM) The DP-GMM model presumes an infinite (or countably large) number of states, with one Gaussian available per state. The first thing we need to do is to write down the joint log-likelihood of every parameter in our model. As always, before we write down that joint log-likelihood, the first thing we must do is correctly specify what the data generating process is. Data generating process for a DP-GMM This could be our data generating process: We start with a large number of states, and for each one, their likelihood of ocurring is goverened by a concentration parameter. With each state and their corresponding probabilities, we draw a number from the corresponding mixture Gaussian. That number's likelihood is proportional to the state from which it was drawn. With this idea in hand, we can start composing together the joint log-likelihood of the model, conditioned on its parameters and data. Log-likelihood for the component weights The first piece we need to compose together is the component weights. We have that already defined! from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? To quickly recap what this is: it's the log likelihood of a categorical probability vector under a Dirichlet process with a specified concentration parameter. Log-likelihood for the Gaussian mixture The second piece we need is the Gaussian mixture log-likelihood. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? And to recap this one really quickly: this is the log likelihood of the observed data under each of the component weights. Joint log-likelihood Put together, the joint log-likelihood of the Gaussian mixture model is: def joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data ): component_probs = np . exp ( log_component_weights ) probs_ll = component_probs_loglike ( log_component_weights , log_concentration , num_components ) mix_ll = mixture_loglike ( log_component_weights , component_mus , log_component_scales , data ) return probs_ll + mix_ll Through log likelihood function, we are expressing the dependence of the mixture Gaussians on the component probs, and the dependence of the component probs on the concentration parameter. Optimization We can now begin optimizing our mixture model parameters. Loss function As always, we define the loss function. def make_joint_loss ( num_components ): def inner ( params , data ): ( log_component_weights , log_concentration , component_mus , log_component_scales ) = params ll = joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data , ) return - ll return inner joint_loss = make_joint_loss ( num_components = 25 ) The closure pattern is here, so that we can set the number of components to use for Dirichlet estimation without making it part of the params to optimize. Gradient function We then define the gradient function: djoint_loss = grad ( joint_loss ) Because I know these work, I am going to skip over test-driving them. Initialization We'll now start by initializing our parameters. k1 , k2 , k3 , k4 = random . split ( key , 4 ) n_components = 50 log_component_weights_init = random . normal ( k1 , shape = ( n_components ,)) log_concentration_init = random . normal ( k2 , shape = ( 1 ,)) component_mus_init = random . normal ( k3 , shape = ( n_components ,)) log_component_scales_init = random . normal ( k4 , shape = ( n_components ,)) params_init = log_component_weights_init , log_concentration_init , component_mus_init , log_component_scales_init Training Loop Now we write the training loop, leveraging the functions we had before. from jax.experimental.optimizers import adam from dl_workshop.gaussian_mixture import make_step_scannable adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = djoint_loss , update_func = adam_update , data = data_mixture , ) step_scannable = jit ( step_scannable ) Run training Finally, we train the model! from time import time start = time () initial_state = adam_init ( params_init ) N_STEPS = 10000 final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( N_STEPS )) end = time () print ( f \"Time taken: { end - start : .2f } seconds.\" ) Visualize training We're going to make the money figure first. Let's visualize the evolution of the mixture Gaussians over training iteration. params_history = adam_get_params ( state_history ) log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history = params_history from dl_workshop.gaussian_mixture import animate_training %% capture params_for_plotting = [ log_component_weights_history , component_mus_history , log_component_scales_history ] animation = animate_training ( params_for_plotting , int ( N_STEPS / 200 ), data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) And for the losses: joint_loss = jit ( joint_loss ) losses = [] for w , c , m , s in zip ( log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history ): prm = ( w , c , m , s ) l = joint_loss ( prm , data_mixture ) losses . append ( l ) plt . plot ( losses ) plt . yscale ( \"log\" ) from dl_workshop.gaussian_mixture import normalize_weights params_opt = adam_get_params ( final_state ) log_component_weights_opt = params_opt [ 0 ] component_weights_opt = np . exp ( log_component_weights_opt ) plt . plot ( normalize_weights ( component_weights_opt ), marker = \"o\" ) Looks like we are able to recover the major components, in the correct proportions! If you remembered what the data looked like in 1 dimension, there were basically only 3 majorly-identifiable components. Given enough training iterations (we had to go to 10,000 iterations), our trained model was able to identify all of them, while assigning insignificant probability mass to the rest. Some caveats While the main point of this chapter was to show you that it is possible to use gradient-based optimization to cluster data, the same caveats that apply to GMM-based clustering also apply here. For example, label switching is prominent: the components that are prominent may switch at any time during the gradient descent process. If you observed the video carefully, you would see that in action too. When it comes to MCMC for fully Bayesian inference, this is a problem. With maximum likelihood estimation using gradient descent, however, this is less of an issue, as we usually only end up taking the final optimized parameters. Summary The primary purpose of this notebook was to show you that gradient descent is not only for supervised machine learning, but also for unsupervised learning. More generally, gradients can be used anywhere there is an \"optimization\" problem setup. In this case, identifying clusters of data in a mixture model is a classic unsupervised machine learning problem, but because we cast it in the form of a log-likelihood optimization problem, we were able to leverage gradients to solve this problem. Aside from that, we saw the JAX idioms in action: vmap , lax.scan , grad , jit and more. Once again, vmap and lax.scan replaced many of the for-loops that we might have otherwise written, grad gave us easy access to gradients, and jit gave us the advantage of compilation.","title":"DP-GMM"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#applying-dirichlet-processes-to-mixture-model-clustering","text":"Over the previous two sections, we learned about Dirichlet processes and Gaussian Mixture Model-based clustering. In this section, we're going to put the two concepts together!","title":"Applying Dirichlet-processes to mixture-model clustering"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#a-data-problem-with-unclear-number-of-modes","text":"Let's start with a data problem that is a bit trickier to solve: one that has multiple numbers of modes, but for which the mixture distribution visually obscures the true number of modes present. from jax import numpy as np , random , vmap , jit , grad , lax import matplotlib.pyplot as plt weights_true = np . array ([ 2 , 10 , 1 , 6 ]) locs_true = np . array ([ - 2. , - 5. , 3. , 8. ]) scale_true = np . array ([ 1.1 , 2 , 1. , 1.5 ,]) base_n_draws = 1000 key = random . PRNGKey ( 42 ) keys = random . split ( key , 4 ) draws = [] for i in range ( 4 ): shape = int ( base_n_draws * weights_true [ i ]), draw = scale_true [ i ] * random . normal ( keys [ i ], shape = shape ) + locs_true [ i ] draws . append ( draw ) data_mixture = np . concatenate ( draws ) plt . hist ( data_mixture ); From the histogram, it should be easy to tell that this is not going to be an easy problem to solve. Firstly, the mixture distributions in reality have 4 components. But what we get looks more like 2 components... or really? Could it be that we're lying by using a histogram? plt . hist ( data_mixture , bins = 100 ); Aha! The case against histograms reveals itself. Turns out there's lots of problems using histograms, and I shan't go deeper into them here, but obscuring data is one of those issues. To learn more, I wrote a blog post on the matter . In any case, this situation is a clear one where the distribution shape clearly masks the number of mixture components. How can we get around this? Here, we can turn to Dirichlet processes as a tool to help us. Because DPs don't impose an exact number of significant categories on us, but instead allow us to control their number probabilistically with a single \"concentration\" parameter, we can instead write down a model to learn the: concentration parameters, optimal relative weighting of components, conditioned on concentration parameters, distribution parameters for each component, conditioned on data. This effectively forms a Dirichlet-Process Gaussian Mixture Model . Let's see this in action!","title":"A data problem with unclear number of modes"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#dirichlet-process-gaussian-mixture-model-dp-gmm","text":"The DP-GMM model presumes an infinite (or countably large) number of states, with one Gaussian available per state. The first thing we need to do is to write down the joint log-likelihood of every parameter in our model. As always, before we write down that joint log-likelihood, the first thing we must do is correctly specify what the data generating process is.","title":"Dirichlet-Process Gaussian Mixture Model (DP-GMM)"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#data-generating-process-for-a-dp-gmm","text":"This could be our data generating process: We start with a large number of states, and for each one, their likelihood of ocurring is goverened by a concentration parameter. With each state and their corresponding probabilities, we draw a number from the corresponding mixture Gaussian. That number's likelihood is proportional to the state from which it was drawn. With this idea in hand, we can start composing together the joint log-likelihood of the model, conditioned on its parameters and data.","title":"Data generating process for a DP-GMM"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#log-likelihood-for-the-component-weights","text":"The first piece we need to compose together is the component weights. We have that already defined! from dl_workshop.gaussian_mixture import component_probs_loglike component_probs_loglike ?? To quickly recap what this is: it's the log likelihood of a categorical probability vector under a Dirichlet process with a specified concentration parameter.","title":"Log-likelihood for the component weights"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#log-likelihood-for-the-gaussian-mixture","text":"The second piece we need is the Gaussian mixture log-likelihood. from dl_workshop.gaussian_mixture import mixture_loglike mixture_loglike ?? And to recap this one really quickly: this is the log likelihood of the observed data under each of the component weights.","title":"Log-likelihood for the Gaussian mixture"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#joint-log-likelihood","text":"Put together, the joint log-likelihood of the Gaussian mixture model is: def joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data ): component_probs = np . exp ( log_component_weights ) probs_ll = component_probs_loglike ( log_component_weights , log_concentration , num_components ) mix_ll = mixture_loglike ( log_component_weights , component_mus , log_component_scales , data ) return probs_ll + mix_ll Through log likelihood function, we are expressing the dependence of the mixture Gaussians on the component probs, and the dependence of the component probs on the concentration parameter.","title":"Joint log-likelihood"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#optimization","text":"We can now begin optimizing our mixture model parameters.","title":"Optimization"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#loss-function","text":"As always, we define the loss function. def make_joint_loss ( num_components ): def inner ( params , data ): ( log_component_weights , log_concentration , component_mus , log_component_scales ) = params ll = joint_loglike ( log_component_weights , log_concentration , num_components , component_mus , log_component_scales , data , ) return - ll return inner joint_loss = make_joint_loss ( num_components = 25 ) The closure pattern is here, so that we can set the number of components to use for Dirichlet estimation without making it part of the params to optimize.","title":"Loss function"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#gradient-function","text":"We then define the gradient function: djoint_loss = grad ( joint_loss ) Because I know these work, I am going to skip over test-driving them.","title":"Gradient function"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#initialization","text":"We'll now start by initializing our parameters. k1 , k2 , k3 , k4 = random . split ( key , 4 ) n_components = 50 log_component_weights_init = random . normal ( k1 , shape = ( n_components ,)) log_concentration_init = random . normal ( k2 , shape = ( 1 ,)) component_mus_init = random . normal ( k3 , shape = ( n_components ,)) log_component_scales_init = random . normal ( k4 , shape = ( n_components ,)) params_init = log_component_weights_init , log_concentration_init , component_mus_init , log_component_scales_init","title":"Initialization"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#training-loop","text":"Now we write the training loop, leveraging the functions we had before. from jax.experimental.optimizers import adam from dl_workshop.gaussian_mixture import make_step_scannable adam_init , adam_update , adam_get_params = adam ( 0.05 ) step_scannable = make_step_scannable ( get_params_func = adam_get_params , dloss_func = djoint_loss , update_func = adam_update , data = data_mixture , ) step_scannable = jit ( step_scannable )","title":"Training Loop"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#run-training","text":"Finally, we train the model! from time import time start = time () initial_state = adam_init ( params_init ) N_STEPS = 10000 final_state , state_history = lax . scan ( step_scannable , initial_state , np . arange ( N_STEPS )) end = time () print ( f \"Time taken: { end - start : .2f } seconds.\" )","title":"Run training"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#visualize-training","text":"We're going to make the money figure first. Let's visualize the evolution of the mixture Gaussians over training iteration. params_history = adam_get_params ( state_history ) log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history = params_history from dl_workshop.gaussian_mixture import animate_training %% capture params_for_plotting = [ log_component_weights_history , component_mus_history , log_component_scales_history ] animation = animate_training ( params_for_plotting , int ( N_STEPS / 200 ), data_mixture ) from IPython.display import HTML HTML ( animation . to_html5_video ()) And for the losses: joint_loss = jit ( joint_loss ) losses = [] for w , c , m , s in zip ( log_component_weights_history , log_concentration_history , component_mus_history , log_component_scales_history ): prm = ( w , c , m , s ) l = joint_loss ( prm , data_mixture ) losses . append ( l ) plt . plot ( losses ) plt . yscale ( \"log\" ) from dl_workshop.gaussian_mixture import normalize_weights params_opt = adam_get_params ( final_state ) log_component_weights_opt = params_opt [ 0 ] component_weights_opt = np . exp ( log_component_weights_opt ) plt . plot ( normalize_weights ( component_weights_opt ), marker = \"o\" ) Looks like we are able to recover the major components, in the correct proportions! If you remembered what the data looked like in 1 dimension, there were basically only 3 majorly-identifiable components. Given enough training iterations (we had to go to 10,000 iterations), our trained model was able to identify all of them, while assigning insignificant probability mass to the rest.","title":"Visualize training"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#some-caveats","text":"While the main point of this chapter was to show you that it is possible to use gradient-based optimization to cluster data, the same caveats that apply to GMM-based clustering also apply here. For example, label switching is prominent: the components that are prominent may switch at any time during the gradient descent process. If you observed the video carefully, you would see that in action too. When it comes to MCMC for fully Bayesian inference, this is a problem. With maximum likelihood estimation using gradient descent, however, this is less of an issue, as we usually only end up taking the final optimized parameters.","title":"Some caveats"},{"location":"04-gaussian-clustering/03-dirichlet-process-clustering/#summary","text":"The primary purpose of this notebook was to show you that gradient descent is not only for supervised machine learning, but also for unsupervised learning. More generally, gradients can be used anywhere there is an \"optimization\" problem setup. In this case, identifying clusters of data in a mixture model is a classic unsupervised machine learning problem, but because we cast it in the form of a log-likelihood optimization problem, we were able to leverage gradients to solve this problem. Aside from that, we saw the JAX idioms in action: vmap , lax.scan , grad , jit and more. Once again, vmap and lax.scan replaced many of the for-loops that we might have otherwise written, grad gave us easy access to gradients, and jit gave us the advantage of compilation.","title":"Summary"},{"location":"appendix/02-partials/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Closures and Partials We're going to take a quick detour and look at this idea of \"partially evaluating a function\". This is going to be important, as it'll allow us to construct functions that are compatible with the requirements of vmap and lax.scan and others in JAX, i.e. they have the correct function signature, but still allow us the flexibility to put in arbitrary things that might be needed for the function to work correctly. There are two ways to do this: you can either use functools.partial , or you can use function closures. Let's see how to do this. Partially evaluating a function using functools.partial For simplicity's sake, let's explore the idea using a function that adds two numbers: def add ( a , b ): return a + b Now, let's say we wanted to fix b to the value 3 , thus generating an add_three function. We can do this two ways. The first is by functools.partial : from functools import partial add_three = partial ( add , b = 3 ) We can now call add_three on any value of a : add_three ( 20 ) 23 If we inspect the function add_three : add_three ? Signature: add_three ( a , * , b = 3 ) Call signature: add_three ( * args , ** kwargs ) Type: partial String form: functools.partial(<function add at 0x163aaeaf0>, b=3) File: ~/anaconda/envs/dl-workshop/lib/python3.9/functools.py Docstring: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords. We see that add_three accepts one positional argument, a , and its value of b has been set to a default of 3 . What if we wanted to fix a to 3 instead? add_three_v2 = partial ( add , a = 3 ) add_three_v2 ? Signature: add_three_v2 ( * , a = 3 , b ) Call signature: add_three_v2 ( * args , ** kwargs ) Type: partial String form: functools.partial(<function add at 0x163aaeaf0>, a=3) File: ~/anaconda/envs/dl-workshop/lib/python3.9/functools.py Docstring: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords. Notice how now the function signature has changed, such that b is not set while a has been. This has implications for how we use the function. Calling the function this way will error out: >>> add_three_v2 ( 3 ) --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 109 - e78f540eb25e > in < module > ----> 1 add_three_v2 ( 3 ) TypeError : add () got multiple values for argument 'a' That is because when we pass in the argument with no keyword specified, it is interpreted as the first positional argument, which as you can see, has already been set. On the other hand, calling the function this way will not: add_three_v2 ( b = 3 ) 6 Creating closures Another pattern that we can use is to use closures. Closures are functions that return a closed function that contains information from the closing function. Confused? Let me illustrate: def closing_function ( a ): def closed_function ( b ): return a + b return closed_function Using this pattern, we can rewrite add_three using closures: def make_add_something ( value ): def closed_function ( b ): return b + value return closed_function add_three_v3 = make_add_something ( 3 ) add_three_v3 ( 5 ) 8 add_three_v3 ? Signature: add_three_v3 ( b ) Docstring: <no docstring> File: ~/github/tutorials/dl-workshop/notebooks/appendix-01-functional-programming/<ipython-input-9-78be55fdfc22> Type: function Now, you'll notice that the signature of add_three_v3 follows that exactly of the closed function. When writing array programs using JAX, this is the key design pattern you'll want to implement: Always return a function that has the function signature that you need. Naming things is the hardest activity in programming, because we are giving categorical names to things, and sometimes their category of thing isn't always clear. Fret not: the pattern I'll give you is the following: def SOME_FUNCTION_generator ( argument1 , argument2 , keyword_arugment1 = default_value1 ): \"\"\"To simplify things, just give the name of the closing function <some_function>_generator.\"\"\" def inner ( arg1 , arg2 , kwarg1 = default_value1 ): \"\"\"This function should follow the API that is neeed.\"\"\" return something return inner","title":"Partials"},{"location":"appendix/02-partials/#closures-and-partials","text":"We're going to take a quick detour and look at this idea of \"partially evaluating a function\". This is going to be important, as it'll allow us to construct functions that are compatible with the requirements of vmap and lax.scan and others in JAX, i.e. they have the correct function signature, but still allow us the flexibility to put in arbitrary things that might be needed for the function to work correctly. There are two ways to do this: you can either use functools.partial , or you can use function closures. Let's see how to do this.","title":"Closures and Partials"},{"location":"appendix/02-partials/#partially-evaluating-a-function-using-functoolspartial","text":"For simplicity's sake, let's explore the idea using a function that adds two numbers: def add ( a , b ): return a + b Now, let's say we wanted to fix b to the value 3 , thus generating an add_three function. We can do this two ways. The first is by functools.partial : from functools import partial add_three = partial ( add , b = 3 ) We can now call add_three on any value of a : add_three ( 20 ) 23 If we inspect the function add_three : add_three ? Signature: add_three ( a , * , b = 3 ) Call signature: add_three ( * args , ** kwargs ) Type: partial String form: functools.partial(<function add at 0x163aaeaf0>, b=3) File: ~/anaconda/envs/dl-workshop/lib/python3.9/functools.py Docstring: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords. We see that add_three accepts one positional argument, a , and its value of b has been set to a default of 3 . What if we wanted to fix a to 3 instead? add_three_v2 = partial ( add , a = 3 ) add_three_v2 ? Signature: add_three_v2 ( * , a = 3 , b ) Call signature: add_three_v2 ( * args , ** kwargs ) Type: partial String form: functools.partial(<function add at 0x163aaeaf0>, a=3) File: ~/anaconda/envs/dl-workshop/lib/python3.9/functools.py Docstring: partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords. Notice how now the function signature has changed, such that b is not set while a has been. This has implications for how we use the function. Calling the function this way will error out: >>> add_three_v2 ( 3 ) --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 109 - e78f540eb25e > in < module > ----> 1 add_three_v2 ( 3 ) TypeError : add () got multiple values for argument 'a' That is because when we pass in the argument with no keyword specified, it is interpreted as the first positional argument, which as you can see, has already been set. On the other hand, calling the function this way will not: add_three_v2 ( b = 3 ) 6","title":"Partially evaluating a function using functools.partial"},{"location":"appendix/02-partials/#creating-closures","text":"Another pattern that we can use is to use closures. Closures are functions that return a closed function that contains information from the closing function. Confused? Let me illustrate: def closing_function ( a ): def closed_function ( b ): return a + b return closed_function Using this pattern, we can rewrite add_three using closures: def make_add_something ( value ): def closed_function ( b ): return b + value return closed_function add_three_v3 = make_add_something ( 3 ) add_three_v3 ( 5 ) 8 add_three_v3 ? Signature: add_three_v3 ( b ) Docstring: <no docstring> File: ~/github/tutorials/dl-workshop/notebooks/appendix-01-functional-programming/<ipython-input-9-78be55fdfc22> Type: function Now, you'll notice that the signature of add_three_v3 follows that exactly of the closed function. When writing array programs using JAX, this is the key design pattern you'll want to implement: Always return a function that has the function signature that you need. Naming things is the hardest activity in programming, because we are giving categorical names to things, and sometimes their category of thing isn't always clear. Fret not: the pattern I'll give you is the following: def SOME_FUNCTION_generator ( argument1 , argument2 , keyword_arugment1 = default_value1 ): \"\"\"To simplify things, just give the name of the closing function <some_function>_generator.\"\"\" def inner ( arg1 , arg2 , kwarg1 = default_value1 ): \"\"\"This function should follow the API that is neeed.\"\"\" return something return inner","title":"Creating closures"}]}
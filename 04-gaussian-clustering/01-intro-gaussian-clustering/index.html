
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.1.4">
    
    
      
        <title>Introduction - Differential Progamming Tutorial</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.bde7dde4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ef6f36e2.min.css">
        
          
          
          <meta name="theme-color" content="#4cae4f">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="https://unpkg.com/mermaid@7.1.2/dist/mermaid.css">
    
      <link rel="stylesheet" href="../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../css/pandas-dataframe.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="green" data-md-color-accent="green">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#gaussian-mixture-model-based-clustering" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Differential Progamming Tutorial" class="md-header__button md-logo" aria-label="Differential Progamming Tutorial" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Differential Progamming Tutorial
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Introduction
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/ericmjl/dl-workshop/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ericmjl/dl-workshop
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Differential Progamming Tutorial" class="md-nav__button md-logo" aria-label="Differential Progamming Tutorial" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Differential Progamming Tutorial
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/ericmjl/dl-workshop/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ericmjl/dl-workshop
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        Preliminaries
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Preliminaries" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Preliminaries
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../00-preliminaries/01-preface/" class="md-nav__link">
        Preface
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../00-preliminaries/02-prerequisites/" class="md-nav__link">
        Prerequisites
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../00-preliminaries/03-setup/" class="md-nav__link">
        Setup
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        Introduction to Differential Programming
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Introduction to Differential Programming" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Introduction to Differential Programming
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../01-differential-programming/01-neural-nets-from-scratch/" class="md-nav__link">
        Neural Networks from Scratch
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../01-differential-programming/02-gradient-optimization/" class="md-nav__link">
        Gradient Based Optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../01-differential-programming/03-linear-model-optimization/" class="md-nav__link">
        Linear Model
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../01-differential-programming/04-logistic-regression/" class="md-nav__link">
        Logistic Regression
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../01-differential-programming/05-neural-networks/" class="md-nav__link">
        Neural Networks
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      <label class="md-nav__link" for="__nav_4">
        JAX Programming Idioms
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="JAX Programming Idioms" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          JAX Programming Idioms
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../02-jax-idioms/00-introduction/" class="md-nav__link">
        Introduction
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../02-jax-idioms/01-loopless-loops/" class="md-nav__link">
        Loopless Loops
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../02-jax-idioms/02-loopy-carry/" class="md-nav__link">
        Loopy Carry
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../02-jax-idioms/03-deterministic-randomness/" class="md-nav__link">
        Deterministic Randomness
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../02-jax-idioms/04-optimized-learning/" class="md-nav__link">
        Optimized Learning
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      <label class="md-nav__link" for="__nav_5">
        The stax Module
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="The stax Module" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          The stax Module
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../03-stax/01-linear/" class="md-nav__link">
        Linear Models with stax
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../03-stax/02-neural/" class="md-nav__link">
        Neural Networks with stax
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" checked>
      
      <label class="md-nav__link" for="__nav_6">
        Gaussian Clustering
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Gaussian Clustering" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Gaussian Clustering
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Introduction
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Introduction
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#likelihoods-of-mixture-data" class="md-nav__link">
    Likelihoods of Mixture Data
  </a>
  
    <nav class="md-nav" aria-label="Likelihoods of Mixture Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-one-datum-under-one-component" class="md-nav__link">
    Log Likelihood of One Datum under One Component
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-one-datum-under-all-components" class="md-nav__link">
    Log Likelihood of One Datum under All Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-all-data-under-all-components" class="md-nav__link">
    Log Likelihood of All Data under All Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-weighting" class="md-nav__link">
    Log Likelihood of Weighting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#review-thus-far" class="md-nav__link">
    Review thus far
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-to-find-maximum-likelihood-values" class="md-nav__link">
    Gradient descent to find maximum likelihood values
  </a>
  
    <nav class="md-nav" aria-label="Gradient descent to find maximum likelihood values">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#loss-function" class="md-nav__link">
    Loss function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-of-loss-function" class="md-nav__link">
    Gradient of loss function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-initialization" class="md-nav__link">
    Parameter Initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-drive-functions" class="md-nav__link">
    Test-drive functions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#defining-performant-training-loops" class="md-nav__link">
    Defining performant training loops
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sanity-checking-whether-learning-has-happened" class="md-nav__link">
    Sanity-checking whether learning has happened
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#visualizing-training-dynamics" class="md-nav__link">
    Visualizing training dynamics
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../02-dirichlet-processes/" class="md-nav__link">
        Dirichlet Processes
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../03-dirichlet-process-clustering/" class="md-nav__link">
        DP-GMM
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      <label class="md-nav__link" for="__nav_7">
        Appendix
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Appendix" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Appendix
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../appendix/02-partials/" class="md-nav__link">
        Partials
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#likelihoods-of-mixture-data" class="md-nav__link">
    Likelihoods of Mixture Data
  </a>
  
    <nav class="md-nav" aria-label="Likelihoods of Mixture Data">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-one-datum-under-one-component" class="md-nav__link">
    Log Likelihood of One Datum under One Component
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-one-datum-under-all-components" class="md-nav__link">
    Log Likelihood of One Datum under All Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-all-data-under-all-components" class="md-nav__link">
    Log Likelihood of All Data under All Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-likelihood-of-weighting" class="md-nav__link">
    Log Likelihood of Weighting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#review-thus-far" class="md-nav__link">
    Review thus far
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-to-find-maximum-likelihood-values" class="md-nav__link">
    Gradient descent to find maximum likelihood values
  </a>
  
    <nav class="md-nav" aria-label="Gradient descent to find maximum likelihood values">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#loss-function" class="md-nav__link">
    Loss function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-of-loss-function" class="md-nav__link">
    Gradient of loss function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-initialization" class="md-nav__link">
    Parameter Initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-drive-functions" class="md-nav__link">
    Test-drive functions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#defining-performant-training-loops" class="md-nav__link">
    Defining performant training loops
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sanity-checking-whether-learning-has-happened" class="md-nav__link">
    Sanity-checking whether learning has happened
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#visualizing-training-dynamics" class="md-nav__link">
    Visualizing training dynamics
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ericmjl/dl-workshop/edit/master/docs/04-gaussian-clustering/01-intro-gaussian-clustering.ipynb" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script>
(function() {
  function addWidgetsRenderer() {
    var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
    var scriptElement = document.createElement('script');
    var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js';
    var widgetState;

    // Fallback for older version:
    try {
      widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

      if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
        widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';
      }
    } catch(e) {}

    scriptElement.src = widgetRendererSrc;
    document.body.appendChild(scriptElement);
  }

  document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
}());
</script>

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://mybinder.org/v2/gh/ericmjl/dl-workshop/master?urlpath=%2Ftree%2Fnotebooks%2F04-gaussian-clustering%2F01-intro-gaussian-clustering.ipynb"><img alt="Binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;retina&#39;</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="gaussian-mixture-model-based-clustering">Gaussian mixture model-based clustering</h1>
<p>In this notebook, we are going to take a look at how to cluster Gaussian-distributed data.</p>
<p>Imagine you have data that are multi-modal, something that looks like the following:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">weights_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>  <span class="c1"># 1:5 ratio</span>
<span class="n">locs_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">])</span>  <span class="c1"># different means</span>
<span class="n">scale_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># different variances</span>

<span class="n">base_n_draws</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">k1</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

<span class="n">draws_1</span> <span class="o">=</span> <span class="n">scale_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">base_n_draws</span> <span class="o">*</span> <span class="n">weights_true</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span> <span class="o">+</span> <span class="n">locs_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">draws_2</span> <span class="o">=</span> <span class="n">scale_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k2</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">base_n_draws</span> <span class="o">*</span> <span class="n">weights_true</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span> <span class="o">+</span> <span class="n">locs_true</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">data_mixture</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">draws_1</span><span class="p">,</span> <span class="n">draws_2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data_mixture</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">40</span><span class="p">);</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="likelihoods-of-mixture-data">Likelihoods of Mixture Data</h2>
<p>We might look at this data and say, "I think there's two clusters of data here." One that belongs to the left mode, and one that belongs to the right mode. By visual inspection, the relative weighting might be about 1:3 to 1:6, or somewhere in between.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What might be the "data generating process" here?</p>
<p>Well, we could claim that when a data point is drawn from the mixture distribution, it could have come from <em>either</em> of the modes.
By basic probability logic, the joint likelihood of observing the data point is:</p>
<ul>
<li>The likelihood that the datum came from the left Gaussian, times the probability of drawing a number from the left Gaussian, plus...</li>
<li>The likelihood that the datum came from the right Gaussian, times the probability of drawing a number from the right Gaussian.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Phrased more generally:</p>
<blockquote>
<p>The sum over "components <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> of the likelihood that the datum <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> came from Gaussian <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> with parameters <span><span class="MathJax_Preview">\mu_j, \sigma_j</span><script type="math/tex">\mu_j, \sigma_j</script></span> times the likelihood of observing a draw from component <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>."</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In math, we would need to calculate:</p>
<div>
<div class="MathJax_Preview">\sum_j P(x_i|\mu_j, \sigma_j) P(\mu_j, \sigma_j|w_j) P(w_j)</div>
<script type="math/tex; mode=display">\sum_j P(x_i|\mu_j, \sigma_j) P(\mu_j, \sigma_j|w_j) P(w_j)</script>
</div>
<p>Now, we can make the middle term <span><span class="MathJax_Preview">P(\mu_j, \sigma_j|w_j)</span><script type="math/tex">P(\mu_j, \sigma_j|w_j)</script></span> is always 1, by assuming that the <span><span class="MathJax_Preview">\mu_j</span><script type="math/tex">\mu_j</script></span> and <span><span class="MathJax_Preview">\sigma_j</span><script type="math/tex">\sigma_j</script></span> chosen are always fixed given the component weight chosen. The expression then simplifies to:</p>
<div>
<div class="MathJax_Preview">\sum_j P(x_i|\mu_j, \sigma_j) P(w_j)</div>
<script type="math/tex; mode=display">\sum_j P(x_i|\mu_j, \sigma_j) P(w_j)</script>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="log-likelihood-of-one-datum-under-one-component">Log Likelihood of One Datum under One Component</h3>
<p>Because this is a summation, let's work out the elementary steps first.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">loglike_one_component</span>

<span class="n">loglike_one_component</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The summation here is because we are operating in logarithmic space.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You might ask, why do we use "log" of the component scale?
This is a math trick that helps us whenever we are doing computations in an unbounded space.
When doing gradient descent,
we can never guarantee that a gradient update on a parameter that ought to be positive-only
will give us a positive number.
Thus, for positive numbers, we operate in logarithmic space.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can quickly write a test here. 
If the component probability is 1.0, 
the component <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> is 0, and the observed datum is also 0, 
it should equal to the log-likelihood of 0 
under a unit Gaussian.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">jax.scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="n">our_test</span> <span class="o">=</span> <span class="n">loglike_one_component</span><span class="p">(</span>
    <span class="n">component_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    <span class="n">component_mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> 
    <span class="n">log_component_scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span><span class="p">),</span> 
    <span class="n">datum</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

<span class="n">ground_truth</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">our_test</span><span class="p">,</span> <span class="n">ground_truth</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="log-likelihood-of-one-datum-under-all-components">Log Likelihood of One Datum under All Components</h3>
<p>Now that we are done with the elementary computation of one datum under one component, we can <code>vmap</code> the log-likelihood calculation over all components, thereby giving us the loglikelihood of a datum under any of the possible given components.</p>
<p>Firstly, we need a function that normalizes component weights to sum to 1. This is enforced just in case during the gradient descent procedure, we end up with weights that do not sum to 1.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">normalize_weights</span><span class="p">,</span> <span class="n">loglike_across_components</span>

<span class="n">normalize_weights</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we leverage the <code>normalize_weights</code> function inside a <code>loglike_across_components</code> function, which <code>vmap</code>s the log likelihood calculation across components:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">loglike_across_components</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Inside that function, we first calculated elementwise the log-likelihood of observing that data under each component.
That only gives us per-component log-likelihoods though.
Because our data could have been drawn from any of those components,
the total likelihood is a <em>sum</em> of the per-component likelihoods.
Thus, we have to elementwise exponentiate the log-likelihoods first.
Because we have sum up each of those probability components together,
a shortcut function we have access to is the <a href="https://en.wikipedia.org/wiki/LogSumExp">logsumexp</a> function,
which first exponentiates each of the probabilities,
sums them up,
and then takes their log again,
thereby accomplishing what we need.</p>
<p>We could have written our own version of the function,
but I think it makes a ton of sense
to trust the numerically-stable,
professionally-implemented version provided
in SciPy!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The choice to pass in <code>log_component_weights</code> rather than <code>weights</code> is because the <code>normalize_weights</code> function assumes that all numbers in the vector are positive, but in gradient descent, we operate in an unbounded space, which may bring us into negative numbers. To make things safe, we assume the numbers come to us from an unbounded space, and then use an exponential transform first before normalizing.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us now test-drive our <code>loglike_across_components</code> function,
which should give us a scalar value at the end.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">loglike_across_components</span><span class="p">(</span>
    <span class="n">log_component_weights</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">weights_true</span><span class="p">),</span>
    <span class="n">component_mus</span><span class="o">=</span><span class="n">locs_true</span><span class="p">,</span>
    <span class="n">log_component_scales</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">scale_true</span><span class="p">),</span>
    <span class="n">datum</span><span class="o">=</span><span class="n">data_mixture</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great, that worked!</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="log-likelihood-of-all-data-under-all-components">Log Likelihood of All Data under All Components</h3>
<p>Now that we've got the log-likelihood of each datum under each component,
we can now <code>vmap</code> the function across all data given to us.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mathematically, this would be:</p>
<div>
<div class="MathJax_Preview">\prod_i \sum_j P(x_i|\mu_j, \sigma_j) P(w_j)</div>
<script type="math/tex; mode=display">\prod_i \sum_j P(x_i|\mu_j, \sigma_j) P(w_j)</script>
</div>
<p>Or in prose:</p>
<blockquote>
<p>The total likelihood of all datum <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> together under all components <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> is given by first summing the likelihoods of each datum <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> under each component <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>, and then taking the product of likelihoods for each data point <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>, assuming data are i.i.d. from the mixture distribution.</p>
</blockquote>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">mixture_loglike</span>

<span class="n">mixture_loglike</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice how we <code>vmap</code>-ed the <code>loglike_across_components</code> function over all data points provided in the function above. This helped us eliminate a for-loop, basically!</p>
<p>If we execute the function, we should get a scalar value.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">mixture_loglike</span><span class="p">(</span>
    <span class="n">log_component_weights</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">weights_true</span><span class="p">),</span>
    <span class="n">component_mus</span><span class="o">=</span><span class="n">locs_true</span><span class="p">,</span>
    <span class="n">log_component_scales</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">scale_true</span><span class="p">),</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data_mixture</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="log-likelihood-of-weighting">Log Likelihood of Weighting</h3>
<p>The final thing we are missing is a generative story for the weights. In other words, we are asking the question, "How did the weights come about?"</p>
<p>We might say that the weights were drawn from a Dirichlet distribution (the generalization of a Beta distribution to multiple dimensions), and as a naïve first pass, were drawn with equal probability.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">weights_loglike</span>

<span class="n">weights_loglike</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">alpha_prior</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">weights_true</span><span class="p">)</span>
<span class="n">weights_loglike</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">weights_true</span><span class="p">),</span> <span class="n">alpha_prior</span><span class="o">=</span><span class="n">alpha_prior</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="review-thus-far">Review thus far</h3>
<p>Now that we have composed together our generative story for the data,
let's pause for a moment and break down our model a bit.
This will serve as a review of what we've done.</p>
<p>Firstly, we have our "model", i.e. the log-likelihood of our data
conditioned on some parameter set and their values.</p>
<p>Secondly, our parameters of the model are:</p>
<ol>
<li>Component weights.</li>
<li>Component central tendencies/means</li>
<li>Component scales/variances.</li>
</ol>
<p>What we're going to attempt next is to use gradient based optimization to learn what those parameters are, conditioned on data, leveraging the JAX idioms that we've learned before.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="gradient-descent-to-find-maximum-likelihood-values">Gradient descent to find maximum likelihood values</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given a mixture Gaussian dataset, one natural task we might want to do is estimate the weights, central tendencies/means and scales/variances from data. This corresponds naturally to a maximum likelihood estimation task.</p>
<p>Now, one thing we know is that JAX's optimizers assume we are <em>minimizing</em> a function, so to use JAX's optimizers with a maximum likelihood function, we simply take the negative of the log likelihood and minimize that.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="loss-function">Loss function</h3>
<p>Let's first take a look at the loss function.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">loss_mixture_weights</span>

<span class="n">loss_mixture_weights</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, our function is designed to be compatible with JAX's <code>grad</code>. We are taking derivatives w.r.t. the first argument, the parameters, which we unpack into our likelihood function parameters.</p>
<p>The two likelihood functions are used inside there too:</p>
<ul>
<li><code>mixture_loglike</code></li>
<li><code>weights_loglike</code></li>
</ul>
<p>The <code>alpha_prior</code> is hard-coded; it's not the most ideal. For convenience, I have just hard-coded it, but the principled way to handle this is to add it as a keyword argument that gets passed in.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="gradient-of-loss-function">Gradient of loss function</h3>
<p>As usual, we now define the gradient function of <code>loss_mixture_weights</code> by calling <code>grad</code> on it:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>

<span class="n">dloss_mixture_weights</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss_mixture_weights</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="parameter-initialization">Parameter Initialization</h3>
<p>Next up, we initialize our parameters randomly. For convenience, we'll use Gaussian draws.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">N_MIXTURE_COMPONENTS</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">k3</span><span class="p">,</span> <span class="n">k4</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">log_component_weights_init</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N_MIXTURE_COMPONENTS</span><span class="p">,))</span>
<span class="n">component_mus_init</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k2</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N_MIXTURE_COMPONENTS</span><span class="p">,))</span>
<span class="n">log_component_scales_init</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k3</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N_MIXTURE_COMPONENTS</span><span class="p">,))</span>

<span class="n">params_init</span> <span class="o">=</span> <span class="n">log_component_weights_init</span><span class="p">,</span> <span class="n">component_mus_init</span><span class="p">,</span> <span class="n">log_component_scales_init</span>
<span class="n">params_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">weights_true</span><span class="p">),</span> <span class="n">locs_true</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">scale_true</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here, you see JAX's controllable handling of random numbers. Our parameters are always going to be initialized in exactly the same way on each notebook cell re-run, since we have explicit keys passed in.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="test-drive-functions">Test-drive functions</h3>
<p>Let's test-drive the functions to make sure that they work correctly.</p>
<p>For the loss function, we should expect to get back a scalar. If we pass in initialized parameters, it should also have a higher value (corresponding to more lower log likelihood) than if we pass in true parameters.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">loss_mixture_weights</span><span class="p">(</span><span class="n">params_true</span><span class="p">,</span> <span class="n">data_mixture</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">loss_mixture_weights</span><span class="p">(</span><span class="n">params_init</span><span class="p">,</span> <span class="n">data_mixture</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Indeed, both criteria are satisfied. </p>
<p>Test-driving the gradient function should give us a tuple of gradients evaluated.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">dloss_mixture_weights</span><span class="p">(</span><span class="n">params_init</span><span class="p">,</span> <span class="n">data_mixture</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="defining-performant-training-loops">Defining performant training loops</h3>
<p>Now, we are going to use JAX's optimizers inside a <code>lax.scan</code>-ed training loop
to get fast training going.</p>
<p>We begin with the elementary "step" function.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">step</span>

<span class="n">step</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This should look familiar to you. At each step of the loop, we unpack params from a JAX optimizer state, obtain gradients, and then update the state using the gradients.</p>
<p>We then make the elementary step function a scannable one using <code>lax.scan</code>.
This will allow us to "scan" the function across an array
that represents the number of optimization steps we will be using.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">make_step_scannable</span>

<span class="n">make_step_scannable</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Recall that the inner function that gets returned here has the API that we require for using <code>lax.scan</code>: </p>
<ul>
<li><code>previous_state</code> corresponds to the <code>carry</code>, and</li>
<li><code>iteration</code> corresponds to the <code>x</code>.</li>
</ul>
<p>Now we actually instantiate the scannable step.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">jax.experimental.optimizers</span> <span class="kn">import</span> <span class="n">adam</span>

<span class="n">adam_init</span><span class="p">,</span> <span class="n">adam_update</span><span class="p">,</span> <span class="n">adam_get_params</span> <span class="o">=</span> <span class="n">adam</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">step_scannable</span> <span class="o">=</span> <span class="n">make_step_scannable</span><span class="p">(</span>
    <span class="n">get_params_func</span><span class="o">=</span><span class="n">adam_get_params</span><span class="p">,</span>
    <span class="n">dloss_func</span><span class="o">=</span><span class="n">dloss_mixture_weights</span><span class="p">,</span>
    <span class="n">update_func</span><span class="o">=</span><span class="n">adam_update</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data_mixture</span><span class="p">,</span> 
<span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, we <code>lax.scan</code> <code>step_scannable</code> over 1000 iterations (constructed as an <code>np.arange()</code> array).</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">lax</span>

<span class="n">initial_state</span> <span class="o">=</span> <span class="n">adam_init</span><span class="p">(</span><span class="n">params_init</span><span class="p">)</span>

<span class="n">final_state</span><span class="p">,</span> <span class="n">state_history</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">step_scannable</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="sanity-checking-whether-learning-has-happened">Sanity-checking whether learning has happened</h3>
<p>We can sanity check whether learning has happened.</p>
<p>The loss function value for optimized parameters should be pretty close to the loss function when we put in true params.
(Do keep in mind that because we have data that are an imperfect sample of the ground truth distribution,
it is possible that our optimized params' negative log likelihood will be different than that of the true params.)</p>
<p>Firstly, we unpack the parameters of the final state:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">params_opt</span> <span class="o">=</span> <span class="n">adam_get_params</span><span class="p">(</span><span class="n">final_state</span><span class="p">)</span>
<span class="n">log_component_weights_opt</span><span class="p">,</span> <span class="n">component_mus_opt</span><span class="p">,</span> <span class="n">log_component_scales_opt</span> <span class="o">=</span> <span class="n">params_opt</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, we look at the loss for the optimized params:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">loss_mixture_weights</span><span class="p">(</span><span class="n">params_opt</span><span class="p">,</span> <span class="n">data_mixture</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It should be lower than the loss for the initialized params</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">loss_mixture_weights</span><span class="p">(</span><span class="n">params_init</span><span class="p">,</span> <span class="n">data_mixture</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Indeed that is so!</p>
<p>And if we inspect the component weights:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_component_weights_opt</span><span class="p">),</span> <span class="n">weights_true</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Indeed, we have optimized our parameters such that they are close to the original 1:5 ratio!</p>
<p>And for our component means?</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">component_mus_opt</span><span class="p">,</span> <span class="n">locs_true</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Really close too!</p>
<p>Finally, for the component scales:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_component_scales_opt</span><span class="p">),</span> <span class="n">scale_true</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Very nice, really close to the ground truth too.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="visualizing-training-dynamics">Visualizing training dynamics</h2>
<p>Let's now visualize how training went.</p>
<p>I have created a function called <code>animate_training</code>, which will provide for us a visual representation.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="n">animate_training</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>animate_training</code> leverages <a href="https://github.com/jwkvam/celluloid"><code>celluloid</code></a> to make easy matplotlib animations. You can check out the package <a href="https://github.com/jwkvam/celluloid">here</a>.</p>
<p>We can now call on <code>animate_training</code> to give us an animation of the mixture Gaussian PDFs as we trained the model.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="o">%%</span><span class="n">capture</span>
<span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">animate_training</span>

<span class="n">params_history</span> <span class="o">=</span> <span class="n">adam_get_params</span><span class="p">(</span><span class="n">state_history</span><span class="p">)</span>

<span class="n">animation</span> <span class="o">=</span> <span class="n">animate_training</span><span class="p">(</span><span class="n">params_history</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">data_mixture</span><span class="p">)</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="n">HTML</span><span class="p">(</span><span class="n">animation</span><span class="o">.</span><span class="n">to_html5_video</span><span class="p">())</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There's some comments to be said on the dynamics here:</p>
<ol>
<li>At first, one Gaussian is used to approximate over the entire distribution. It's not a good fit, but approximates it fine enough.</li>
<li>However, our optimization routine continues to push forward, eventually finding the bimodal pattern. Once this happens, the PDFs fit very nicely to the data samples.</li>
</ol>
<p>This phenomena is also reflected in the loss:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">dl_workshop.gaussian_mixture</span> <span class="kn">import</span> <span class="n">get_loss</span>
<span class="n">get_loss</span><span class="err">??</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because <code>states_history</code> is the result of <code>lax.scan</code>-ing, we can <code>vmap</code> our <code>get_loss</code> function over the <code>states_history</code> object to get back an array of losses that can then be plotted:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vmap</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="n">losses</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">get_loss</span><span class="p">,</span> <span class="n">get_params_func</span><span class="o">=</span><span class="n">adam_get_params</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">loss_mixture_weights</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data_mixture</span><span class="p">))(</span><span class="n">state_history</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">);</span>
</code></pre></div>



</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You should notice the first plateau, followed by the second plateau.
This corresponds to the two phases of learning.</p>
<p>Now, thus far, we have set up the problem in a fashion that is essentially "trivial".
What if, however, we wanted to try fitting a mixture Gaussian where we didn't know exactly how many mixture components there <em>ought</em> to be?</p>
<p>To check that out, head over to the next section in this chapter.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">


<div class="codehilite"><pre><span></span><code>
</code></pre></div>



</div>
</div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../../03-stax/02-neural/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Neural Networks with stax
            </div>
          </div>
        </a>
      
      
        <a href="../02-dirichlet-processes/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Dirichlet Processes
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="http://www.shortwhale.com/ericmjl" target="_blank" rel="noopener" title="www.shortwhale.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://github.com/ericmjl" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://twitter.com/ericmjl" target="_blank" rel="noopener" title="twitter.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://linkedin.com/in/ericmjl" target="_blank" rel="noopener" title="linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["tabs"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../../assets/javascripts/workers/search.4fa0e4ee.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.1d3bfcf1.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js"></script>
      
    
  </body>
</html>